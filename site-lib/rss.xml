<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Ernad's Knowledge Base]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>site-lib/media/favicon.png</url><title>Ernad&apos;s Knowledge Base</title><link></link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Thu, 31 Jul 2025 21:18:01 GMT</lastBuildDate><atom:link href="site-lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Thu, 31 Jul 2025 21:17:54 GMT</pubDate><ttl>60</ttl><dc:creator></dc:creator><item><title><![CDATA[Index]]></title><description><![CDATA[
<a data-href="Supervised Learning" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Supervised Learning</a>
<br><a data-href="Unsupervised Learning" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Unsupervised Learning</a>
<br><a data-href="Reinforcement Learning" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Reinforcement Learning</a> <br><a data-href="Neural Network" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Neural Network</a>
<br><a data-href="Convolutional Neural Network" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Convolutional Neural Network</a>
<br><a data-href="Recurrent Neural Network" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Recurrent Neural Network</a> <br><a data-href="PyTorch" href=".html" class="internal-link" target="_self" rel="noopener nofollow">PyTorch</a>
<br><a data-href="Scikit-Learn" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Scikit-Learn</a>
<br><a data-href="OpenCV" href=".html" class="internal-link" target="_self" rel="noopener nofollow">OpenCV</a>
<br><a data-href="Natural Language Processing" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Natural Language Processing</a>
<br><a data-href="Computer Vision" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Computer Vision</a>
<br><a data-href="Robotics" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Robotics</a> Genetic Algorithms &amp; Evolutionary Methods
Bayesian Methods
Graph Algorithms Data Preprocessing
Data Exploration
Data Mining Techniques
]]></description><link>index.html</link><guid isPermaLink="false">Index.md</guid><pubDate>Thu, 31 Jul 2025 20:59:16 GMT</pubDate></item><item><title><![CDATA[Manhattan Distance]]></title><description><![CDATA[The Manhattan distance is a <a data-href="Distance Measure" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Distance Measure</a> defined as the sum of the absolute differences of the Cartesian coordinates of two points. It is the distance between two points using only grid-like movements (horizontal and vertical). The Manhattan distance is also the norm of the distance between two vectors in <a data-href="Lp Space" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Lp Space</a>.The Manhattan distance between two objects in -dimensional space is calculated as:
<br>Positive: Like any other distance metric, the <a data-tooltip-position="top" aria-label="Range" data-href="Range" href="notes/math/range.html#_1" class="internal-link" target="_self" rel="noopener nofollow">range</a> of the Manhattan distance is where a 0 distance indicates that the two points are at the same location.
Symmetric: The Manhattan distance is symmetric, meaning <br><a data-href="Triangle Inequality" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Triangle Inequality</a>: The Manhattan distance obeys the triangle inequality, which states that the distance from to is always less than or equal to the distance from to plus the distance from to . Meaning that taking a detour through a third point cannot result in a shorter distance than a direct path from to . <br>‚ÄúTaxicab geometry,‚Äù&nbsp;Wikipedia, Jan. 21, 2022. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Taxicab_geometry" target="_self">https://en.wikipedia.org/wiki/Taxicab_geometry</a>
<br>GeeksforGeeks, ‚ÄúClustering Distance Measures,‚Äù&nbsp;GeeksforGeeks, May 24, 2024. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.geeksforgeeks.org/machine-learning/clustering-distance-measures/#common-distance-measures" target="_self">https://www.geeksforgeeks.org/machine-learning/clustering-distance-measures/#common-distance-measures</a> (accessed Jul. 31, 2025).
]]></description><link>notes/math/manhattan-distance.html</link><guid isPermaLink="false">NOTES/Math/Manhattan Distance.md</guid><pubDate>Thu, 31 Jul 2025 00:00:00 GMT</pubDate></item><item><title><![CDATA[Outlier]]></title><description><![CDATA[Outliers are data points that differ significantly from the other observations in a dataset. Outliers may occur due to measurement or recording error, or could possibly represent an important anomaly warranting further analysis. There is no fixed definition of what constitutes as an outlier, typically, specific domain knowledge is usually necessary to understand whether a specific observation is an outlier, or is a natural phenomenon of the dataset.
Measurement Error: Outliers may occur due to user error in the data collection process, or could occur due to errors in autonomous systems, such as sensor failure.
Natural Variation: Outliers may represent perfectly legitimate values that are an inherent part of the naturally occurring variations in the underlying domain of the dataset.
Anomaly: Outliers may represent unusual behavior, such as fraudulent transactions, which warrant further investigation and analysis. Outliers can have a significant impact on various statistical measures, such as <a data-href="Mean" href="notes/math/mean.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Mean</a>, <a data-href="Standard Deviation" href="notes/math/standard-deviation.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Standard Deviation</a>, or <a data-href="Range" href="notes/math/range.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Range</a>.
<br>Outliers could also hinder the performance of some machine learning models such as <a data-href="Logistic Regression" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Logistic Regression</a> or <a data-href="K-Nearest-Neighbors" href=".html" class="internal-link" target="_self" rel="noopener nofollow">K-Nearest-Neighbors</a>. Global Outliers: A global outlier deviates significantly from the entire population globally.
Collective Outliers: A group or subset of data points that collectively deviate considerably from the overall distribution. Typically require special techniques to detect.
Contextual/Local Outliers: Data points whose value deviate significantly relative to other data points within the same "context." Contextual outliers may not be considered outliers when considered globally, meaning they need special attention to be properly detected and analyzed.
While the terms outliers and extreme values may be used interchangeably, they have distinct definitions in statistics:
Outlier: is a data point that varies significantly from the rest of the dataset.
Extreme values: values that reside at the outer edges of the dataset, representing the highest and lowest points in a dataset. Extreme values may be outliers, or they be a natural part of a distribution.
<br>Z-score, sometimes called the standard score, measures how many <a data-tooltip-position="top" aria-label="Standard Deviation" data-href="Standard Deviation" href="notes/math/standard-deviation.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Standard Deviations</a> a data object is from the <a data-tooltip-position="top" aria-label="Mean" data-href="Mean" href="notes/math/mean.html#_0" class="internal-link" target="_self" rel="noopener nofollow">mean</a> of the distribution. A positive z-score indicates the values is greater than the <a href="notes/math/mean.html#_0" target="_self" rel="noopener noreferrer" from="82" to="86" origin-text="mean" class="internal-link virtual-link-a">mean</a>üîó, while a negative score indicates it is less than the mean.Mathematically, the z-score is defined as:Where: is the z-score. is the given data value.
<br> is the population <a href="notes/math/mean.html#_0" target="_self" rel="noopener noreferrer" from="19" to="23" origin-text="mean" class="internal-link virtual-link-a">mean</a>üîó.
<br> is the population <a href="notes/math/standard-deviation.html#_0" target="_self" rel="noopener noreferrer" from="20" to="38" origin-text="standard deviation" class="internal-link virtual-link-a">standard deviation</a>üîó.
The z-score is commonly used to detect outliers by flagging any values that are outside of a specified threshold (commonly -3 and 3).<br>The Interquartile <a href="notes/math/range.html#_0" target="_self" rel="noopener noreferrer" from="18" to="23" origin-text="Range" class="internal-link virtual-link-a">Range</a>üîó (IQR) is a measure of statistical dispersion that represents the range within which the central 50% of the data points lie.Mathematically, the IQR is defined as:Where:
<br> is the value of the 3rd <a data-href="Quartile" href="notes/math/quartile.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Quartile</a>.
<br> is the value of the 1st <a href="notes/math/quartile.html#_0" target="_self" rel="noopener noreferrer" from="25" to="33" origin-text="Quartile" class="internal-link virtual-link-a">Quartile</a>üîó.
The IQR is commonly used to detect outliers by flagging any values that are outside of a specified boundary, typically:
Lower Bound: Upper Bound: Any data point below the lower bound or above the upper bound is considered an outlier.
<br>K-Nearest Neighbors is a supervised machine learning algorithm which can be used for both classification and regression tasks. The algorithm relies on <a data-tooltip-position="top" aria-label="Distance Metric" data-href="Distance Metric" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Distance Metrics</a> such as <a data-href="Euclidean Distance" href="notes/math/euclidean-distance.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Euclidean Distance</a>, <a data-href="Manhattan Distance" href="notes/math/manhattan-distance.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Manhattan Distance</a>, or <a data-href="Minkowski Distance" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Minkowski Distance</a> to find the "-nearest neighbors" of a data object, where is a parameter indicating the number of neighbors to consider when making a prediction.
<br>Regression: For regression tasks, the algorithm takes the average values of the k-nearest-neighbors of the data object, where the neighbors come from the training set. Then the <a data-tooltip-position="top" aria-label="Mean" data-href="Mean" href="notes/math/mean.html#_0" class="internal-link" target="_self" rel="noopener nofollow">mean</a> of the neighbors' values is the predicted value for the object.
Classification: For classification tasks, a majority vote among the object's k-nearest-neighbors is taken to determine the category for the given data object.
K-nearest-neighbors is used to detect outliers by assigning an outlier score to a data object, which is done by measuring the distance of an object from its nearest neighbors. Though, this approach is not effective for collective outliers.DBSCAN is a density-based clustering algorithm that clusters data based on the density of data points. DBSCAN excels at identifying an arbitrary number of clusters, and can handle nested clusters of arbitrary shapes.
Two parameters are chosen, represents the radius within which to classify neighbors; and minPts, which represents the minimum number of neighbors within to classify the point as a "core point."
A data point is considered a core point if the amount of other data points that falls within its radius is at least minPts.
After all core points are identified, for each core point, create a cluster of the core point and all the points within its radius.
Expand the cluster by iterating through all reachable points and adding them to the cluster if they're core points.
Points that are not reachable from any core points are labelled as noise or outliers.
DBSCAN is popular algorithm for anomaly detection, since outliers are detected based on relative density of data.<br>One straightforward method for dealing with outliers is simply to remove them from the dataset. This approach if effective as long as outliers are not relevant to the analysis, such as the case where the focus is not on <a data-href="Anomaly Detection" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Anomaly Detection</a>.Transformations can be applied to the data to minimize the effect of outliers. Some popular techniques include:
<br><a data-href="Scaling" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Scaling</a>: Adjusting the <a href="notes/math/range.html#_0" target="_self" rel="noopener noreferrer" from="16" to="21" origin-text="range" class="internal-link virtual-link-a">range</a>üîó of the data to reduce the influence of extreme values. This includes methods such as <a data-href="Min-Max Scaling" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Min-Max Scaling</a> or <a data-href="Z-Score Normalization" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Z-Score Normalization</a>.
<br><a data-href="Winsorization" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Winsorization</a>: Replacing outlier values with the nearest value within a specific percentile range. For example, any values outside of the middle 95% percentile are replaced with the nearest values within that range.
<br><a data-href="Log Transformation" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Log Transformation</a>: Applying logarithmic transformations to reduce <a data-href="Variance" href="notes/math/variance.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Variance</a> and make the data more normally distributed.
<br>Another approach to handling outliers is explicitly modeling them. This can be done by adding a new <a data-href="Binary Data" href="notes/math/binary-data.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Binary Data</a> attribute that specifies whether a given data object is an outlier or not.
J. Han, M. Kamber, and J. Pei,&nbsp;Data Mining : Concepts and Techniques. Burlington, Ma: Elsevier, 2012.
<br>I. Cohen, ‚ÄúOutlier Detection &amp; Analysis: The Different Types of Outliers,‚Äù&nbsp;Anodot, Feb. 25, 2022. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.anodot.com/blog/quick-guide-different-types-outliers/" target="_self">https://www.anodot.com/blog/quick-guide-different-types-outliers/</a>
<br>GeeksforGeeks, ‚ÄúTypes of Outliers in Data Mining,‚Äù&nbsp;GeeksforGeeks, Jul. 2021. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.geeksforgeeks.org/data-analysis/types-of-outliers-in-data-mining/" target="_self">https://www.geeksforgeeks.org/data-analysis/types-of-outliers-in-data-mining/</a> (accessed Jul. 08, 2025).
<br>S. Glen, ‚ÄúOutliers: Finding Them in Data, Formula, Examples. Easy Steps and Video,‚Äù&nbsp;Statistics How To. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.statisticshowto.com/statistics-basics/find-outliers/" target="_self">https://www.statisticshowto.com/statistics-basics/find-outliers/</a>
<br>Wikipedia Contributors, ‚ÄúOutlier,‚Äù&nbsp;Wikipedia, Apr. 07, 2019. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Outlier" target="_self">https://en.wikipedia.org/wiki/Outlier</a>
<br>GeeksforGeeks, ‚ÄúHow to Detect Outliers in Machine Learning,‚Äù&nbsp;GeeksforGeeks, Jan. 12, 2019. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.geeksforgeeks.org/machine-learning/machine-learning-outlier/" target="_self">https://www.geeksforgeeks.org/machine-learning/machine-learning-outlier/</a> (accessed Jul. 10, 2025).
<br>Wikipedia Contributors, ‚ÄúStandard score,‚Äù&nbsp;Wikipedia, Sep. 12, 2019. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Standard_score" target="_self">https://en.wikipedia.org/wiki/Standard_score</a>
]]></description><link>notes/math/outlier.html</link><guid isPermaLink="false">NOTES/Math/Outlier.md</guid><pubDate>Tue, 08 Jul 2025 00:00:00 GMT</pubDate></item><item><title><![CDATA[Transitive Property]]></title><description><![CDATA[If a relation is transitive then for all possible elements , and , if that relation holds for and , and that same relation holds between and , then that relation must hold between and .Where: is some relation such as equality or inequality. Equality: If and then .
Inequality: If and then .
Set Inclusion: If and then .
Implication: If and then .
Inheritance: If inherits from and inherits from then inherits from . Mathematics: Used extensively in algebra, geometry, and set theory to establish relationships between numbers, variables, or sets.
Computer Science: Used in graph theory algorithms like <a data-href="Warshall's Algorithm" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Warshall's Algorithm</a> which is used to determine the <a data-href="Transitive Closure" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Transitive Closure</a> of a directed graph. The <a data-href="Transitive Closure" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Transitive Closure</a> of a directed graph represents which vertices are reachable from others. The <a data-href="Transitive Reduction" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Transitive Reduction</a> of a directed graph is the smallest reduction that has the same reachability relation as the original graph.
Logic: The transitive property is a fundamental property in proofs and reasoning, allowing for the derivation of conclusions based on established relationships.
The reflexive property states that any value or expression is equal to itself. The equality relation is an example of a reflexive operation, since all real numbers or variables are equal to themselves.A relation is symmetric if for all possible elements and , if is related to , then is related to . Equality is an example of a symmetric relation.<br>An equivalence relation is a relation that is reflexive, symmetric, and transitive. Equivalence relations can group elements of a set into distinct categories called <a data-href="Equivalence Classes" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Equivalence Classes</a>, which are disjoint subsets containing elements that are equivalent to one another.<br>A partial order on a set is a binary relation that is reflexive, antisymmetric, and transitive. It is a way to order elements in a set where not all pairs of elements need to be comparable. Partial orders are visualized using a <a data-href="Hasse Diagram" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Hasse Diagram</a>.<br>Preorder is a binary relation defined on a set that is both reflexive and transitive. It is a generalization of partial orders in the sense that it does not require antisymmetry. Preorders are commonly applied in <a data-href="Decision Theory" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Decision Theory</a> to model preferences, or in <a data-href="Game Theory" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Game Theory</a> to compare strategies.
A binary relation is intransitive if there exists three values where transitivity does not hold.
Antitransitivity is a stronger property which holds if for any three values, transitivity never holds. <br>GeeksforGeeks, ‚ÄúTransitive Property,‚Äù&nbsp;GeeksforGeeks, Mar. 07, 2024. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.geeksforgeeks.org/maths/transitive-property/" target="_self">https://www.geeksforgeeks.org/maths/transitive-property/</a> (accessed Jul. 25, 2025)
<br>‚ÄúTransitive relation,‚Äù&nbsp;Wikipedia, Jan. 29, 2023. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Transitive_relation" target="_self">https://en.wikipedia.org/wiki/Transitive_relation</a>
<br>GeeksforGeeks, ‚ÄúEquivalence Relations,‚Äù&nbsp;GeeksforGeeks, Nov. 09, 2023. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.geeksforgeeks.org/maths/equivalence-relations/" target="_self">https://www.geeksforgeeks.org/maths/equivalence-relations/</a>
<br>‚ÄúIntransitivity,‚Äù&nbsp;Wikipedia, Mar. 07, 2024. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Intransitivity" target="_self">https://en.wikipedia.org/wiki/Intransitivity</a>
]]></description><link>notes/math/transitive-property.html</link><guid isPermaLink="false">NOTES/Math/Transitive Property.md</guid><pubDate>Fri, 25 Jul 2025 00:00:00 GMT</pubDate></item><item><title><![CDATA[Median]]></title><description><![CDATA[The Median is a <a data-href="Measure of Central Tendency" href="notes/math/measure-of-central-tendency.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Measure of Central Tendency</a> that represents the middle value of an ordered dataset. It is particularly useful in scenarios where the data is skewed or contains <a data-tooltip-position="top" aria-label="Outlier" data-href="Outlier" href="notes/math/outlier.html#_0" class="internal-link" target="_self" rel="noopener nofollow">outliers</a>, as it provides a more accurate representation of the center of the dataset compared to other measures like the <a data-href="Mean" href="notes/math/mean.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Mean</a>. The median can only be applied to <a data-href="Numerical Data" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Numerical Data</a> and not <a data-href="Categorical Data" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Categorical Data</a>.<br>The median is defined as the 50th <a data-href="Percentile" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Percentile</a> or the second <a data-href="Quartile" href="notes/math/quartile.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Quartile</a> (Q2), which divides the dataset into two equal halves. This means that half of the data points are below the median and half are above it.
Uniqueness: In a finite dataset, the median is unique, meaning there is only one median.
<br>Robustness: The median is robust to <a href="notes/math/outlier.html#_0" target="_self" rel="noopener noreferrer" from="26" to="34" origin-text="outliers" class="internal-link virtual-link-a">outliers</a>üîó, making it a reliable measure in skewed or noisy datasets.
Non-Parametric: The median does not assume a particular distribution of the underlying data.
Invariance: The median remains unchanged under linear transformations of the dataset.
If is even, the formula for the median is:While if is odd, the formula for the median is:<br>The multivariate median extends the concept of the median to multiple dimensions. One of the most common multivariate median is the <a data-href="Geometric Median" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Geometric Median</a> which focuses on minimizing the <a data-tooltip-position="top" aria-label="Euclidean Distance" data-href="Euclidean Distance" href="notes/math/euclidean-distance.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Euclidean distance</a> of a set of points in a Euclidean space. The geometric median is defined as:Where:
<br> denotes the <a href="notes/math/euclidean-distance.html#_0" target="_self" rel="noopener noreferrer" from="13" to="31" origin-text="Euclidean distance" class="internal-link virtual-link-a">Euclidean distance</a>üîó. represents the -th data point in -dimensional space. <br>Machine Learning: Multivariate medians are commonly employed in machine learning, particularly in clustering algorithms when dealing with centroid initialization, such as in <a data-href="DBSCAN" href=".html" class="internal-link" target="_self" rel="noopener nofollow">DBSCAN</a>.
Computer Vision: The geometric median can be used to find a central point among pixel locations, helping in tasks such as object tracking.
<br><a data-href="Robust Statistics" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Robust Statistics</a>: Multivariate medians are utilized as a <a href="notes/math/measure-of-central-tendency.html#_0" target="_self" rel="noopener noreferrer" from="41" to="68" origin-text="measure of central tendency" class="internal-link virtual-link-a">measure of central tendency</a>üîó over other measures such as the <a href="notes/math/mean.html#_0" target="_self" rel="noopener noreferrer" from="101" to="105" origin-text="mean" class="internal-link virtual-link-a">mean</a>üîó, due to median's robustness to <a href="notes/math/outlier.html#_0" target="_self" rel="noopener noreferrer" from="137" to="145" origin-text="outliers" class="internal-link virtual-link-a">outliers</a>üîó.
<br>Other common <a data-tooltip-position="top" aria-label="Measure of Central Tendency" data-href="Measure of Central Tendency" href="notes/math/measure-of-central-tendency.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Measures of Central Tendency</a> include the <a data-href="Mean" href="notes/math/mean.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Mean</a>, which represents the average value of a population, the <a data-href="Mode" href="notes/math/mode.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Mode</a>, which identifies the most frequently occurring value in a set, and the <a data-href="Midrange" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Midrange</a>, calculated as the average of the maximum and minimum values.
J. Han, M. Kamber, and J. Pei,&nbsp;Data Mining : Concepts and Techniques. Burlington, Ma: Elsevier, 2012.
<br>Wikipedia, ‚ÄúMedian,‚Äù&nbsp;Wikipedia, Apr. 17, 2020. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Median" target="_self">https://en.wikipedia.org/wiki/Median</a>
]]></description><link>notes/math/median.html</link><guid isPermaLink="false">NOTES/Math/Median.md</guid><pubDate>Mon, 07 Jul 2025 00:00:00 GMT</pubDate></item><item><title><![CDATA[Math MOC]]></title><description><![CDATA[ ]]></description><link>notes/math/math-moc.html</link><guid isPermaLink="false">NOTES/Math/Math MOC.md</guid><pubDate>Thu, 31 Jul 2025 00:00:00 GMT</pubDate></item><item><title><![CDATA[Binary Data]]></title><description><![CDATA[Binary data is a type of <a data-href="Categorical Data" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Categorical Data</a> with only two possible values, typically 1 and 0. Binary data is a specific type of <a data-href="Nominal Data" href="notes/math/nominal-data.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Nominal Data</a>, meaning the values are non-numeric, and qualitative.
Presence: Binary data is commonly used to represent the presence or absence of a specific attribute, 1 meaning the attribute is present, 0 meaning the attribute is absent.
Truth: Binary data is also commonly used to represent truth, where 1 indicates truth, and 0 indicated falsehood.
Opposing Categories: Binary data is commonly used to represent 2 opposing categories, such as male/female, or employed/unemployed.
<br>Since <a href="notes/math/nominal-data.html#_0" target="_self" rel="noopener noreferrer" from="6" to="18" origin-text="nominal data" class="internal-link virtual-link-a">nominal data</a>üîó lacks numerical significance, data operations such as <a data-href="Mean" href="notes/math/mean.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Mean</a> or <a data-href="Median" href="notes/math/median.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Median</a> cannot be performed. However, the frequency of nominal data values can be analyzed, and a measure like the <a data-href="Mode" href="notes/math/mode.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Mode</a> can describe the most common category within a given dataset.<br><a data-href="Binary Regression" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Binary Regression</a> estimates a function that maps one or more independent variables to a single dependent binary variable. Common techniques include:
<br>
<a data-href="Logistic Regression" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Logistic Regression</a>: A statistical method used for binary classification, predicting the <a data-tooltip-position="top" aria-label="Probability" data-href="Probability" href="notes/math/probability.html#_0" class="internal-link" target="_self" rel="noopener nofollow">probability</a> that a given input vector belongs to a certain binary category. It is essentially a <a data-href="Regression" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Regression</a> model with a Logistic Function applied to map the output to a value between 0 and 1. <br>
<a data-href="Probit Regression" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Probit Regression</a>: Similar to logistic regression, though, it assumes that errors between the predicted and actual values are normally distributed. Probit regression also uses the <a data-href="Probit Link Function" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Probit Link Function</a> rather than the logistic function. J. Han, M. Kamber, and J. Pei,&nbsp;Data Mining : Concepts and Techniques. Burlington, Ma: Elsevier, 2012.
<br>‚ÄúBinary data,‚Äù&nbsp;Wikipedia, Sep. 13, 2021. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Binary_data" target="_self">https://en.wikipedia.org/wiki/Binary_data</a>
Wikipedia Contributors, ‚ÄúBinary regression,‚Äù&nbsp;Wikipedia, Mar. 27, 2022.
]]></description><link>notes/math/binary-data.html</link><guid isPermaLink="false">NOTES/Math/Binary Data.md</guid><pubDate>Tue, 08 Jul 2025 00:00:00 GMT</pubDate></item><item><title><![CDATA[Least Squares]]></title><description><![CDATA[The least squares method is an optimization technique that attempts to find a linear function which minimizes the sum of squared distances between observed and predicted values. This method is widely used in <a data-href="Regression" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Regression</a> analysis for finding the best-fit function between a matrix of features and one or more independent variables.<br><img alt="linearRegression.png" src="resources/linearregression.png" target="_self"><br>In a <a data-href="Simple Linear Regression" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Simple Linear Regression</a>, the model is defined as:Where: is the dependent variable. is the slope of the line. is the independent variable. is the y-intercept.
<br>The objective is to minimize the sum of squared <a data-tooltip-position="top" aria-label="Euclidean Distance" data-href="Euclidean Distance" href="notes/math/euclidean-distance.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Euclidean distances</a>:Where: is the number of data points. are the observed data points.
This is equivalent to min
Calculate Slope: Use the following formula to calculate the slope, : Calculate Y-Intercept: Use the slope calculated in the previous step and the following formula to calculate the y-intercept, : Formulate the Function: Construct the line of best fit in the form of <br>Sensitive to <a data-tooltip-position="top" aria-label="Outlier" data-href="Outlier" href="notes/math/outlier.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Outliers</a>:The least squares method is sensitive to <a href="notes/math/outlier.html#_0" target="_self" rel="noopener noreferrer" from="42" to="50" origin-text="outliers" class="internal-link virtual-link-a">outliers</a>üîó, which could unfairly skew the resulting regression line, resulting in inaccurate predictions.
<br>Assumes Linearity: The least squares method assumes a linear relationship between the dependent and independent variables. If the underlying relationship is non-linear, this can introduce <a data-href="Bias" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Bias</a> in the predictions.
<br>Assumes <a data-href="Homoscedasticity" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Homoscedasticity</a>: The least squares method assumes that the <a data-tooltip-position="top" aria-label="Variance" data-href="Variance" href="notes/math/variance.html#_0" class="internal-link" target="_self" rel="noopener nofollow">variance</a> of errors is a constant (homoscedastic), rather than a function of the independent variable (<a data-href="Heteroscedastic" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Heteroscedastic</a>).
<br><a data-href="Multicollinearity" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Multicollinearity</a>: If the independent variables are highly correlated, the least squares method has difficulty determining the effect of each attribute on the dependent variable, making the model unstable.
The most common form of the least squares method, commonly used in Linear Regression models. It simply minimizes the sum of squared residuals without any weights.<br>Extends ordinary least squares to handle heteroscedasticity by assigning weights to data objects based on <a data-href="Covariance Matrix" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Covariance Matrix</a> of the residuals.<br>A specific type of generalized least squares, used when the dataset exhibits heteroscedasticity. WLS is more robust because it weighs the influence of each data point based on its <a href="notes/math/variance.html#_0" target="_self" rel="noopener noreferrer" from="180" to="188" origin-text="variance" class="internal-link virtual-link-a">variance</a>üîó. <br>Also known as L2 regularization, ridge regression is a type of linear regression that introduces a penalty term to the OLS cost function. Ridge regression adds the squared coefficients as a penalty to the cost function, thereby, punishing large coefficient values. This helps prevent overfitting and addresses any <a data-href="Multicollinearity" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Multicollinearity</a> in the independent variables.
The cost function for ridge regression defined as:Where: is the actual output. is the predicted output. are the coefficients is the regularization parameter which determines the magnitude of the penalty term.
<br>By adjusting the ridge parameter, Œª, ridge regression allows for control over the bias-<a href="notes/math/variance.html#_0" target="_self" rel="noopener noreferrer" from="87" to="95" origin-text="variance" class="internal-link virtual-link-a">variance</a>üîó tradeoff. As increases, the bias of the model increases and the variance decreases. Optimizing this parameter can achieve the ideal balance between overfitting and underfitting model.Also known as L1 regularization, lasso regression is a type of linear regression which, like ridge regression, introduces a penalty term to the OLS cost function. Lasso regression adds the absolute coefficient values to the cost function, this allows the coefficients of irrelevant variables to be shrunk down to 0, thereby, simplifying and regularizing the model.
The cost function for lasso regression defined as:Where: is the actual output. is the predicted output. are the coefficients is the regularization parameter which determines the magnitude of the penalty term.
<br>Unlike ridge regression, which shrinks coefficients but typically keeps all predictors in the model, lasso regression can set the coefficients of irrelevant variables exactly to zero. This makes it an effective technique for <a data-href="Feature Selection" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Feature Selection</a>, that is, identifying and retaining only the most important variables.
<br>Prabhu Raghav, ‚ÄúLinear Regression Simplified - Ordinary Least Square vs Gradient Descent,‚Äù&nbsp;Medium, May 15, 2018. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://medium.com/data-science/linear-regression-simplified-ordinary-least-square-vs-gradient-descent-48145de2cf76" target="_self">https://medium.com/data-science/linear-regression-simplified-ordinary-least-square-vs-gradient-descent-48145de2cf76</a> (accessed Jul. 21, 2025).
<br>GeeksforGeeks, ‚ÄúLeast Square Method | Definition Graph and Formula,‚Äù&nbsp;GeeksforGeeks, Jul. 06, 2023. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.geeksforgeeks.org/maths/least-square-method/" target="_self">https://www.geeksforgeeks.org/maths/least-square-method/</a>
<br>‚ÄúLeast squares,‚Äù&nbsp;Wikipedia, Dec. 19, 2019. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Least_squares" target="_self">https://en.wikipedia.org/wiki/Least_squares</a>
<br>A. Menon, ‚ÄúLinear Regression Using Least Squares - TDS Archive - Medium,‚Äù&nbsp;Medium, Sep. 08, 2018. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://medium.com/data-science/linear-regression-using-least-squares-a4c3456e8570" target="_self">https://medium.com/data-science/linear-regression-using-least-squares-a4c3456e8570</a> (accessed Jul. 21, 2025).
<br>The Organic Chemistry Tutor, ‚ÄúLinear Regression Using Least Squares Method - Line of Best Fit Equation,‚Äù&nbsp;YouTube. Jul. 13, 2020. [YouTube Video]. Available: <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.youtube.com/watch?v=P8hT5nDai6A" target="_self">https://www.youtube.com/watch?v=P8hT5nDai6A</a>
<br>GeeksforGeeks, ‚ÄúRidge Regression,‚Äù&nbsp;GeeksforGeeks, Jun. 11, 2024. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.geeksforgeeks.org/machine-learning/what-is-ridge-regression/" target="_self">https://www.geeksforgeeks.org/machine-learning/what-is-ridge-regression/</a> (accessed Jul. 23, 2025).
<br>GeeksforGeeks, ‚ÄúWhat is Lasso Regression?,‚Äù&nbsp;GeeksforGeeks, May 15, 2024. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.geeksforgeeks.org/machine-learning/what-is-lasso-regression/" target="_self">https://www.geeksforgeeks.org/machine-learning/what-is-lasso-regression/</a>
]]></description><link>notes/math/least-squares.html</link><guid isPermaLink="false">NOTES/Math/Least Squares.md</guid><pubDate>Mon, 21 Jul 2025 00:00:00 GMT</pubDate><enclosure url="resources/linearregression.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;resources/linearregression.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Euclidean Distance]]></title><description><![CDATA[The Euclidean distance is the most popular <a data-href="Distance Measure" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Distance Measure</a> that quantifies the dissimilarity between <a data-href="Numerical Data" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Numerical Data</a>. It represents the straight-line distance between two points in Euclidean space and is calculated using the <a data-href="Pythagorean Theorem" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Pythagorean Theorem</a>. Let and be two data objects described by numerical attributes. The Euclidean distance between these two objects is: This formula computes the square root of the sum of the squared differences of each corresponding attribute, providing a measure of the straight-line distance in the multidimensional space.
<br>Positive: Like any other distance metric, the <a data-tooltip-position="top" aria-label="Range" data-href="Range" href="notes/math/range.html#_0" class="internal-link" target="_self" rel="noopener nofollow">range</a> of the Euclidean distance is where a 0 distance indicates that the two points are at the same location.
Symmetric: The Euclidean distance is symmetric, meaning <br><a data-href="Triangle Inequality" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Triangle Inequality</a>: The Euclidean distance obeys the triangle inequality, which states that the distance from to is always less than or equal to the distance from to plus the distance from to . Meaning that taking a detour through a third point cannot result in a shorter distance than a direct path from to .
The squared Euclidean distance is computed as only the sum of squared differences:<br>The squared Euclidean distance amplifies greater distances more so then the standard Euclidean distance, and is faster and easier to compute. This makes it more desirable in problems where significant distances should be penalized harsher, such as in <a data-href="Clustering" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Clustering</a> or <a data-href="Outlier Detection" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Outlier Detection</a>. Though, the squared Euclidian distance does not obey the triangle inequality.<br>Squared Euclidean distance is a <a data-href="Convex Function" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Convex Function</a>, which makes it more desirable in optimization theory since it permits the use of <a data-href="Convex Analysis" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Convex Analysis</a>.<br><a href="notes/math/least-squares.html#_0" target="_self" rel="noopener noreferrer" from="0" to="13" origin-text="Least squares" class="internal-link virtual-link-a">Least squares</a>üîó is an optimization technique that attempts to find the function which minimizes the sum of the square Euclidean distances between the observed and predicted values. This method is widely used in <a data-href="Machine Learning" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Machine Learning</a>, particularly in <a data-href="Regression" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Regression</a> analysis.<br>Divergence is a kind of distance measure that applies to <a href="notes/math/probability.html#_0" target="_self" rel="noopener noreferrer" from="57" to="68" origin-text="probability" class="internal-link virtual-link-a">probability</a>üîó distributions. The squared Euclidean distance is the simplest divergence measure.<br>Other common divergence measures include <a data-href="Kullback‚ÄìLeibler Divergence" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Kullback‚ÄìLeibler Divergence</a> and <a data-href="Jensen-Shannon Divergence" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Jensen-Shannon Divergence</a>.
<br>Clustering: Euclidean distance is used in clustering algorithms such as <a data-href="K-Means Clustering" href=".html" class="internal-link" target="_self" rel="noopener nofollow">K-Means Clustering</a> to measure distance between data points.
<br><a data-href="Classification" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Classification</a>: Classification algorithms like <a data-href="K-Nearest-Neighbors" href=".html" class="internal-link" target="_self" rel="noopener nofollow">K-Nearest-Neighbors</a> may utilize Euclidean distance to classify data points based on the label of their nearest neighbors. <br><a data-href="Outlier" href="notes/math/outlier.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Outlier</a> Detection: The Euclidean or squared Euclidean distance can be used to identify data points which deviate significantly from the rest of the dataset.
<br>Multivariate Analysis: Euclidean distance can be used in techniques like <a data-href="Principal Component Analysis" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Principal Component Analysis</a> or <a data-href="Multidimensional Scaling" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Multidimensional Scaling</a> to measure the distance between points with multiple dimensions.
<br>Least Squares Method: The Euclidean distance is a key step for the method of <a href="notes/math/least-squares.html#_0" target="_self" rel="noopener noreferrer" from="57" to="70" origin-text="least squares" class="internal-link virtual-link-a">least squares</a>üîó, which is commonly used to optimize regression problems.
<br>Divergence: Squared Euclidean distance is a simple measure of divergence, allowing you to compare <a href="notes/math/probability.html#_0" target="_self" rel="noopener noreferrer" from="88" to="99" origin-text="probability" class="internal-link virtual-link-a">probability</a>üîó distributions.
Optimization: Squared Euclidean distance is preferred in optimization theory due to its smoothness and convexity, permitting the use of convex analysis. Path Planning: Euclidean distance can be used for calculating the shortest path between points. <br>Localization: in <a data-href="Simultaneous Localization and Mapping" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Simultaneous Localization and Mapping</a> problems, the Euclidean distance can be used to determine a robot's position relative to known landmarks. Image Recognition: Euclidean distance is used to compare feature vectors in images, aiding in object recognition.
Image Segmentation: Euclidean distance is used to measure similarities between pixel values in clustering-based image segmentation.
Other common distance measures include:
<br><a data-href="Chebyshev Distance" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Chebyshev Distance</a>: The maximum absolute difference between 2 vectors across all dimensions.
<br><a data-href="Minkowski Distance" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Minkowski Distance</a>: A generalized distance measure that is defined by a parameter whose common values are the <a href="notes/math/manhattan-distance.html#_0" target="_self" rel="noopener noreferrer" from="29" to="47" origin-text="Manhattan distance" class="internal-link virtual-link-a">Manhattan distance</a>üîó, Euclidean distance, and Chebyshev distance.
<br><a data-href="Manhattan Distance" href="notes/math/manhattan-distance.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Manhattan Distance</a>: The shortest distance between 2 vectors using only 90¬∞ movements.
<br><a data-href="Jaccard Index" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Jaccard Index</a>: Used to compare sets and is defined as the size of the intersection of 2 sets, over the size of their union. J. Han and M. Kamber,&nbsp;Data Mining : Concepts and Techniques, 3rd ed. Amsterdam ; Boston: Elsevier/Morgan Kaufmann, 2012.
<br>GeeksforGeeks, ‚ÄúEuclidean Distance,‚Äù&nbsp;GeeksforGeeks, Mar. 13, 2024. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.geeksforgeeks.org/maths/euclidean-distance/" target="_self">https://www.geeksforgeeks.org/maths/euclidean-distance/</a>
<br>Wikipedia Contributors, ‚ÄúEuclidean distance,‚Äù&nbsp;Wikipedia, Apr. 01, 2019. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Euclidean_distance" target="_self">https://en.wikipedia.org/wiki/Euclidean_distance</a>
<br>‚Äú<a href="notes/math/least-squares.html#_0" target="_self" rel="noopener noreferrer" from="1" to="14" origin-text="Least squares" class="internal-link virtual-link-a">Least squares</a>üîó,‚Äù&nbsp;Wikipedia, Dec. 19, 2019. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Least_squares" target="_self">https://en.wikipedia.org/wiki/Least_squares</a>
<br>Maarten Grootendorst, ‚Äú9 Distance Measures in Data Science | TDS Archive,‚Äù&nbsp;Medium, Feb. 2021. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://medium.com/data-science/9-distance-measures-in-data-science-918109d069fa" target="_self">https://medium.com/data-science/9-distance-measures-in-data-science-918109d069fa</a>
]]></description><link>notes/math/euclidean-distance.html</link><guid isPermaLink="false">NOTES/Math/Euclidean Distance.md</guid><pubDate>Thu, 17 Jul 2025 00:00:00 GMT</pubDate></item><item><title><![CDATA[Variance]]></title><description><![CDATA[Variance is a <a data-href="Measure of Dispersion" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Measure of Dispersion</a> that summarizes the spread of values in a dataset from the average. It represents the average squared distance from each data point to the <a data-tooltip-position="top" aria-label="Mean" data-href="Mean" href="notes/math/mean.html#_0" class="internal-link" target="_self" rel="noopener nofollow">mean</a> of the dataset.
<br>
Low Variance: Indicates that the data points tend to be very close to the <a href="notes/math/mean.html#_0" target="_self" rel="noopener noreferrer" from="62" to="66" origin-text="mean" class="internal-link virtual-link-a">mean</a>üîó, suggesting consistency and reliability in the dataset. <br>
High Variance: Implies that data points are more spread out across the <a data-tooltip-position="top" aria-label="Range" data-href="Range" href="notes/math/range.html#_0" class="internal-link" target="_self" rel="noopener nofollow">range</a>, indicating greater variability and less predictability. <br><img alt="HighVsLowSTD.png" src="resources/highvslowstd.png" target="_self">
Non-Negative: The variance can never be negative, since it is an average distance measure and distance can also never be negative.
The variance of a numeric attribute is defined as:For a population, the variance is calculated using:Where: represents the population variance.
<br> represents the <a data-tooltip-position="top" aria-label="Mean" data-href="Mean" href="notes/math/mean.html#_0" class="internal-link" target="_self" rel="noopener nofollow">mean</a> of the population. represents the number of observations.
For a sample, the variance is calculated using:Where: represents the sample variance.
<br> represents the <a href="notes/math/mean.html#_0" target="_self" rel="noopener noreferrer" from="16" to="20" origin-text="mean" class="internal-link virtual-link-a">mean</a>üîó of the sample. represents the number of observations.
<br>The <a href="notes/math/standard-deviation.html#_0" target="_self" rel="noopener noreferrer" from="4" to="22" origin-text="standard deviation" class="internal-link virtual-link-a">standard deviation</a>üîó is equal to the square root of the variance of the same dataset. Standard deviation is expressed in the same units as the original data, while variance is expressed in squared units.<br>The expected value, or <a href="notes/math/mean.html#_0" target="_self" rel="noopener noreferrer" from="23" to="27" origin-text="mean" class="internal-link virtual-link-a">mean</a>üîó, of a random variable , denoted , is a <a data-tooltip-position="top" aria-label="Measure of Central Tendency" data-href="Measure of Central Tendency" href="notes/math/measure-of-central-tendency.html#_0" class="internal-link" target="_self" rel="noopener nofollow">measure of central tendency</a> that represents the average outcome of a random variable. Intuitively, it is the average of the outcomes of many samples from .<br>The variance of a random variable measures the dispersion of a random variable around its expected value . It is defined as the expected value of the squared differences from the <a href="notes/math/mean.html#_0" target="_self" rel="noopener noreferrer" from="74" to="78" origin-text="mean" class="internal-link virtual-link-a">mean</a>üîó:Where: is the expected value of .
<br>Covariance is vital in understanding the relationships between random variables in <a data-tooltip-position="top" aria-label="Probability" data-href="Probability" href="notes/math/probability.html#_0" class="internal-link" target="_self" rel="noopener nofollow">probability</a> distributions. The value of the covariance between two variables is represents the direction of the linear relationship between them.<br>A <a data-href="Covariance Matrix" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Covariance Matrix</a> is a square matrix containing the covariance between multiple variables.The formula for calculating the covariance is:Where:
<br> and represent the <a href="notes/math/mean.html#_0" target="_self" rel="noopener noreferrer" from="15" to="19" origin-text="mean" class="internal-link virtual-link-a">mean</a>üîó of and respectively. represents the number of samples in the dataset. Positive Covariance: Indicates that as increases, increases.
Negative Covariance: Indicates that as increases, decreases.
Zero/Near-Zero Covariance: Indicates no linear relationship between and .
Covariance is a key step in calculating correlation, which normalizes the covariance value to a standard scale. Correlation is useful for assessing the strength and direction of the relationship between two variables.<br>A dataset is homoscedastic if the variance of the residuals (errors) is constant across the <a href="notes/math/range.html#_0" target="_self" rel="noopener noreferrer" from="92" to="97" origin-text="range" class="internal-link virtual-link-a">range</a>üîó of the independent variable(s). Conversely, if the variance changes as a function of the independent variable, the dataset is heteroscedastic.<br>Homoscedasticity is an important concept in <a data-href="Regression" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Regression</a> analysis as it is an essential assumption for many regression models such as <a data-href="Linear Regression" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Linear Regression</a> based on the <a data-href="Least Squares" href="notes/math/least-squares.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Least Squares</a> method.
Residuals vs. Fitted Values Plot: A scatter plot of residuals against the predicted values. A random scatter indicates that the dataset is homoscedastic.
<br><a data-href="Breusch-Pagan Test" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Breusch-Pagan Test</a>: Creates an initial regression model, then fits a new model on the squared residuals of original model against the independent variable. If the corresponding <a data-href="P-Value" href=".html" class="internal-link" target="_self" rel="noopener nofollow">P-Value</a> from the <a data-href="Chi-Squared Test" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Chi-Squared Test</a> is less than some chosen significance level, then heteroscedasticity is assumed.
<br><a data-href="White Test" href=".html" class="internal-link" target="_self" rel="noopener nofollow">White Test</a>: A more general test which can detect non-linear heteroscedastic relationships. The methodology is similar to that of the Breusch-Pagan test, though the White test involves regressing the squared residuals against the original independent variables, their squares, and their cross-products. <br><a data-href="Weighted Least Squares" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Weighted Least Squares</a>: Weighs the effect of observations on the regression line based on its estimated variance.
<br>Transformations: Applying a <a data-href="Log Transformation" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Log Transformation</a> or <a data-href="Box-Cox Transformation" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Box-Cox Transformation</a> can potentially reduce or eliminate heteroscedasticity entirely. J. Han, M. Kamber, and J. Pei,&nbsp;Data Mining : Concepts and Techniques. Burlington, Ma: Elsevier, 2012.
<br>GeeksforGeeks, ‚ÄúVariance,‚Äù&nbsp;GeeksforGeeks, Apr. 27, 2022. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.geeksforgeeks.org/maths/variance/" target="_self">https://www.geeksforgeeks.org/maths/variance/</a>
<br>Wikipedia Contributors, ‚ÄúVariance,‚Äù&nbsp;Wikipedia, Jan. 08, 2019. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Variance" target="_self">https://en.wikipedia.org/wiki/Variance</a>
<br>J. Starmer, ‚ÄúCovariance, Clearly Explained!!!,‚Äù&nbsp;YouTube. Jul. 29, 2019. Available: <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.youtube.com/watch?v=qtaqvPAeEJY" target="_self">https://www.youtube.com/watch?v=qtaqvPAeEJY</a>
<br>GeeksforGeeks, ‚ÄúCovariance and Correlation,‚Äù&nbsp;GeeksforGeeks, Jun. 25, 2018. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.geeksforgeeks.org/engineering-mathematics/mathematics-covariance-and-correlation/" target="_self">https://www.geeksforgeeks.org/engineering-mathematics/mathematics-covariance-and-correlation/</a> (accessed Jul. 16, 2025).
<br>Z. Bobbit, ‚ÄúThe Breusch-Pagan Test: Definition &amp; Example,‚Äù&nbsp;Statology, Dec. 31, 2020. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.statology.org/breusch-pagan-test/" target="_self">https://www.statology.org/breusch-pagan-test/</a>
<br>J. Waples, ‚ÄúHeteroscedasticity: A Full Guide to Unequal Variance,‚Äù&nbsp;Datacamp.com, Dec. 10, 2024. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.datacamp.com/tutorial/heteroscedasticity" target="_self">https://www.datacamp.com/tutorial/heteroscedasticity</a>
]]></description><link>notes/math/variance.html</link><guid isPermaLink="false">NOTES/Math/Variance.md</guid><pubDate>Thu, 10 Jul 2025 00:00:00 GMT</pubDate><enclosure url="resources/highvslowstd.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;resources/highvslowstd.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Associative Property]]></title><description><![CDATA[The associative property states that the sum or product of any group of values is not affected by how the values are grouped. It allows you to transform mathematical expressions into equivalent forms without altering its value.The associative property applies to addition. For any numbers , and :The associative property applies to multiplication. For any numbers , and :The associative property applies to matrix multiplication as well. For matrices , , and with compatible dimensions:Operations such as subtraction or division are not associative:
Subtraction: Division: In <a data-href="Propositional Logic" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Propositional Logic</a>, associativity is a rule of replacement that applies to some <a data-href="Logical Connectives" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Logical Connectives</a>, allowing for the rearrangement of grouping symbols without changing the truth value of the logical expression.Associativity applies to conjunction (AND), for any variables , , and :Associativity applies to disjunction (OR), for any variables , , and :<br>In set theory, the associative property refers to how the <a data-href="Union" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Union</a> and <a data-href="Intersection" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Intersection</a> of sets can be grouped without altering the outcome of the expression.for any sets , , and :for any sets , , and :<br>The <a href="notes/math/commutative-property.html#_0" target="_self" rel="noopener noreferrer" from="4" to="24" origin-text="commutative property" class="internal-link virtual-link-a">commutative property</a>üîó states of values does not effect their sum or product.
Addition: Multiplication: <br>Note that the <a href="notes/math/commutative-property.html#_0" target="_self" rel="noopener noreferrer" from="14" to="34" origin-text="commutative property" class="internal-link virtual-link-a">commutative property</a>üîó does not apply to matrix multiplication.
<br>GeeksforGeeks, ‚ÄúAssociative Property,‚Äù&nbsp;GeeksforGeeks, Sep. 29, 2023. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.geeksforgeeks.org/maths/associative-property/" target="_self">https://www.geeksforgeeks.org/maths/associative-property/</a> (accessed Jul. 19, 2025)
<br>‚ÄúAssociative property,‚Äù&nbsp;Wikipedia, Jan. 16, 2023. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Associative_property" target="_self">https://en.wikipedia.org/wiki/Associative_property</a>
]]></description><link>notes/math/associative-property.html</link><guid isPermaLink="false">NOTES/Math/Associative Property.md</guid><pubDate>Sat, 19 Jul 2025 00:00:00 GMT</pubDate></item><item><title><![CDATA[Standard Deviation]]></title><description><![CDATA[Standard deviation is a <a data-href="Measure of Dispersion" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Measure of Dispersion</a> that summarizes the amount variation in a dataset. It represents the average distance between each data point and the <a data-href="Mean" href="notes/math/mean.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Mean</a> of the population.
<br>
Low Standard Deviation: Indicates that the data points tend to be very close to the <a href="notes/math/mean.html#_0" target="_self" rel="noopener noreferrer" from="62" to="66" origin-text="mean" class="internal-link virtual-link-a">mean</a>üîó, suggesting consistency and reliability in the dataset. <br>
High Standard Deviation: Implies that data points are more spread out across the <a data-tooltip-position="top" aria-label="Range" data-href="Range" href="notes/math/range.html#_0" class="internal-link" target="_self" rel="noopener nofollow">range</a>, indicating greater variability and less predictability. <br><img alt="HighVsLowSTD.png" src="resources/highvslowstd.png" target="_self">
Non-Negative: The standard deviation can never be negative, since it is an average distance measure and distance can also never be negative.
<br>Sensitive to <a data-tooltip-position="top" aria-label="Outlier" data-href="Outlier" href="notes/math/outlier.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Outliers</a>: Extreme <a href="notes/math/outlier.html#_0" target="_self" rel="noopener noreferrer" from="10" to="18" origin-text="outliers" class="internal-link virtual-link-a">outliers</a>üîó can have a significant impact on the standard deviation.
Same Units: The standard deviation is expressed in the same units as the underlying dataset.
The standard deviation of a numeric attribute , denoted with , is defined as:For a population, the standard deviation is calculated using:Where:
<br> represents the <a href="notes/math/mean.html#_0" target="_self" rel="noopener noreferrer" from="16" to="20" origin-text="mean" class="internal-link virtual-link-a">mean</a>üîó of the population. represents the number of observations.
For a sample, the standard deviation is calculated using:Where:
<br> represents the <a href="notes/math/mean.html#_0" target="_self" rel="noopener noreferrer" from="16" to="20" origin-text="mean" class="internal-link virtual-link-a">mean</a>üîó of the sample. represents the number of observations.
<br>The standard deviation is equal to the square root of the <a data-href="Variance" href="notes/math/variance.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Variance</a> of the same dataset. <a href="notes/math/variance.html#_0" target="_self" rel="noopener noreferrer" from="22" to="30" origin-text="Variance" class="internal-link virtual-link-a">Variance</a>üîó measures the average of the squared differences from the <a href="notes/math/mean.html#_0" target="_self" rel="noopener noreferrer" from="88" to="92" origin-text="mean" class="internal-link virtual-link-a">mean</a>üîó, providing insight into the spread of the data.Other common measures of dispersion include:
<br><a data-href="Interquartile Range" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Interquartile Range</a> (IQR): which is the distance covered by the middle 50% of the dataset.
<br><a data-href="Range" href="notes/math/range.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Range</a>: which is the difference between the maximum and minimum values in a dataset
<br><a data-tooltip-position="top" aria-label="Quartile" data-href="Quartile" href="notes/math/quartile.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Quartiles</a>: which are the three values that divide a dataset into four equal parts. J. Han, M. Kamber, and J. Pei,&nbsp;Data Mining : Concepts and Techniques. Burlington, Ma: Elsevier, 2012.
<br>GeeksforGeeks, ‚ÄúStandard Deviation Formula, Examples &amp; How to Calculate,‚Äù&nbsp;GeeksforGeeks, Jul. 06, 2022. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.geeksforgeeks.org/maths/standard-deviation-formula/" target="_self">https://www.geeksforgeeks.org/maths/standard-deviation-formula/</a>
<br>J. Frost, ‚ÄúStandard Deviation: Interpretations and Calculations,‚Äù&nbsp;Statistics By Jim, 2024. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://statisticsbyjim.com/basics/standard-deviation/" target="_self">https://statisticsbyjim.com/basics/standard-deviation/</a>
]]></description><link>notes/math/standard-deviation.html</link><guid isPermaLink="false">NOTES/Math/Standard Deviation.md</guid><pubDate>Thu, 10 Jul 2025 00:00:00 GMT</pubDate><enclosure url="resources/highvslowstd.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;resources/highvslowstd.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Commutative Property]]></title><description><![CDATA[The commutative property states that the sums and products of values is unaffected by the order those values come in. It allows you to transform mathematical expressions into equivalent forms without altering its value.The commutative property applies to addition. For any numbers , and :The commutative property applies to multiplication. For any numbers , and :Operations such as subtraction or division are not commutative:
Subtraction: , subtraction is actually <a data-href="Anti-Commutative" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Anti-Commutative</a>, meaning, Division: Exponentiation: Matrix Multiplication: For matrices , , and with compatible dimensions: The anti-commutative property refers to specific operations where switching the order of arguments negates the result of the expression.Subtraction is an anti-commutative operation. Meaning, for any 2 values and :<br>Some <a data-href="Logical Connectives" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Logical Connectives</a> are commutative, allowing for the rearrangement of variables without changing the truth value of the logical expression.The commutative property applies to conjunction (AND), for any variables and :The commutative property applies to disjunction (OR), for any variables and :<br>In set theory, the commutative property refers to how the <a data-href="Union" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Union</a> and <a data-href="Intersection" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Intersection</a> of sets are unaffected by the order of the sets they are applied to.for any sets , and :for any sets , and :<br>The <a href="notes/math/associative-property.html#_0" target="_self" rel="noopener noreferrer" from="4" to="24" origin-text="associative property" class="internal-link virtual-link-a">associative property</a>üîó states that the sum or product of any group of values is not affected by how the values are grouped.
Addition: Multiplication: <br>‚ÄúCommutative property,‚Äù&nbsp;Wikipedia, Dec. 04, 2020. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Commutative_property" target="_self">https://en.wikipedia.org/wiki/Commutative_property</a>
<br>GeeksforGeeks, ‚ÄúCommutative Property Definition | Commutative Law and Examples,‚Äù&nbsp;GeeksforGeeks, Dec. 28, 2023. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.geeksforgeeks.org/maths/commutative-property/" target="_self">https://www.geeksforgeeks.org/maths/commutative-property/</a> (accessed Jul. 20, 2025).
]]></description><link>notes/math/commutative-property.html</link><guid isPermaLink="false">NOTES/Math/Commutative Property.md</guid><pubDate>Sun, 20 Jul 2025 00:00:00 GMT</pubDate></item><item><title><![CDATA[Quantile]]></title><description><![CDATA[Quantiles are points in a dataset which divide the dataset into equal parts. Some examples of quantiles include the:
<a data-href="Median" href="notes/math/median.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Median</a>: which divides the dataset into two equal parts;
<br><a data-tooltip-position="top" aria-label="Quartile" data-href="Quartile" href="notes/math/quartile.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Quartiles</a>: which divide the dataset into four equal parts; and <br><a data-href="Percentiles" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Percentiles</a>: which divide the dataset into 100 equal parts.
<br><img alt="Quartiles &amp; Quantiles | Calculation, Definition &amp; Interpretation" src="https://www.scribbr.com/wp-content/uploads/2022/05/Quartiles-probability-distribution.webp" referrerpolicy="no-referrer" target="_self" class="is-unresolved"><br>
<a data-tooltip-position="top" aria-label="https://www.scribbr.com/statistics/quartiles-quantiles/" rel="noopener nofollow" class="external-link is-unresolved" href="https://www.scribbr.com/statistics/quartiles-quantiles/" target="_self">Quartiles &amp; Quantiles | Calculation, Definition &amp; Interpretation</a>Q-quantiles are the values which divide a dataset into equal (or nearly equal) parts. The 100-quantiles (percentiles), for example, divide the dataset into 100 parts.To divide a dataset into equal parts:
Sort the dataset in ascending order.
Calculate the position of the the th quantile using: <br>If is an integer, the quantile is the value at that position in the sorted dataset. If is not an integer, <a data-href="Interpolate" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Interpolate</a> it, that is, round it up, and take the <a data-href="Mean" href="notes/math/mean.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Mean</a> of the values at positions and .
NumPy has multiple functions for computing quantiles including:
numpy.percentile(): Which takes a dataset and percentile (e.g. 50) as arguments, and returns the value at that percentile.
numpy.quantile(): Which takes a dataset and a decimal value representing the percentile (e.g. 0.50) as input, and returns the value at that quantile.
<br>Quantiles are commonly used to summarize the distribution of a dataset, and is commonly used as both a <a data-tooltip-position="top" aria-label="Measure of Central Tendency" data-href="Measure of Central Tendency" href="notes/math/measure-of-central-tendency.html#_0" class="internal-link" target="_self" rel="noopener nofollow">measure of central tendency</a> and a <a data-href="Measure of Dispersion" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Measure of Dispersion</a>.<br>Quantiles are commonly used to identify <a href="notes/math/outlier.html#_0" target="_self" rel="noopener noreferrer" from="40" to="48" origin-text="outliers" class="internal-link virtual-link-a">outliers</a>üîó. One method involves flagging any observations that are more than 1.5 times the <a data-href="Interquartile Range" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Interquartile Range</a> (IQR) above the third <a href="notes/math/quartile.html#_0" target="_self" rel="noopener noreferrer" from="23" to="31" origin-text="quartile" class="internal-link virtual-link-a">quartile</a>üîó or below the first quartile.
<br>
<a data-href="Box Plot" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Box Plot</a>: Visualizes the <a data-href="Five-Number Summary" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Five-Number Summary</a> of a dataset, where the box represents the IQR, the line in the box is the <a data-href="Median" href="notes/math/median.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Median</a>, the "whiskers" represent the 25% of the data below and above the first and third <a href="notes/math/quartile.html#_0" target="_self" rel="noopener noreferrer" from="83" to="91" origin-text="quartile" class="internal-link virtual-link-a">quartile</a>üîó respectively, and the lines at the edge of each whisker represents the minimum and maximum values. <br>
<a data-href="Q-Q Plot" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Q-Q Plot</a>: A comparative visualization method which plots the quantiles of two distributions against each other, where typically, a real dataset is plotted against a theoretical dataset (usually a normal distribution). <br>
<a data-href="Violin Plot" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Violin Plot</a>: Overlays density curves over a box plot, where the width of the curve indicates the density of data points at specific values. J. Han, M. Kamber, and J. Pei,&nbsp;Data Mining : Concepts and Techniques. Burlington, Ma: Elsevier, 2012.
<br>GeeksforGeeks, ‚ÄúQuantiles in Machine Learning,‚Äù&nbsp;GeeksforGeeks, Mar. 12, 2024. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.geeksforgeeks.org/data-science/quantiles-in-machine-learning/" target="_self">https://www.geeksforgeeks.org/data-science/quantiles-in-machine-learning/</a>
<br>‚ÄúQuantile,‚Äù&nbsp;Wikipedia, Dec. 12, 2020. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Quantile" target="_self">https://en.wikipedia.org/wiki/Quantile</a>
<br>Atlassian, ‚ÄúA Complete Guide to Violin Plots,‚Äù&nbsp;Atlassian. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.atlassian.com/data/charts/violin-plot-complete-guide" target="_self">https://www.atlassian.com/data/charts/violin-plot-complete-guide</a>
]]></description><link>notes/math/quantile.html</link><guid isPermaLink="false">NOTES/Math/Quantile.md</guid><pubDate>Wed, 16 Jul 2025 00:00:00 GMT</pubDate><enclosure url="https://www.scribbr.com/wp-content/uploads/2022/05/Quartiles-probability-distribution.webp" length="0" type="image/webp"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;https://www.scribbr.com/wp-content/uploads/2022/05/Quartiles-probability-distribution.webp&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Range]]></title><description><![CDATA[The range is a <a data-href="Measure of Dispersion" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Measure of Dispersion</a> that represents the difference between the smallest and largest value in a dataset. The range is calculated as:
Easy to compute: Very easy value to calculate even by hand.
Intuitive: Illustrates the spread of the data by a simple value that is in the same scale as the underlying dataset making it easy to understand. <br>Sensitive to <a data-tooltip-position="top" aria-label="Outlier" data-href="Outlier" href="notes/math/outlier.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Outliers</a>: The range, in the worst case, can be entirely determined by two <a href="notes/math/outlier.html#_0" target="_self" rel="noopener noreferrer" from="66" to="74" origin-text="outliers" class="internal-link virtual-link-a">outliers</a>üîó that do not accurately represent the underlying dataset, making the range a misleading metric in such a case.
Vague: Does not provide much information regarding the distribution of a population, making it very uninformative. <br><a data-href="Interquartile Range" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Interquartile Range</a>: The IQR of a dataset is the difference between the first <a data-tooltip-position="top" aria-label="Quartile" data-href="Quartile" href="notes/math/quartile.html#_0" class="internal-link" target="_self" rel="noopener nofollow">quartile</a> , and the third <a href="notes/math/quartile.html#_0" target="_self" rel="noopener noreferrer" from="16" to="24" origin-text="quartile" class="internal-link virtual-link-a">quartile</a>üîó . It provides the spread of the middle 50% of the data.
<br><a data-href="Midrange" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Midrange</a>: The midrange of a dataset is the <a data-tooltip-position="top" aria-label="Mean" data-href="Mean" href="notes/math/mean.html#_0" class="internal-link" target="_self" rel="noopener nofollow">mean</a> of the maximum and minimum values. It is calculated as: J. Han, M. Kamber, and J. Pei,&nbsp;Data Mining : Concepts and Techniques. Burlington, Ma: Elsevier, 2012.
<br>GeeksforGeeks, ‚ÄúRange in Statistics,‚Äù&nbsp;GeeksforGeeks, Oct. 08, 2023. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.geeksforgeeks.org/maths/range-in-statistics/" target="_self">https://www.geeksforgeeks.org/maths/range-in-statistics/</a>
]]></description><link>notes/math/range.html</link><guid isPermaLink="false">NOTES/Math/Range.md</guid><pubDate>Tue, 15 Jul 2025 00:00:00 GMT</pubDate></item><item><title><![CDATA[Mean]]></title><description><![CDATA[The mean is a <a data-href="Measure of Central Tendency" href="notes/math/measure-of-central-tendency.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Measure of Central Tendency</a> which attempts to summarize an entire dataset with a single number. It provides an illustration of the average value within a collection of values, making it essential for data analysis tasks. The mean can only be applied to <a data-href="Numerical Data" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Numerical Data</a> and not <a data-href="Categorical Data" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Categorical Data</a>.<br>There are several types of means, each suited for different applications and fields. The most commonly used type is the arithmetic mean, which is calculated by summing all values and dividing by the number of observations. Other variations include the geometric mean, often used in financial contexts, the harmonic mean, which is beneficial in situations involving rates, and <a data-href="Root-Mean Square" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Root-Mean Square</a>, often used to measure the average voltage of an AC source.<br>Other common <a data-tooltip-position="top" aria-label="Measure of Central Tendency" data-href="Measure of Central Tendency" href="notes/math/measure-of-central-tendency.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Measures of Central Tendency</a> include the <a data-href="Median" href="notes/math/median.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Median</a>, which represents the middle value when the data is ordered, the <a data-href="Mode" href="notes/math/mode.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Mode</a>, which identifies the most frequently occurring value in a set, and the <a data-href="Midrange" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Midrange</a>, calculated as the average of the maximum and minimum values.<br>The arithmetic mean, commonly referred to as the average, is the sum of all values in a dataset divided by the number of values. It is the most widely used <a data-href="Measure of Central Tendency" href="notes/math/measure-of-central-tendency.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Measure of Central Tendency</a>.There are two types of arithmetic means:
Sample Mean (): This represents the average value of a subset drawn from a larger population.
Group Mean (): This denotes the average of values within a specific category or attribute of a dataset.
The formula for calculating the sample mean is:Where: is the sample mean. is the number of values in the dataset.
<br> is the value of an individual object at <a href="index.html#_0" target="_self" rel="noopener noreferrer" from="41" to="46" origin-text="index" class="internal-link virtual-link-a">index</a>üîó i.
<br>The arithmetic mean is commonly used in statistics the summarize datasets and provide a simple <a data-href="Measure of Central Tendency" href="notes/math/measure-of-central-tendency.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Measure of Central Tendency</a>. Though an arithmetic mean is susceptible to <a href="notes/math/outlier.html#_0" target="_self" rel="noopener noreferrer" from="46" to="54" origin-text="outliers" class="internal-link virtual-link-a">outliers</a>üîó, making it a less relevant metric for skewed datasets.<br>The trimmed mean is an arithmetic mean which discards a specified number of values from both ends of the value <a data-href="range" href="notes/math/range.html#_0" class="internal-link" target="_self" rel="noopener nofollow">range</a>. This is done to minimize the effect of <a data-tooltip-position="top" aria-label="Outlier" data-href="Outlier" href="notes/math/outlier.html#_0" class="internal-link" target="_self" rel="noopener nofollow">outliers</a> on the mean, which can skew the results. The trimmed mean generally gives a more accurate representation of the center making it particularly useful in datasets prone to <a href="notes/math/outlier.html#_0" target="_self" rel="noopener noreferrer" from="171" to="179" origin-text="outliers" class="internal-link virtual-link-a">outliers</a>üîó or extreme variations.<br>The <a data-href="Interquartile" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Interquartile</a> Mean is a specific type of trimmed mean that excludes the first and last <a data-href="Quartile" href="notes/math/quartile.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Quartile</a> of ordered data. This results in the average of the middle 50% of values, offering a robust <a data-href="measure of central tendency" href="notes/math/measure-of-central-tendency.html#_0" class="internal-link" target="_self" rel="noopener nofollow">measure of central tendency</a> that is less influenced by extreme values.The formula for the trimmed mean is:The formula for the Interquartile mean is:<br>The trimmed mean is useful for analyzing skewed datasets, or datasets that have large amount of <a href="notes/math/outlier.html#_0" target="_self" rel="noopener noreferrer" from="96" to="104" origin-text="outliers" class="internal-link virtual-link-a">outliers</a>üîó. The trimmed mean offers a more stable measure that is less affected by outliers, making it valuable across various disciplines.In a weighted mean, instead of each value contributing equally like in an arithmetic mean, each value is assigned a weight (or coefficient) based on its significance. This allows for a more nuanced average that reflects the importance of different contributions. The arithmetic mean is a weighted mean where all weights are equal.The formula for calculating the weighted mean is:<br>The weighted mean is used when there is a need to model the relative importance of various attributes. This is commonly seen in artificial intelligence techniques such as <a data-href="Ensemble Machine Learning" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Ensemble Machine Learning</a> algorithms.The harmonic mean calculates the average of a set of numbers that are defined in relation to some unit. It is calculated by taking the reciprocal of the arithmetic mean of the reciprocals of each value in the dataset.The formula for the harmonic mean is:<br>In <a data-href="Machine Learning" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Machine Learning</a>, the harmonic mean is commonly used to calculate the <a data-href="F1 Score" href=".html" class="internal-link" target="_self" rel="noopener nofollow">F1 Score</a> of a model. It is also commonly used when analyzing speed and rates, such as finding the average speed of multiple segments of a journey.<br>The geometric mean calculates the average of a set of values by using the product of a values rather than their sum. It involves multiplying all the values in the dataset and then taking the th root of that product, where is the total number of values in the set. The geometric mean is less susceptible to <a data-tooltip-position="top" aria-label="Outlier" data-href="Outlier" href="notes/math/outlier.html#_0" class="internal-link" target="_self" rel="noopener nofollow">outliers</a> than the arithmetic mean since each value is part of a product rather than a sum.It is called the geometric mean because it's commonly used to find the side length of a square that has the same area as a rectangle with given side lengths. For example, if you have a rectangle with dimensions , the length of a square with equal volume is the geometric mean of and , which is .The formula for the geometric mean is:<br>The geometric mean is commonly used to calculate average rates of return on investments over time. The geometric mean can also be employed in <a data-href="Data Normalization" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Data Normalization</a> to normalize features, particularly when the values span multiple orders of magnitude.The root mean square (RMS), often denoted as , is an average of the magnitude of a set of values, and is and is also known as the quadratic mean. It is particularly useful in sets with values of both positive and negative numbers. RMS is calculated by taking the square root of the arithmetic average of the squared values.For a continuous function defined over the interval , the RMS is determined by squaring the function, averaging the squared values over the interval, and then taking the square root of that average.The formula for discrete RMS is:The formula for continuous RMS is:Root-mean square is widely used in signal processing the measure the power of AC currents and voltages. In regression analysis, the RMS error is a common metric for evaluation the performance of a model. RMS is also used to evaluate the performance of control systems regarding the stability and responsiveness to input signals. RMS can also be used in optimization algorithms for neural networks, such as RMSprop.<br>The mean of a <a data-href="Probability Distribution" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Probability Distribution</a> represents the average outcome of some <a data-href="Random Variable" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Random Variable</a>. It is a specific type of weighted mean where each outcome of some random variable is weighted by the <a data-href="Probability" href="notes/math/probability.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Probability</a> of that outcome occurring.<br>The <a data-href="Expected Value" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Expected Value</a> of a random variable, denoted as , is the weighted average of its possible outcomes. This concept is crucial in various fields, including artificial intelligence, especially in the context of stochastic task environments, where uncertainty plays a significant role.<br>For discrete <a href="notes/math/probability.html#_0" target="_self" rel="noopener noreferrer" from="13" to="24" origin-text="probability" class="internal-link virtual-link-a">probability</a>üîó distributions, the expected value is defined as:<br>Where is the <a data-href="Probability Mass Function" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Probability Mass Function</a>.<br>For continuous <a href="notes/math/probability.html#_0" target="_self" rel="noopener noreferrer" from="15" to="26" origin-text="probability" class="internal-link virtual-link-a">probability</a>üîó distributions, the expected value is defined as:<br>Where is the <a data-href="Probability Density Function" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Probability Density Function</a>.<br>The expected value is a fundamental metric in decision theory for AI systems, guiding the decision-making process by allowing agents to evaluate actions based on their expected rewards. Expected value is also important in game theory for evaluating strategies based on their expected payoffs. In <a data-href="Machine Learning" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Machine Learning</a>, the expected value is used for supervised techniques, particularly for loss functions.<br>The mean of a continuous function over a specific interval is defined as the integral of the function divided by the length of the interval. <a data-href="Root-Mean Square" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Root-Mean Square</a> is a type of mean of a function.The formula for the mean of a continuous function defined over the interval is:<br>The mean of a function has many applications, such as in statistics, where it provides the <a data-href="measure of central tendency" href="notes/math/measure-of-central-tendency.html#_0" class="internal-link" target="_self" rel="noopener nofollow">measure of central tendency</a> for continuous random variables. It is also commonly applied in machine learning when performing feature engineering, such as when normalizing data.
<br>S. Glen, ‚ÄúMean, <a href="notes/math/median.html#_0" target="_self" rel="noopener noreferrer" from="16" to="22" origin-text="Median" class="internal-link virtual-link-a">Median</a>üîó, <a href="notes/math/mode.html#_0" target="_self" rel="noopener noreferrer" from="24" to="28" origin-text="Mode" class="internal-link virtual-link-a">Mode</a>üîó: What They Are, How to Find Them,‚Äù&nbsp;Statistics How To, 2022. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/" target="_self">https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/</a>
<br>Wikipedia contributors, ‚ÄúMean,‚Äù Wikipedia, Apr. 25, 2025. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Mean" target="_self">https://en.wikipedia.org/wiki/Mean</a>
<br>J. Frost, ‚ÄúWhat is the Mean in Statistics?,‚Äù&nbsp;Statistics By Jim, Aug. 21, 2021. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://statisticsbyjim.com/basics/mean_average/" target="_self">https://statisticsbyjim.com/basics/mean_average/</a>
J. Han and M. Kamber,&nbsp;Data Mining : Concepts and Techniques, 3rd ed. Amsterdam ; Boston: Elsevier/Morgan Kaufmann, 2012.
<br>‚ÄúExpectation | Mean | Average,‚Äù&nbsp;<a data-tooltip-position="top" aria-label="http://www.probabilitycourse.com" rel="noopener nofollow" class="external-link is-unresolved" href="http://www.probabilitycourse.com" target="_self">www.probabilitycourse.com</a>. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.probabilitycourse.com/chapter3/3_2_2_expectation.php" target="_self">https://www.probabilitycourse.com/chapter3/3_2_2_expectation.php</a>
‚Äå]]></description><link>notes/math/mean.html</link><guid isPermaLink="false">NOTES/Math/Mean.md</guid><pubDate>Fri, 04 Jul 2025 00:00:00 GMT</pubDate></item><item><title><![CDATA[Turing Test]]></title><description><![CDATA[A Turing test is a benchmark proposed by Alan Turing for evaluating a machine's ability to exhibit intelligence that is indistinguishable from that of a human. The test involves three participants, a human interrogator, a human respondent, and a machine. The interrogator communicates with both the human and machine via text, if the judge is unable to confidently identify which agent is the human and which is the machine, then the machine passes the Turing test. The result does not depend on the machine's ability to respond correctly, , only on how close the machine's language resembles human natural language. The CAPTCHA system is a famous implementation of a Turing test.To pass a Turing test a machine must have <a data-href="Natural Language Processing" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Natural Language Processing</a> and generating abilities. The machine also needs some form of <a data-href="Knowledge Representation" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Knowledge Representation</a> to be able to keep track of conversation, as well as <a data-href="Reasoning" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Reasoning</a>, so that the machine demonstrates some form of rationality to the interrogator.<br>There is an extension of the Turing test known as the total Turing test, where the machine has to also exhibit human sensory abilities and physical interactions. In this instance, the machine would also need robust <a data-href="Robotics" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Robotics</a>, <a data-href="Computer Vision" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Computer Vision</a>, <a data-href="Speech Recognition" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Speech Recognition</a>, navigation, and physical manipulation capabilities as well as a physical appearance that is indistinguishable from a human.One of the strengths of the Turing test lies in its simplicity. The test is straightforward to implement and understand which encourages broader participation in the realm of AI research.The Turing test is particularly versatile, allowing it to be applied to various forms of AI models, making it relevant across sub-disciplines within artificial intelligence research. The evaluation metrics of the test are also simple to expand or contract if specific capabilities would like to be included or excluded- such as computer vision or robotics. This allows researchers to tailor their assessment based on the goals of their project, rather than trying to conform to specific parameters of the test.Due to its subjective nature, the Turing Test is susceptible to human variability, as different interrogators may perceive specific behaviors as "human-like" or intelligent in varying ways. This inherent subjectivity can lead to inconsistent results, which undermines the test's reliability as a benchmark for evaluating artificial intelligence systems. As a result, the effectiveness and usefulness of the Turing test as a measure for machine intelligence has been heavily scrutinized by various researchers in the field of AI.Since the Turing Test focuses primarily on actions and behaviors, it overlooks the underlying processes that lead to the formulation of those behaviors, raising questions about whether those processes truly reflect intelligence. Many computer scientists argue that analyzing actions and behaviors alone is insufficient for determining whether a machine possesses intelligence. This critique emphasizes the need for a more comprehensive evaluation that considers not only observable outputs but also the mechanisms which deliver those outputs.Some critics of the Turing Test argue that the focus of AI research should be on augmenting or improving human behavior rather than merely mimicking it. They point out that certain human behaviors are inherently unintelligent, while some intelligent behaviors may be fundamentally inhuman. For example, a machine might deliberately avoid providing a correct answer to a challenging mathematical question to avoid raising suspicion with the interrogator. Some believe that this approach represents a misguided use of research efforts, diverting attention from the potential of AI to enhance human capabilities and solve complex problems.
<br>Wikipedia contributors, ‚ÄúTuring test,‚Äù Wikipedia, Jun. 24, 2025. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Turing_test#Tractability_and_simplicity" target="_self">https://en.wikipedia.org/wiki/Turing_test#Tractability_and_simplicity</a>
<br>GeeksforGeeks, ‚ÄúTuring Test in artificial intelligence,‚Äù GeeksforGeeks, Sep. 16, 2024. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.geeksforgeeks.org/artificial-intelligence/turing-test-artificial-intelligence/" target="_self">https://www.geeksforgeeks.org/artificial-intelligence/turing-test-artificial-intelligence/</a>
Peter. R. Norvig, Artificial Intelligence: A Modern Approach, Global Edition. 2021.
]]></description><link>notes/ai/turing-test.html</link><guid isPermaLink="false">NOTES/AI/Turing Test.md</guid><pubDate>Wed, 02 Jul 2025 00:00:00 GMT</pubDate></item><item><title><![CDATA[Measure of Central Tendency]]></title><description><![CDATA[A measure of central tendency is a statistical measure that attempts to describe the center of a dataset. The measure attempts to summarize the dataset with a single value that represents the middle or "average" of the data. The most common measures are the <a data-href="Mean" href="notes/math/mean.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Mean</a>, <a data-href="Median" href="notes/math/median.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Median</a>, and <a data-href="Mode" href="notes/math/mode.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Mode</a>. Depending on the characteristics of the underlying dataset, one measure may be more appropriate than the others.<br> <img alt="Pasted image 20250708171735.png" src="resources/pasted-image-20250708171735.png" target="_self"><br>
<a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.studyforfe.com/blog/measures-of-central-tendencies-and-dispersions" target="_self">https://www.studyforfe.com/blog/measures-of-central-tendencies-and-dispersions</a><br>The <a href="notes/math/mean.html#_0" target="_self" rel="noopener noreferrer" from="4" to="8" origin-text="mean" class="internal-link virtual-link-a">mean</a>üîó, often referred to as the average, is one of the most widely used measures of central tendency. There are various types of means, with the arithmetic mean being the most common. The arithmetic mean is calculated by summing all values in a dataset and dividing by the number of values.<br>The sample <a href="notes/math/mean.html#_0" target="_self" rel="noopener noreferrer" from="11" to="15" origin-text="mean" class="internal-link virtual-link-a">mean</a>üîó, denoted as , is defined by the formula:Where is the number of values in the dataset.
<br>The <a href="notes/math/mean.html#_0" target="_self" rel="noopener noreferrer" from="4" to="8" origin-text="mean" class="internal-link virtual-link-a">mean</a>üîó is sensitive to <a data-tooltip-position="top" aria-label="Outlier" data-href="Outlier" href="notes/math/outlier.html#_0" class="internal-link" target="_self" rel="noopener nofollow">outliers</a>, so it may not provide an accurate representation of the center when the underlying dataset is asymmetric, or has many extreme <a href="notes/math/outlier.html#_0" target="_self" rel="noopener noreferrer" from="128" to="136" origin-text="outliers" class="internal-link virtual-link-a">outliers</a>üîó. For symmetric distributions, the mean is a useful measure that provides the average value of the dataset.
The mean is often used in algorithms such as linear regression, where it helps minimize error in predictions.
<br>The <a href="notes/math/median.html#_0" target="_self" rel="noopener noreferrer" from="4" to="10" origin-text="median" class="internal-link virtual-link-a">median</a>üîó is the middle value of a dataset when it is ordered. If there is an even number of values, then the <a href="notes/math/mean.html#_0" target="_self" rel="noopener noreferrer" from="111" to="115" origin-text="mean" class="internal-link virtual-link-a">mean</a>üîó (average) of the two middlemost values are taken.<br>If is even, the formula for the <a href="notes/math/median.html#_0" target="_self" rel="noopener noreferrer" from="30" to="36" origin-text="median" class="internal-link virtual-link-a">median</a>üîó is:<br>While if is odd, the formula for the <a href="notes/math/median.html#_0" target="_self" rel="noopener noreferrer" from="29" to="35" origin-text="median" class="internal-link virtual-link-a">median</a>üîó is:
<br>The <a href="notes/math/median.html#_0" target="_self" rel="noopener noreferrer" from="4" to="10" origin-text="median" class="internal-link virtual-link-a">median</a>üîó is less susceptible to <a href="notes/math/outlier.html#_0" target="_self" rel="noopener noreferrer" from="34" to="42" origin-text="outliers" class="internal-link virtual-link-a">outliers</a>üîó than the <a href="notes/math/mean.html#_0" target="_self" rel="noopener noreferrer" from="52" to="56" origin-text="mean" class="internal-link virtual-link-a">mean</a>üîó, therefore, it provides a more accurate measure of the center for skewed distributions.
<br>In data science and machine learning, the <a href="notes/math/mode.html#_0" target="_self" rel="noopener noreferrer" from="42" to="46" origin-text="mode" class="internal-link virtual-link-a">mode</a>üîó is useful for categorical variables, such as determining the most common class label in classification tasks.
<br>The <a href="notes/math/mode.html#_0" target="_self" rel="noopener noreferrer" from="4" to="8" origin-text="mode" class="internal-link virtual-link-a">mode</a>üîó of a dataset is the value that appears most frequently. A dataset can have:
<br>One <a href="notes/math/mode.html#_0" target="_self" rel="noopener noreferrer" from="4" to="8" origin-text="mode" class="internal-link virtual-link-a">mode</a>üîó (unimodal)
Two modes (bimodal)
Multiple modes (multimodal)
<br>The <a href="notes/math/mode.html#_0" target="_self" rel="noopener noreferrer" from="4" to="8" origin-text="mode" class="internal-link virtual-link-a">mode</a>üîó is useful for analyzing <a data-href="Nominal Data" href="notes/math/nominal-data.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Nominal Data</a>, as it helps identify the most "popular" category within a given set of values.<br>The <a href="notes/math/mode.html#_0" target="_self" rel="noopener noreferrer" from="4" to="8" origin-text="mode" class="internal-link virtual-link-a">mode</a>üîó can be defined as:Where is the frequency of the value in a given dataset.
<br>Unlike the <a href="notes/math/mean.html#_0" target="_self" rel="noopener noreferrer" from="11" to="15" origin-text="mean" class="internal-link virtual-link-a">mean</a>üîó or <a href="notes/math/median.html#_0" target="_self" rel="noopener noreferrer" from="19" to="25" origin-text="median" class="internal-link virtual-link-a">median</a>üîó, the <a href="notes/math/mode.html#_0" target="_self" rel="noopener noreferrer" from="31" to="35" origin-text="mode" class="internal-link virtual-link-a">mode</a>üîó can be directly applied to <a data-href="Categorical Data" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Categorical Data</a>, making it one of the most simple and versatile measures of central tendency. J. Han and M. Kamber,&nbsp;Data Mining : Concepts and Techniques, 3rd ed. Amsterdam ; Boston: Elsevier/Morgan Kaufmann, 2012.
<br>Laerd Statistics, ‚ÄúMeasures of central tendency,‚Äù&nbsp;Laerd Statistics, 2018. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://statistics.laerd.com/statistical-guides/measures-central-tendency-mean-mode-median.php" target="_self">https://statistics.laerd.com/statistical-guides/measures-central-tendency-mean-mode-median.php</a>
<br>‚ÄúCentral tendency,‚Äù&nbsp;Wikipedia, Jul. 13, 2020. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Central_tendency" target="_self">https://en.wikipedia.org/wiki/Central_tendency</a>
]]></description><link>notes/math/measure-of-central-tendency.html</link><guid isPermaLink="false">NOTES/Math/Measure of Central Tendency.md</guid><pubDate>Sun, 06 Jul 2025 00:00:00 GMT</pubDate><enclosure url="." length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;.&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Quartile]]></title><description><![CDATA[Quartiles are a type of <a data-href="Quantile" href="notes/math/quantile.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Quantile</a> that divide an ordered dataset into 4 equal parts. The quartiles of a dataset are three values, where each value represents a certain <a data-href="Percentile" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Percentile</a> of the data. Quartiles are a <a data-href="Measure of Dispersion" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Measure of Dispersion</a>, meaning it assesses the spread of a population.
<br>First Quartile (Q1): The 25th percentile, meaning that 25% of the data falls below the first quartile. It can be thought of as the <a data-href="Median" href="notes/math/median.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Median</a> of the lower half of the data.
<br>Second Quartile (Q2): The 50th percentile, or the <a href="notes/math/median.html#_0" target="_self" rel="noopener noreferrer" from="31" to="37" origin-text="median" class="internal-link virtual-link-a">median</a>üîó of the dataset. 50% of the data falls below the second quartile.
Third Quartile (Q3): The 75th percentile, 75% of the data falls below the third quartile. It can be thought of as the median of the upper half of the dataset.
The five-number summary is a set of five values that describe the distribution of a dataset or population and consists of the following:
Minimum: The smallest value in the dataset.
First Quartile (Q1): The 25th percentile.
<br><a href="notes/math/median.html#_0" target="_self" rel="noopener noreferrer" from="0" to="6" origin-text="Median" class="internal-link virtual-link-a">Median</a>üîó (Q2): The 50th percentile.
Third Quartile (Q3): The 75th percentile.
Maximum: The largest value in the dataset.<br>
The five-number summary can be visualized using a <a data-href="Boxplot" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Boxplot</a> and is useful for identifying potential <a data-tooltip-position="top" aria-label="Outlier" data-href="Outlier" href="notes/math/outlier.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Outliers</a> in a population.
The calculate the quartiles of a dataset, follow these steps:
Sort the data, typically in ascending order.
<br>Find the median (Q2), which divides the dataset in half. If there‚Äôs an odd number of data points, exclude the <a href="notes/math/median.html#_0" target="_self" rel="noopener noreferrer" from="90" to="96" origin-text="median" class="internal-link virtual-link-a">median</a>üîó; if even, include it in both halves.
Find the first quartile, which is the median of the lower half of the dataset.
Find the third quartile, which is the median of the upper half of the dataset.
<br>The Interquartile <a href="notes/math/range.html#_0" target="_self" rel="noopener noreferrer" from="18" to="23" origin-text="Range" class="internal-link virtual-link-a">Range</a>üîó (IQR) is a measure of statistical dispersion that represents the range within which the central 50% of the data points lie. It is defined as:<br>The IQR provides insights on the spread of the data around the <a href="notes/math/median.html#_0" target="_self" rel="noopener noreferrer" from="63" to="69" origin-text="median" class="internal-link virtual-link-a">median</a>üîó, and is commonly used to identify <a href="notes/math/outlier.html#_0" target="_self" rel="noopener noreferrer" from="104" to="112" origin-text="outliers" class="internal-link virtual-link-a">outliers</a>üîó where values that are below or above are considered outliers.<br>A boxplot, sometimes referred to as a whisker plot, provides a visual summary of a distribution of a dataset. The plot depicts the values of the <a data-href="Five-Number Summary" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Five-Number Summary</a> which consists of the <a href="notes/math/median.html#_0" target="_self" rel="noopener noreferrer" from="23" to="29" origin-text="median" class="internal-link virtual-link-a">median</a>üîó, first and third quartiles, and the maximum and minimum values.The boxplot consists of:
<br>The Box: Which represents the <a data-href="Interquartile Range" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Interquartile Range</a> of the distribution, which can be thought of as the middle 50% of the data.
<br>Median Line: The line in the center of the box which represents the second quartile, or the <a href="notes/math/median.html#_0" target="_self" rel="noopener noreferrer" from="81" to="87" origin-text="median" class="internal-link virtual-link-a">median</a>üîó of the dataset.
Whiskers: The lines extending from the box to the value of the minimum or maximum value of the distribution. It is common for the whiskers to be set to the smallest and largest values with 1.5 times the IQR, rather than the true minimum and maximum values of the population.<br>
<img alt="Pasted image 20250708093049.png" src="resources/pasted-image-20250708093049.png" target="_self"><br>
<a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.machinelearningplus.com/plots/python-boxplot/" target="_self">https://www.machinelearningplus.com/plots/python-boxplot/</a> J. Han, M. Kamber, and J. Pei,&nbsp;Data Mining : Concepts and Techniques. Burlington, Ma: Elsevier, 2012.
<br>‚ÄúQuartile,‚Äù&nbsp;Wikipedia, Nov. 26, 2021. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Quartile" target="_self">https://en.wikipedia.org/wiki/Quartile</a>
<br>‚ÄúFind a Five-Number Summary in Statistics: Easy Steps,‚Äù&nbsp;Statistics How To. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.statisticshowto.com/statistics-basics/how-to-find-a-five-number-summary-in-statistics/" target="_self">https://www.statisticshowto.com/statistics-basics/how-to-find-a-five-number-summary-in-statistics/</a>
<br>M. Yi, ‚ÄúA Complete Guide to Box Plots,‚Äù&nbsp;Atlassian, 2025. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.atlassian.com/data/charts/box-plot-complete-guide" target="_self">https://www.atlassian.com/data/charts/box-plot-complete-guide</a>
<br>S. Glen, ‚ÄúInterquartile <a href="notes/math/range.html#_0" target="_self" rel="noopener noreferrer" from="24" to="29" origin-text="Range" class="internal-link virtual-link-a">Range</a>üîó (IQR): What it is and How to Find it,‚Äù&nbsp;Statistics How To, 2022. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.statisticshowto.com/probability-and-statistics/interquartile-range/" target="_self">https://www.statisticshowto.com/probability-and-statistics/interquartile-range/</a>
]]></description><link>notes/math/quartile.html</link><guid isPermaLink="false">NOTES/Math/Quartile.md</guid><pubDate>Mon, 07 Jul 2025 00:00:00 GMT</pubDate><enclosure url="." length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;.&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Ordinal Data]]></title><description><![CDATA[Ordinal data is a type of <a data-href="Categorical Data" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Categorical Data</a> that represents categories that a defined order or ranking. Unlike <a data-href="Nominal Data" href="notes/math/nominal-data.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Nominal Data</a>, which has no intrinsic ordering, ordinal data allows for the comparison of the relative positioning of items.A key property of ordinal data is that the interval between adjacent ranks is not necessarily equal. For example, the interval between "expensive" and "average" is not necessarily equal to the interval between "cheap" and "average."Another key property of Ordinal data is that it is non-numeric. While ordinal variables may be represented by numbers (e.g. 1, 2, 3), these numbers are only labels that indicate orders and not precise values.Examples of ordinal data include attributes like education levels (e.g. High School, Bachelor's, Masters), or survey responses (e.g. Unsatisfied, Neutral, Satisfied).The analysis of ordinal data requires specific statistical techniques that respect the inherent order of the categories while acknowledging that the intervals between them are not necessarily uniform.
<br>The <a data-href="Median" href="notes/math/median.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Median</a> and <a data-href="Mode" href="notes/math/mode.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Mode</a> can be used to summarize the central tendencies and identify the most frequent categories.
<br><a data-tooltip-position="top" aria-label="Bar Chart" data-href="Bar Chart" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Bar Charts</a> can visualize the frequency distribution of categories and illustrate the most common or uncommon values. <br>Because ordinal data does not meet the assumptions required for <a data-tooltip-position="top" aria-label="Parametric Test" data-href="Parametric Test" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Parametric Tests</a>, <a data-tooltip-position="top" aria-label="Non-Parametric Test" data-href="Non-Parametric Test" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Non-Parametric Tests</a>, such as <a data-href="Mann-Whitney U Test" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Mann-Whitney U Test</a> or <a data-href="Kruskal-Wallis Test" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Kruskal-Wallis Test</a> are often used to analyze the difference between groups.
<br><a data-href="Spearman's Rank" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Spearman's Rank</a> correlation can be used to assess relationships between ordinal attributes, providing potential insights on the strength and direction of correlations. J. Han and M. Kamber,&nbsp;Data Mining : Concepts and Techniques, 3rd ed. Haryana, India ; Burlington, Ma: Elsevier, 2018.
<br>‚ÄúTypes of Data | Introduction to Data Science,‚Äù&nbsp;<a data-tooltip-position="top" aria-label="http://www.stat.lsa.umich.edu" rel="noopener nofollow" class="external-link is-unresolved" href="http://www.stat.lsa.umich.edu" target="_self">www.stat.lsa.umich.edu</a>. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://dept.stat.lsa.umich.edu/~kshedden/introds/topics/types_of_data/" target="_self">https://dept.stat.lsa.umich.edu/~kshedden/introds/topics/types_of_data/</a>
<br>‚ÄúOrdinal data,‚Äù&nbsp;Wikipedia, Apr. 03, 2020. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Ordinal_data" target="_self">https://en.wikipedia.org/wiki/Ordinal_data</a>
]]></description><link>notes/math/ordinal-data.html</link><guid isPermaLink="false">NOTES/Math/Ordinal Data.md</guid><pubDate>Mon, 07 Jul 2025 00:00:00 GMT</pubDate></item><item><title><![CDATA[Mode]]></title><description><![CDATA[The mode is a <a data-href="Measure of Central Tendency" href="notes/math/measure-of-central-tendency.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Measure of Central Tendency</a> which represents the most frequently occurring value in a dataset or population. The mode is particularly versatile since it can be applied to both <a data-href="Categorical Data" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Categorical Data</a> and <a data-href="Numerical Data" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Numerical Data</a>.
Unimodal: A dataset with only one mode.
Bimodal: A dataset with exactly two modes.
Multimodal: A dataset with three or more modes.
<br>In a discrete <a href="notes/math/probability.html#_0" target="_self" rel="noopener noreferrer" from="14" to="25" origin-text="probability" class="internal-link virtual-link-a">probability</a>üîó distribution, the mode is the value that has the highest probability of occurring.<br>In a continuous <a href="notes/math/probability.html#_0" target="_self" rel="noopener noreferrer" from="16" to="27" origin-text="probability" class="internal-link virtual-link-a">probability</a>üîó distribution, the mode is the value at which the <a data-href="Probability Density Function" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Probability Density Function</a> reaches it's maximum. It represents the peak of the distribution.<br>Other common <a data-tooltip-position="top" aria-label="Measure of Central Tendency" data-href="Measure of Central Tendency" href="notes/math/measure-of-central-tendency.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Measures of Central Tendency</a> include the <a data-href="Median" href="notes/math/median.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Median</a>, which represents the middle value when the data is ordered, the <a data-href="Mean" href="notes/math/mean.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Mean</a>, which represents the average value of a population, and the <a data-href="Midrange" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Midrange</a>, calculated as the average of the maximum and minimum values.
J. Han, M. Kamber, and J. Pei,&nbsp;Data Mining : Concepts and Techniques. Burlington, Ma: Elsevier, 2012.
<br>GeeksforGeeks, ‚ÄúMode in Statistics | Definition, Formula, How to Calculate Mode,‚Äù&nbsp;GeeksforGeeks, Sep. 20, 2021. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://www.geeksforgeeks.org/maths/what-is-mode/" target="_self">https://www.geeksforgeeks.org/maths/what-is-mode/</a>
<br>Wikipedia Contributors, ‚ÄúMode (statistics),‚Äù&nbsp;Wikipedia, Oct. 10, 2019. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Mode_(statistics)" target="_self">https://en.wikipedia.org/wiki/Mode_(statistics)</a>
]]></description><link>notes/math/mode.html</link><guid isPermaLink="false">NOTES/Math/Mode.md</guid><pubDate>Tue, 08 Jul 2025 00:00:00 GMT</pubDate></item><item><title><![CDATA[Nominal Data]]></title><description><![CDATA[Nominal data is a type of <a data-href="Categorical Data" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Categorical Data</a> that represents categories that don't have a specific order or ranking. Unlike <a data-href="Ordinal Data" href="notes/math/ordinal-data.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Ordinal Data</a>, which has a defined order, nominal data is purely qualitative. It describes specific qualities without conveying any measure of numerical significance.<br>Since nominal data lacks numerical significance, data operations such as <a data-href="Mean" href="notes/math/mean.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Mean</a> or <a data-href="Median" href="notes/math/median.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Median</a> cannot be performed. However, the frequency of nominal data values can be analyzed, and a measure like the <a data-href="Mode" href="notes/math/mode.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Mode</a> can describe the most common category within a given dataset.<br>A specific type of nominal data is <a data-href="Binary Data" href="notes/math/binary-data.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Binary Data</a>, which consists of only two categories, typically represented as 1 and 0. Binary fields are commonly used to represent whether a certain feature is present or not in a given data object.Examples of nominal data include attributes like eye color (e.g., blue, brown, green), gender (e.g., male, female, non-binary), and nationality (e.g., American, Canadian). These categories do not imply any hierarchy or order.<br>Nominal data, lacking any numerical significance, primarily relies on frequency distribution analysis for meaningful insights. One of the most effective techniques for this is the use of a <a data-href="Bar Chart" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Bar Chart</a>, which visually represent the frequency of each category.
J. Han and M. Kamber,&nbsp;Data Mining : Concepts and Techniques, 3rd ed. Haryana, India ; Burlington, Ma: Elsevier, 2018.
<br>‚ÄúTypes of Data | Introduction to Data Science,‚Äù&nbsp;_<a data-tooltip-position="top" aria-label="http://www.stat.lsa.umich.edu." rel="noopener nofollow" class="external-link is-unresolved" href="http://www.stat.lsa.umich.edu." target="_self">www.stat.lsa.umich.edu.</a> <a rel="noopener nofollow" class="external-link is-unresolved" href="https://dept.stat.lsa.umich.edu/~kshedden/introds/topics/types_of_data/" target="_self">https://dept.stat.lsa.umich.edu/~kshedden/introds/topics/types_of_data/</a>
Wikipedia Contributors, ‚ÄúNominal category,‚Äù&nbsp;Wikipedia, Oct. 07, 2024.
]]></description><link>notes/math/nominal-data.html</link><guid isPermaLink="false">NOTES/Math/Nominal Data.md</guid><pubDate>Mon, 07 Jul 2025 00:00:00 GMT</pubDate></item><item><title><![CDATA[Probability]]></title><description><![CDATA[Probability is a branch of mathematics that deals with the analysis of events and their likelihood of occurring. The probability of an event, written is a number between 0 and 1, with 0 representing impossible and 1 representing certainty. Probability theory is a framework for making inferences of events that have elements of uncertainty or randomness within them or their outcomes. Random Experiment - any process or action that yields uncertain outcomes.Sample Space - commonly denoted as , is the set of all possible outcomes of a random experiment.Event - a subset of the sample space, representing specific outcomes.Power Set: The set of all possible subsets of a sample space, including the empty set or .The probability of an event is calculated by dividing the number of favorable outcomes by the total size of the sample space:The complement of is denoted as or and is equal to
If events A and B are mutually exclusive, that is they can never occur simultaneously, then Joint probability is the probability of 2 events happening simultaneously. If 2 events are independent, then their joint probability is:If 2 events are mutually exclusive, then the probability of one or the other occurring is equal to the sum of their probabilitiesIf 2 events aren't necessarily mutually exclusive, then you simply substract their intersection in order to prevent counting the values in their intersection twice,The probability of one event occurring, given that another has already occurred, denoted as (the probability of A given B), is<a data-href="Bayes' Theorem" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Bayes' Theorem</a> states thatA random variable is a formalization that assigns a numerical value to the outcome of a random event or experiment. Random variables can be classified into two main types:
Discrete random variables take on a countable number of distinct values.
<br>Continuous random variables take on a infinite number of values within a <a href="notes/math/range.html#_0" target="_self" rel="noopener noreferrer" from="46" to="51" origin-text="range" class="internal-link virtual-link-a">range</a>üîó.
Random variables are typically denoted by capital letters. For example, the set can represent the random variables 'heads' and 'tails' in a coin flip experiment. Their values are represented with their corresponding lowercase letter.A probability distribution is a mathematical function which describes the likelihood of different outcomes for the domain of a random variable. The probability of each outcome is between 0 and 1 (inclusive), and the sum of probabilities of each outcome must sum to 1.Discrete probability distributions describe the probability of each possible value in the domain of a discrete random variable.<br>The <a data-href="Probability Mass Function" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Probability Mass Function</a> (PMF) gives the probability of each possible value of a random variable :The sum of all outcomes must sum to 1:Common Discrete Distributions:
<br><a data-href="Bernoulli Distribution" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Bernoulli Distribution</a> - <br><a data-href="Binomial Distribution" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Binomial Distribution</a> - <br><a data-href="Poisson Distribution" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Poisson Distribution</a> - <br>Continuous probability distributions describe the probability of a continuous random variable. These distributions are characterized by a <a data-href="Probability Density Function" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Probability Density Function</a> (PDF).<br>The probability density function describes the likelihood of a random variable taking on a value in a specific <a href="notes/math/range.html#_0" target="_self" rel="noopener noreferrer" from="111" to="116" origin-text="range" class="internal-link virtual-link-a">range</a>üîó:where is the PDF and must satisfy:Common Continuous Distributions:
<br><a data-href="Normal Distribution" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Normal Distribution</a> - <br><a data-href="Exponential Distribution" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Exponential Distribution</a> - <br><a data-href="Uniform Distribution" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Uniform Distribution</a> - <br>The <a data-href="Expected Value" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Expected Value</a> of a random variable, denoted as , is the weighted average of its possible outcomes. This concept is crucial in various fields, including artificial intelligence, especially in the context of stochastic task environments, where uncertainty plays a significant role.For discrete probability distributions, the expected value is defined as:<br>Where is the <a data-href="Probability Mass Function" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Probability Mass Function</a>.For continuous probability distributions, the expected value is defined as:<br>Where is the <a data-href="Probability Density Function" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Probability Density Function</a>.<br>The <a data-href="Central Limit Theorem" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Central Limit Theorem</a> states that if we take random samples from any population, the <a data-href="Mean" href="notes/math/mean.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Mean</a> of those samples will form a normal distribution as the sample size gets sufficiently large.<br>The <a data-href="Law of Large Numbers" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Law of Large Numbers</a> states that the <a data-href="Mean" href="notes/math/mean.html#_0" class="internal-link" target="_self" rel="noopener nofollow">Mean</a> of the outcomes obtained from a large number of independent samples will converge to the expected value of the underlying probability distribution.<br>Probability is the foundation of many <a data-href="Machine Learning" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Machine Learning</a> algorithms such as:
<br><a data-href="Na√Øve Bayes Classifier" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Na√Øve Bayes Classifier</a> - a classification model based on Bayes' theorem with strong assumptions about feature independence.
<br><a data-href="Hidden Markov Model" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Hidden Markov Model</a> - A statistical model that represents a system which is assumed to be a <a data-href="Markov Process" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Markov Process</a>.
<br><a data-href="Decision Theory" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Decision Theory</a> utilizes probability to make rational decisions under uncertainty. Relevant subject include:
<br><a data-href="Utility Function" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Utility Function</a> - Assigning subjective desirability to outcomes.
<br><a data-href="Markov Decision Process" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Markov Decision Process</a> - A framework for modeling sequential decision-making with stochastic outcomes.
<br>A <a data-href="Bayesian Network" href=".html" class="internal-link" target="_self" rel="noopener nofollow">Bayesian Network</a> is a probabilistic model implemented as a directed acyclic graph that represents a set of random variables and their conditional dependencies, as well as a set of conditional probability distribution tables.
Peter. R. Norvig, Artificial Intelligence: A Modern Approach, Global Edition. 2021.
<br>Wikipedia contributors, ‚ÄúProbability theory,‚Äù Wikipedia, Apr. 23, 2025. <a rel="noopener nofollow" class="external-link is-unresolved" href="https://en.wikipedia.org/wiki/Probability_theory" target="_self">https://en.wikipedia.org/wiki/Probability_theory</a>
]]></description><link>notes/math/probability.html</link><guid isPermaLink="false">NOTES/Math/Probability.md</guid><pubDate>Wed, 02 Jul 2025 00:00:00 GMT</pubDate></item></channel></rss>