{"createdTime":1754083139414,"shownInTree":["notes/math/variance.html","notes/math/transitive-property.html","notes/math/standard-deviation.html","notes/math/range.html","notes/math/quartile.html","notes/math/quantile.html","notes/math/propositional-logic.html","notes/math/probability.html","notes/math/outlier.html","notes/math/ordinal-data.html","notes/math/nominal-data.html","notes/math/mode.html","notes/math/median.html","notes/math/measure-of-central-tendency.html","notes/math/mean.html","notes/math/manhattan-distance.html","notes/math/least-squares.html","notes/math/euclidean-distance.html","notes/math/commutative-property.html","notes/math/binary-data.html","notes/math/associative-property.html","notes/ai/turing-test.html","textbooks/data-mining-concepts-and-techniques/summary.html","textbooks/data-mining-concepts-and-techniques/practice-problems.html","textbooks/ai-a-modern-approach/summary.html","textbooks/ai-a-modern-approach/practice-problems.html","index.html"],"attachments":["site-lib/scripts/graph-wasm.wasm","site-lib/fonts/94f2f163d4b698242fef.otf","site-lib/fonts/72505e6a122c6acd5471.woff2","site-lib/fonts/2d5198822ab091ce4305.woff2","site-lib/fonts/c8ba52b05a9ef10f4758.woff2","site-lib/fonts/cb10ffd7684cd9836a05.woff2","site-lib/fonts/b5f0f109bc88052d4000.woff2","site-lib/fonts/cbe0ae49c52c920fd563.woff2","site-lib/fonts/535a6cf662596b3bd6a6.woff2","site-lib/fonts/70cc7ff27245e82ad414.ttf","site-lib/fonts/454577c22304619db035.ttf","site-lib/fonts/52ac8f3034507f1d9e53.ttf","site-lib/fonts/05b618077343fbbd92b7.ttf","site-lib/fonts/4bb6ac751d1c5478ff3a.woff2","site-lib/media/6155340132a851f6089e.svg","site-lib/media/2308ab1944a6bfa5c5b8.svg","site-lib/fonts/mathjax_zero.woff","site-lib/fonts/mathjax_main-regular.woff","site-lib/fonts/mathjax_main-bold.woff","site-lib/fonts/mathjax_math-italic.woff","site-lib/fonts/mathjax_main-italic.woff","site-lib/fonts/mathjax_math-bolditalic.woff","site-lib/fonts/mathjax_size1-regular.woff","site-lib/fonts/mathjax_size2-regular.woff","site-lib/fonts/mathjax_size3-regular.woff","site-lib/fonts/mathjax_size4-regular.woff","site-lib/fonts/mathjax_ams-regular.woff","site-lib/fonts/mathjax_calligraphic-regular.woff","site-lib/fonts/mathjax_calligraphic-bold.woff","site-lib/fonts/mathjax_fraktur-regular.woff","site-lib/fonts/mathjax_fraktur-bold.woff","site-lib/fonts/mathjax_sansserif-regular.woff","site-lib/fonts/mathjax_sansserif-bold.woff","site-lib/fonts/mathjax_sansserif-italic.woff","site-lib/fonts/mathjax_script-regular.woff","site-lib/fonts/mathjax_typewriter-regular.woff","site-lib/fonts/mathjax_vector-regular.woff","site-lib/fonts/mathjax_vector-bold.woff","site-lib/html/file-tree-content.html","site-lib/scripts/webpage.js","site-lib/scripts/graph-wasm.js","site-lib/scripts/graph-render-worker.js","site-lib/media/favicon.png","site-lib/styles/snippets.css","site-lib/styles/obsidian.css","site-lib/styles/theme.css","site-lib/styles/global-variable-styles.css","site-lib/styles/supported-plugins.css","site-lib/styles/main-styles.css","resources/linearregression.png","resources/highvslowstd.png","resources/pasted-image-20250708171735.png","resources/pasted-image-20250708093049.png","site-lib/rss.xml","site-lib/html/custom-head-content-content.html"],"allFiles":["index.html","site-lib/html/custom-head-content-content.html","textbooks/data-mining-concepts-and-techniques/summary.html","notes/math/propositional-logic.html","notes/math/associative-property.html","textbooks/ai-a-modern-approach/summary.html","textbooks/data-mining-concepts-and-techniques/practice-problems.html","notes/math/standard-deviation.html","notes/math/variance.html","notes/math/transitive-property.html","notes/math/range.html","notes/math/quartile.html","notes/math/quantile.html","notes/math/probability.html","notes/math/outlier.html","notes/math/ordinal-data.html","notes/math/nominal-data.html","notes/math/mode.html","notes/math/measure-of-central-tendency.html","notes/math/median.html","notes/math/least-squares.html","notes/math/mean.html","notes/math/manhattan-distance.html","notes/math/euclidean-distance.html","notes/math/commutative-property.html","notes/math/binary-data.html","notes/ai/turing-test.html","textbooks/ai-a-modern-approach/practice-problems.html","site-lib/scripts/graph-wasm.wasm","site-lib/fonts/94f2f163d4b698242fef.otf","site-lib/fonts/72505e6a122c6acd5471.woff2","site-lib/fonts/2d5198822ab091ce4305.woff2","site-lib/fonts/c8ba52b05a9ef10f4758.woff2","site-lib/fonts/cb10ffd7684cd9836a05.woff2","site-lib/fonts/b5f0f109bc88052d4000.woff2","site-lib/fonts/cbe0ae49c52c920fd563.woff2","site-lib/fonts/535a6cf662596b3bd6a6.woff2","site-lib/fonts/70cc7ff27245e82ad414.ttf","site-lib/fonts/454577c22304619db035.ttf","site-lib/fonts/52ac8f3034507f1d9e53.ttf","site-lib/fonts/05b618077343fbbd92b7.ttf","site-lib/fonts/4bb6ac751d1c5478ff3a.woff2","site-lib/media/6155340132a851f6089e.svg","site-lib/media/2308ab1944a6bfa5c5b8.svg","site-lib/fonts/mathjax_zero.woff","site-lib/fonts/mathjax_main-regular.woff","site-lib/fonts/mathjax_main-bold.woff","site-lib/fonts/mathjax_math-italic.woff","site-lib/fonts/mathjax_main-italic.woff","site-lib/fonts/mathjax_math-bolditalic.woff","site-lib/fonts/mathjax_size1-regular.woff","site-lib/fonts/mathjax_size2-regular.woff","site-lib/fonts/mathjax_size3-regular.woff","site-lib/fonts/mathjax_size4-regular.woff","site-lib/fonts/mathjax_ams-regular.woff","site-lib/fonts/mathjax_calligraphic-regular.woff","site-lib/fonts/mathjax_calligraphic-bold.woff","site-lib/fonts/mathjax_fraktur-regular.woff","site-lib/fonts/mathjax_fraktur-bold.woff","site-lib/fonts/mathjax_sansserif-regular.woff","site-lib/fonts/mathjax_sansserif-bold.woff","site-lib/fonts/mathjax_sansserif-italic.woff","site-lib/fonts/mathjax_script-regular.woff","site-lib/fonts/mathjax_typewriter-regular.woff","site-lib/fonts/mathjax_vector-regular.woff","site-lib/fonts/mathjax_vector-bold.woff","site-lib/html/file-tree-content.html","site-lib/scripts/webpage.js","site-lib/scripts/graph-wasm.js","site-lib/scripts/graph-render-worker.js","site-lib/media/favicon.png","site-lib/styles/snippets.css","site-lib/styles/obsidian.css","site-lib/styles/theme.css","site-lib/styles/global-variable-styles.css","site-lib/styles/supported-plugins.css","site-lib/styles/main-styles.css"],"webpages":{"notes/math/variance.html":{"title":"Variance","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-10Variance is a <a data-href=\"Measure of Dispersion\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Measure of Dispersion</a> that summarizes the spread of values in a dataset from the average. It represents the average squared distance from each data point to the <a data-tooltip-position=\"top\" aria-label=\"Mean\" data-href=\"Mean\" href=\"notes/math/mean.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">mean</a> of the dataset.\n<br>\nLow Variance: Indicates that the data points tend to be very close to the <a href=\"notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"62\" to=\"66\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a>, suggesting consistency and reliability in the dataset. <br>\nHigh Variance: Implies that data points are more spread out across the <a data-tooltip-position=\"top\" aria-label=\"Range\" data-href=\"Range\" href=\"notes/math/range.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">range</a>, indicating greater variability and less predictability. <br><img alt=\"HighVsLowSTD.png\" src=\"resources/highvslowstd.png\" target=\"_self\">\nNon-Negative: The variance can never be negative, since it is an average distance measure and distance can also never be negative.\nThe variance of a numeric attribute is defined as:For a population, the variance is calculated using:Where: represents the population variance.\n<br> represents the <a data-tooltip-position=\"top\" aria-label=\"Mean\" data-href=\"Mean\" href=\"notes/math/mean.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">mean</a> of the population. represents the number of observations.\nFor a sample, the variance is calculated using:Where: represents the sample variance.\n<br> represents the <a href=\"notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"16\" to=\"20\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a> of the sample. represents the number of observations.\n<br>The <a href=\"notes/math/standard-deviation.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"4\" to=\"22\" origin-text=\"standard deviation\" class=\"internal-link virtual-link-a\">standard deviation</a> is equal to the square root of the variance of the same dataset. Standard deviation is expressed in the same units as the original data, while variance is expressed in squared units.<br>The expected value, or <a href=\"notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"23\" to=\"27\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a>, of a random variable , denoted , is a <a data-tooltip-position=\"top\" aria-label=\"Measure of Central Tendency\" data-href=\"Measure of Central Tendency\" href=\"notes/math/measure-of-central-tendency.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">measure of central tendency</a> that represents the average outcome of a random variable. Intuitively, it is the average of the outcomes of many samples from .<br>The variance of a random variable measures the dispersion of a random variable around its expected value . It is defined as the expected value of the squared differences from the <a href=\"notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"74\" to=\"78\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a>:Where: is the expected value of .\n<br>Covariance is vital in understanding the relationships between random variables in <a data-tooltip-position=\"top\" aria-label=\"Probability\" data-href=\"Probability\" href=\"notes/math/probability.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">probability</a> distributions. The value of the covariance between two variables is represents the direction of the linear relationship between them.<br>A <a data-href=\"Covariance Matrix\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Covariance Matrix</a> is a square matrix containing the covariance between multiple variables.The formula for calculating the covariance is:Where:\n<br> and represent the <a href=\"notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"15\" to=\"19\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a> of and respectively. represents the number of samples in the dataset. Positive Covariance: Indicates that as increases, increases.\nNegative Covariance: Indicates that as increases, decreases.\nZero/Near-Zero Covariance: Indicates no linear relationship between and .\nCovariance is a key step in calculating correlation, which normalizes the covariance value to a standard scale. Correlation is useful for assessing the strength and direction of the relationship between two variables.<br>A dataset is homoscedastic if the variance of the residuals (errors) is constant across the <a href=\"notes/math/range.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"92\" to=\"97\" origin-text=\"range\" class=\"internal-link virtual-link-a\">range</a> of the independent variable(s). Conversely, if the variance changes as a function of the independent variable, the dataset is heteroscedastic.<br>Homoscedasticity is an important concept in <a data-href=\"Regression\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Regression</a> analysis as it is an essential assumption for many regression models such as <a data-href=\"Linear Regression\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Linear Regression</a> based on the <a data-href=\"Least Squares\" href=\"notes/math/least-squares.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Least Squares</a> method.\nResiduals vs. Fitted Values Plot: A scatter plot of residuals against the predicted values. A random scatter indicates that the dataset is homoscedastic.\n<br><a data-href=\"Breusch-Pagan Test\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Breusch-Pagan Test</a>: Creates an initial regression model, then fits a new model on the squared residuals of original model against the independent variable. If the corresponding <a data-href=\"P-Value\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">P-Value</a> from the <a data-href=\"Chi-Squared Test\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Chi-Squared Test</a> is less than some chosen significance level, then heteroscedasticity is assumed.\n<br><a data-href=\"White Test\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">White Test</a>: A more general test which can detect non-linear heteroscedastic relationships. The methodology is similar to that of the Breusch-Pagan test, though the White test involves regressing the squared residuals against the original independent variables, their squares, and their cross-products. <br><a data-href=\"Weighted Least Squares\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Weighted Least Squares</a>: Weighs the effect of observations on the regression line based on its estimated variance.\n<br>Transformations: Applying a <a data-href=\"Log Transformation\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Log Transformation</a> or <a data-href=\"Box-Cox Transformation\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Box-Cox Transformation</a> can potentially reduce or eliminate heteroscedasticity entirely. J. Han, M. Kamber, and J. Pei,&nbsp;Data Mining : Concepts and Techniques. Burlington, Ma: Elsevier, 2012.\n<br>GeeksforGeeks, “Variance,”&nbsp;GeeksforGeeks, Apr. 27, 2022. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/maths/variance/\" target=\"_self\">https://www.geeksforgeeks.org/maths/variance/</a>\n<br>Wikipedia Contributors, “Variance,”&nbsp;Wikipedia, Jan. 08, 2019. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Variance\" target=\"_self\">https://en.wikipedia.org/wiki/Variance</a>\n<br>J. Starmer, “Covariance, Clearly Explained!!!,”&nbsp;YouTube. Jul. 29, 2019. Available: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.youtube.com/watch?v=qtaqvPAeEJY\" target=\"_self\">https://www.youtube.com/watch?v=qtaqvPAeEJY</a>\n<br>GeeksforGeeks, “Covariance and Correlation,”&nbsp;GeeksforGeeks, Jun. 25, 2018. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/engineering-mathematics/mathematics-covariance-and-correlation/\" target=\"_self\">https://www.geeksforgeeks.org/engineering-mathematics/mathematics-covariance-and-correlation/</a> (accessed Jul. 16, 2025).\n<br>Z. Bobbit, “The Breusch-Pagan Test: Definition &amp; Example,”&nbsp;Statology, Dec. 31, 2020. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.statology.org/breusch-pagan-test/\" target=\"_self\">https://www.statology.org/breusch-pagan-test/</a>\n<br>J. Waples, “Heteroscedasticity: A Full Guide to Unequal Variance,”&nbsp;Datacamp.com, Dec. 10, 2024. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.datacamp.com/tutorial/heteroscedasticity\" target=\"_self\">https://www.datacamp.com/tutorial/heteroscedasticity</a>\n","aliases":["Variances"],"inlineTags":[],"frontmatterTags":["#math","#statistics"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Variance","level":1,"id":"Variance_0"},{"heading":"Interpretation","level":3,"id":"Interpretation_0"},{"heading":"Properties","level":3,"id":"Properties_0"},{"heading":"Calculation","level":2,"id":"Calculation_0"},{"heading":"Population Variance","level":3,"id":"Population_Variance_0"},{"heading":"Sample Variance","level":3,"id":"Sample_Variance_0"},{"heading":"Relation to <a data-href=\"Standard Deviation\" href=\"Standard Deviation\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Standard Deviation</a>","level":4,"id":"Relation_to_[[Standard_Deviation]]_0"},{"heading":"<a data-href=\"Expected Value\" href=\"Expected Value\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Expected Value</a>","level":2,"id":"[[Expected_Value]]_0"},{"heading":"Variance of Random Variables","level":3,"id":"Variance_of_Random_Variables_0"},{"heading":"<a data-href=\"Covariance\" href=\"Covariance\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Covariance</a>","level":2,"id":"[[Covariance]]_0"},{"heading":"Formula","level":3,"id":"Formula_0"},{"heading":"Interpretation","level":3,"id":"Interpretation_1"},{"heading":"<a data-href=\"Correlation\" href=\"Correlation\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Correlation</a>","level":3,"id":"[[Correlation]]_0"},{"heading":"<a data-href=\"Homoscedasticity\" href=\"Homoscedasticity\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Homoscedasticity</a> and <a data-href=\"Heteroscedasticity\" href=\"Heteroscedasticity\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Heteroscedasticity</a>","level":2,"id":"[[Homoscedasticity]]_and_[[Heteroscedasticity]]_0"},{"heading":"Regression Analysis","level":3,"id":"Regression_Analysis_0"},{"heading":"Statistical Tests","level":3,"id":"Statistical_Tests_0"},{"heading":"Addressing Heteroscedasticity","level":3,"id":"Addressing_Heteroscedasticity_0"},{"heading":"References","level":2,"id":"References_0"}],"links":[".html","notes/math/mean.html#_0","notes/math/mean.html#_0","notes/math/range.html#_0","notes/math/mean.html#_0","notes/math/mean.html#_0","notes/math/standard-deviation.html#_0","notes/math/standard-deviation.html#_0",".html","notes/math/mean.html#_0","notes/math/measure-of-central-tendency.html#_0","notes/math/mean.html#_0",".html","notes/math/probability.html#_0",".html","notes/math/mean.html#_0",".html",".html",".html","notes/math/range.html#_0",".html",".html","notes/math/least-squares.html#_0",".html",".html",".html",".html",".html",".html",".html"],"author":"","coverImageURL":"resources/highvslowstd.png","fullURL":"notes/math/variance.html","pathToRoot":"../..","attachments":["resources/highvslowstd.png"],"createdTime":1752165687104,"modifiedTime":1754247600661,"sourceSize":6070,"sourcePath":"NOTES/Math/Variance.md","exportPath":"notes/math/variance.html","showInTree":true,"treeOrder":24,"backlinks":["notes/math/least-squares.html","notes/math/outlier.html","notes/math/standard-deviation.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown"},"notes/math/transitive-property.html":{"title":"Transitive Property","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-25If a relation is transitive then for all possible elements , and , if that relation holds for and , and that same relation holds between and , then that relation must hold between and .Where: is some relation such as equality or inequality. Equality: If and then .\nInequality: If and then .\nSet Inclusion: If and then .\nImplication: If and then .\nInheritance: If inherits from and inherits from then inherits from . Mathematics: Used extensively in algebra, geometry, and set theory to establish relationships between numbers, variables, or sets.\nComputer Science: Used in graph theory algorithms like <a data-href=\"Warshall's Algorithm\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Warshall's Algorithm</a> which is used to determine the <a data-href=\"Transitive Closure\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Transitive Closure</a> of a directed graph. The <a data-href=\"Transitive Closure\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Transitive Closure</a> of a directed graph represents which vertices are reachable from others. The <a data-href=\"Transitive Reduction\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Transitive Reduction</a> of a directed graph is the smallest reduction that has the same reachability relation as the original graph.\nLogic: The transitive property is a fundamental property in proofs and reasoning, allowing for the derivation of conclusions based on established relationships.\nThe reflexive property states that any value or expression is equal to itself. The equality relation is an example of a reflexive operation, since all real numbers or variables are equal to themselves.A relation is symmetric if for all possible elements and , if is related to , then is related to . Equality is an example of a symmetric relation.<br>An equivalence relation is a relation that is reflexive, symmetric, and transitive. Equivalence relations can group elements of a set into distinct categories called <a data-href=\"Equivalence Classes\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Equivalence Classes</a>, which are disjoint subsets containing elements that are equivalent to one another.<br>A partial order on a set is a binary relation that is reflexive, antisymmetric, and transitive. It is a way to order elements in a set where not all pairs of elements need to be comparable. Partial orders are visualized using a <a data-href=\"Hasse Diagram\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Hasse Diagram</a>.<br>Preorder is a binary relation defined on a set that is both reflexive and transitive. It is a generalization of partial orders in the sense that it does not require antisymmetry. Preorders are commonly applied in <a data-href=\"Decision Theory\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Decision Theory</a> to model preferences, or in <a data-href=\"Game Theory\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Game Theory</a> to compare strategies.\nA binary relation is intransitive if there exists three values where transitivity does not hold.\nAntitransitivity is a stronger property which holds if for any three values, transitivity never holds. <br>GeeksforGeeks, “Transitive Property,”&nbsp;GeeksforGeeks, Mar. 07, 2024. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/maths/transitive-property/\" target=\"_self\">https://www.geeksforgeeks.org/maths/transitive-property/</a> (accessed Jul. 25, 2025)\n<br>“Transitive relation,”&nbsp;Wikipedia, Jan. 29, 2023. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Transitive_relation\" target=\"_self\">https://en.wikipedia.org/wiki/Transitive_relation</a>\n<br>GeeksforGeeks, “Equivalence Relations,”&nbsp;GeeksforGeeks, Nov. 09, 2023. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/maths/equivalence-relations/\" target=\"_self\">https://www.geeksforgeeks.org/maths/equivalence-relations/</a>\n<br>“Intransitivity,”&nbsp;Wikipedia, Mar. 07, 2024. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Intransitivity\" target=\"_self\">https://en.wikipedia.org/wiki/Intransitivity</a>\n","aliases":["Transitivity"],"inlineTags":[],"frontmatterTags":["#math"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Transitive Property","level":1,"id":"Transitive_Property_0"},{"heading":"Relations","level":3,"id":"Relations_0"},{"heading":"Applications","level":3,"id":"Applications_0"},{"heading":"Related Properties","level":2,"id":"Related_Properties_0"},{"heading":"<a data-href=\"Reflexive Property\" href=\"Reflexive Property\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Reflexive Property</a>","level":3,"id":"[[Reflexive_Property]]_0"},{"heading":"<a data-href=\"Symmetric Property\" href=\"Symmetric Property\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Symmetric Property</a>","level":3,"id":"[[Symmetric_Property]]_0"},{"heading":"<a data-href=\"Equivalence Relation\" href=\"Equivalence Relation\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Equivalence Relation</a>","level":3,"id":"[[Equivalence_Relation]]_0"},{"heading":"<a data-href=\"Preorder\" href=\"Preorder\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Preorder</a> &amp; <a data-href=\"Partial Order\" href=\"Partial Order\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Partial Order</a>","level":3,"id":"[[Preorder]]_&_[[Partial_Order]]_0"},{"heading":"Intransitivity &amp; Antitransitivity","level":3,"id":"Intransitivity_&_Antitransitivity_0"},{"heading":"References","level":2,"id":"References_0"}],"links":[".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html"],"author":"","coverImageURL":"","fullURL":"notes/math/transitive-property.html","pathToRoot":"../..","attachments":[],"createdTime":1753469739589,"modifiedTime":1754247595889,"sourceSize":3717,"sourcePath":"NOTES/Math/Transitive Property.md","exportPath":"notes/math/transitive-property.html","showInTree":true,"treeOrder":23,"backlinks":[],"type":"markdown"},"notes/math/standard-deviation.html":{"title":"Standard Deviation","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-10Standard deviation is a <a data-href=\"Measure of Dispersion\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Measure of Dispersion</a> that summarizes the amount variation in a dataset. It represents the average distance between each data point and the <a data-href=\"Mean\" href=\"https://emujakic.github.io/TechKB/notes/math/mean.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Mean</a> of the population.\n<br>\nLow Standard Deviation: Indicates that the data points tend to be very close to the <a href=\"https://emujakic.github.io/TechKB/notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"62\" to=\"66\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a>, suggesting consistency and reliability in the dataset. <br>\nHigh Standard Deviation: Implies that data points are more spread out across the <a data-tooltip-position=\"top\" aria-label=\"Range\" data-href=\"Range\" href=\"https://emujakic.github.io/TechKB/notes/math/range.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">range</a>, indicating greater variability and less predictability. <br><img alt=\"HighVsLowSTD.png\" src=\"https://emujakic.github.io/TechKB/resources/highvslowstd.png\" target=\"_self\">\nNon-Negative: The standard deviation can never be negative, since it is an average distance measure and distance can also never be negative.\n<br>Sensitive to <a data-tooltip-position=\"top\" aria-label=\"Outlier\" data-href=\"Outlier\" href=\"https://emujakic.github.io/TechKB/notes/math/outlier.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Outliers</a>: Extreme <a href=\"https://emujakic.github.io/TechKB/notes/math/outlier.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"10\" to=\"18\" origin-text=\"outliers\" class=\"internal-link virtual-link-a\">outliers</a> can have a significant impact on the standard deviation.\nSame Units: The standard deviation is expressed in the same units as the underlying dataset.\nThe standard deviation of a numeric attribute , denoted with , is defined as:For a population, the standard deviation is calculated using:Where:\n<br> represents the <a href=\"https://emujakic.github.io/TechKB/notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"16\" to=\"20\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a> of the population. represents the number of observations.\nFor a sample, the standard deviation is calculated using:Where:\n<br> represents the <a href=\"https://emujakic.github.io/TechKB/notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"16\" to=\"20\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a> of the sample. represents the number of observations.\n<br>The standard deviation is equal to the square root of the <a data-href=\"Variance\" href=\"https://emujakic.github.io/TechKB/notes/math/variance.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Variance</a> of the same dataset. <a href=\"https://emujakic.github.io/TechKB/notes/math/variance.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"22\" to=\"30\" origin-text=\"Variance\" class=\"internal-link virtual-link-a\">Variance</a> measures the average of the squared differences from the <a href=\"https://emujakic.github.io/TechKB/notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"88\" to=\"92\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a>, providing insight into the spread of the data.Other common measures of dispersion include:\n<br><a data-href=\"Interquartile Range\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Interquartile Range</a> (IQR): which is the distance covered by the middle 50% of the dataset.\n<br><a data-href=\"Range\" href=\"https://emujakic.github.io/TechKB/notes/math/range.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Range</a>: which is the difference between the maximum and minimum values in a dataset\n<br><a data-tooltip-position=\"top\" aria-label=\"Quartile\" data-href=\"Quartile\" href=\"https://emujakic.github.io/TechKB/notes/math/quartile.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Quartiles</a>: which are the three values that divide a dataset into four equal parts. J. Han, M. Kamber, and J. Pei,&nbsp;Data Mining : Concepts and Techniques. Burlington, Ma: Elsevier, 2012.\n<br>GeeksforGeeks, “Standard Deviation Formula, Examples &amp; How to Calculate,”&nbsp;GeeksforGeeks, Jul. 06, 2022. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/maths/standard-deviation-formula/\" target=\"_self\">https://www.geeksforgeeks.org/maths/standard-deviation-formula/</a>\n<br>J. Frost, “Standard Deviation: Interpretations and Calculations,”&nbsp;Statistics By Jim, 2024. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://statisticsbyjim.com/basics/standard-deviation/\" target=\"_self\">https://statisticsbyjim.com/basics/standard-deviation/</a>\n","aliases":["Standard Deviations"],"inlineTags":[],"frontmatterTags":["#math","#statistics","#unfinished"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Standard Deviation","level":1,"id":"Standard_Deviation_0"},{"heading":"Interpretation","level":3,"id":"Interpretation_0"},{"heading":"Properties","level":3,"id":"Properties_0"},{"heading":"Calculation","level":2,"id":"Calculation_0"},{"heading":"Population Standard Deviation","level":3,"id":"Population_Standard_Deviation_0"},{"heading":"Sample Standard Deviation","level":3,"id":"Sample_Standard_Deviation_0"},{"heading":"Relation to <span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Variance.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"12\" to=\"20\" origin-text=\"Variance\" class=\"internal-link virtual-link-a\">Variance</a></span>","level":4,"id":"Relation_to_Variance_0"},{"heading":"Other Common Measures of Dispersion","level":2,"id":"Other_Common_Measures_of_Dispersion_0"},{"heading":"References","level":2,"id":"References_0"}],"links":[".html","notes/math/mean.html#_0","notes/math/mean.html#_0","notes/math/range.html#_0","notes/math/outlier.html#_0","notes/math/outlier.html#_0","notes/math/mean.html#_0","notes/math/mean.html#_0","notes/math/variance.html#_0","notes/math/variance.html#_0","notes/math/variance.html#_0","notes/math/mean.html#_0",".html","notes/math/range.html#_0","notes/math/quartile.html#_0"],"author":"Ernad Mujakic","coverImageURL":"https://emujakic.github.io/TechKB/resources/highvslowstd.png","fullURL":"https://emujakic.github.io/TechKB/notes/math/standard-deviation.html","pathToRoot":"../..","attachments":["resources/highvslowstd.png"],"createdTime":1752158879614,"modifiedTime":1754340040519,"sourceSize":2848,"sourcePath":"NOTES/Math/Standard Deviation.md","exportPath":"notes/math/standard-deviation.html","showInTree":true,"treeOrder":22,"backlinks":["notes/math/outlier.html","notes/math/variance.html","textbooks/data-mining-concepts-and-techniques/practice-problems.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown"},"notes/math/range.html":{"title":"Range","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-15The range is a <a data-href=\"Measure of Dispersion\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Measure of Dispersion</a> that represents the difference between the smallest and largest value in a dataset. The range is calculated as:\nEasy to compute: Very easy value to calculate even by hand.\nIntuitive: Illustrates the spread of the data by a simple value that is in the same scale as the underlying dataset making it easy to understand. <br>Sensitive to <a data-tooltip-position=\"top\" aria-label=\"Outlier\" data-href=\"Outlier\" href=\"notes/math/outlier.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Outliers</a>: The range, in the worst case, can be entirely determined by two <a href=\"notes/math/outlier.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"66\" to=\"74\" origin-text=\"outliers\" class=\"internal-link virtual-link-a\">outliers</a> that do not accurately represent the underlying dataset, making the range a misleading metric in such a case.\nVague: Does not provide much information regarding the distribution of a population, making it very uninformative. <br><a data-href=\"Interquartile Range\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Interquartile Range</a>: The IQR of a dataset is the difference between the first <a data-tooltip-position=\"top\" aria-label=\"Quartile\" data-href=\"Quartile\" href=\"notes/math/quartile.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">quartile</a> , and the third <a href=\"notes/math/quartile.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"16\" to=\"24\" origin-text=\"quartile\" class=\"internal-link virtual-link-a\">quartile</a> . It provides the spread of the middle 50% of the data.\n<br><a data-href=\"Midrange\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Midrange</a>: The midrange of a dataset is the <a data-tooltip-position=\"top\" aria-label=\"Mean\" data-href=\"Mean\" href=\"notes/math/mean.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">mean</a> of the maximum and minimum values. It is calculated as: J. Han, M. Kamber, and J. Pei,&nbsp;Data Mining : Concepts and Techniques. Burlington, Ma: Elsevier, 2012.\n<br>GeeksforGeeks, “Range in Statistics,”&nbsp;GeeksforGeeks, Oct. 08, 2023. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/maths/range-in-statistics/\" target=\"_self\">https://www.geeksforgeeks.org/maths/range-in-statistics/</a>\n","aliases":[],"inlineTags":[],"frontmatterTags":["#math","#statistics"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Range","level":1,"id":"Range_0"},{"heading":"Advantages","level":3,"id":"Advantages_0"},{"heading":"Disadvantages","level":3,"id":"Disadvantages_0"},{"heading":"Types of Ranges","level":3,"id":"Types_of_Ranges_0"},{"heading":"References","level":2,"id":"References_0"}],"links":[".html","notes/math/outlier.html#_0","notes/math/outlier.html#_0",".html","notes/math/quartile.html#_0","notes/math/quartile.html#_0",".html","notes/math/mean.html#_0"],"author":"","coverImageURL":"","fullURL":"notes/math/range.html","pathToRoot":"../..","attachments":[],"createdTime":1752606554931,"modifiedTime":1754247593125,"sourceSize":1540,"sourcePath":"NOTES/Math/Range.md","exportPath":"notes/math/range.html","showInTree":true,"treeOrder":21,"backlinks":["notes/math/euclidean-distance.html","notes/math/manhattan-distance.html","notes/math/mean.html","notes/math/outlier.html","notes/math/standard-deviation.html","notes/math/variance.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown"},"notes/math/quartile.html":{"title":"Quartile","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-07Quartiles are a type of <a data-href=\"Quantile\" href=\"notes/math/quantile.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Quantile</a> that divide an ordered dataset into 4 equal parts. The quartiles of a dataset are three values, where each value represents a certain <a data-href=\"Percentile\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Percentile</a> of the data. Quartiles are a <a data-href=\"Measure of Dispersion\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Measure of Dispersion</a>, meaning it assesses the spread of a population.\n<br>First Quartile (Q1): The 25th percentile, meaning that 25% of the data falls below the first quartile. It can be thought of as the <a data-href=\"Median\" href=\"notes/math/median.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Median</a> of the lower half of the data.\n<br>Second Quartile (Q2): The 50th percentile, or the <a href=\"notes/math/median.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"31\" to=\"37\" origin-text=\"median\" class=\"internal-link virtual-link-a\">median</a> of the dataset. 50% of the data falls below the second quartile.\nThird Quartile (Q3): The 75th percentile, 75% of the data falls below the third quartile. It can be thought of as the median of the upper half of the dataset.\nThe five-number summary is a set of five values that describe the distribution of a dataset or population and consists of the following:\nMinimum: The smallest value in the dataset.\nFirst Quartile (Q1): The 25th percentile.\n<br><a href=\"notes/math/median.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"0\" to=\"6\" origin-text=\"Median\" class=\"internal-link virtual-link-a\">Median</a> (Q2): The 50th percentile.\nThird Quartile (Q3): The 75th percentile.\nMaximum: The largest value in the dataset.<br>\nThe five-number summary can be visualized using a <a data-href=\"Boxplot\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Boxplot</a> and is useful for identifying potential <a data-tooltip-position=\"top\" aria-label=\"Outlier\" data-href=\"Outlier\" href=\"notes/math/outlier.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Outliers</a> in a population.\nThe calculate the quartiles of a dataset, follow these steps:\nSort the data, typically in ascending order.\n<br>Find the median (Q2), which divides the dataset in half. If there’s an odd number of data points, exclude the <a href=\"notes/math/median.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"90\" to=\"96\" origin-text=\"median\" class=\"internal-link virtual-link-a\">median</a>; if even, include it in both halves.\nFind the first quartile, which is the median of the lower half of the dataset.\nFind the third quartile, which is the median of the upper half of the dataset.\n<br>The Interquartile <a href=\"notes/math/range.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"18\" to=\"23\" origin-text=\"Range\" class=\"internal-link virtual-link-a\">Range</a> (IQR) is a measure of statistical dispersion that represents the range within which the central 50% of the data points lie. It is defined as:<br>The IQR provides insights on the spread of the data around the <a href=\"notes/math/median.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"63\" to=\"69\" origin-text=\"median\" class=\"internal-link virtual-link-a\">median</a>, and is commonly used to identify <a href=\"notes/math/outlier.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"104\" to=\"112\" origin-text=\"outliers\" class=\"internal-link virtual-link-a\">outliers</a> where values that are below or above are considered outliers.<br>A boxplot, sometimes referred to as a whisker plot, provides a visual summary of a distribution of a dataset. The plot depicts the values of the <a data-href=\"Five-Number Summary\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Five-Number Summary</a> which consists of the <a href=\"notes/math/median.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"23\" to=\"29\" origin-text=\"median\" class=\"internal-link virtual-link-a\">median</a>, first and third quartiles, and the maximum and minimum values.The boxplot consists of:\n<br>The Box: Which represents the <a data-href=\"Interquartile Range\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Interquartile Range</a> of the distribution, which can be thought of as the middle 50% of the data.\n<br>Median Line: The line in the center of the box which represents the second quartile, or the <a href=\"notes/math/median.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"81\" to=\"87\" origin-text=\"median\" class=\"internal-link virtual-link-a\">median</a> of the dataset.\nWhiskers: The lines extending from the box to the value of the minimum or maximum value of the distribution. It is common for the whiskers to be set to the smallest and largest values with 1.5 times the IQR, rather than the true minimum and maximum values of the population.<br>\n<img alt=\"Pasted image 20250708093049.png\" src=\"resources/pasted-image-20250708093049.png\" target=\"_self\"><br>\n<a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.machinelearningplus.com/plots/python-boxplot/\" target=\"_self\">https://www.machinelearningplus.com/plots/python-boxplot/</a> J. Han, M. Kamber, and J. Pei,&nbsp;Data Mining : Concepts and Techniques. Burlington, Ma: Elsevier, 2012.\n<br>“Quartile,”&nbsp;Wikipedia, Nov. 26, 2021. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Quartile\" target=\"_self\">https://en.wikipedia.org/wiki/Quartile</a>\n<br>“Find a Five-Number Summary in Statistics: Easy Steps,”&nbsp;Statistics How To. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.statisticshowto.com/statistics-basics/how-to-find-a-five-number-summary-in-statistics/\" target=\"_self\">https://www.statisticshowto.com/statistics-basics/how-to-find-a-five-number-summary-in-statistics/</a>\n<br>M. Yi, “A Complete Guide to Box Plots,”&nbsp;Atlassian, 2025. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.atlassian.com/data/charts/box-plot-complete-guide\" target=\"_self\">https://www.atlassian.com/data/charts/box-plot-complete-guide</a>\n<br>S. Glen, “Interquartile <a href=\"notes/math/range.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"24\" to=\"29\" origin-text=\"Range\" class=\"internal-link virtual-link-a\">Range</a> (IQR): What it is and How to Find it,”&nbsp;Statistics How To, 2022. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.statisticshowto.com/probability-and-statistics/interquartile-range/\" target=\"_self\">https://www.statisticshowto.com/probability-and-statistics/interquartile-range/</a>\n","aliases":["Quartiles"],"inlineTags":[],"frontmatterTags":["#math","#statistics"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Quartile","level":1,"id":"Quartile_0"},{"heading":"<a data-href=\"Five-Number Summary\" href=\"Five-Number Summary\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Five-Number Summary</a>","level":3,"id":"[[Five-Number_Summary]]_0"},{"heading":"Calculation","level":2,"id":"Calculation_0"},{"heading":"<a data-href=\"Interquartile Range\" href=\"Interquartile Range\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Interquartile Range</a> (IQR)","level":2,"id":"[[Interquartile_Range]]_(IQR)_0"},{"heading":"<a data-href=\"Boxplot\" href=\"Boxplot\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Boxplot</a> and <span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Outlier.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"5\" to=\"12\" origin-text=\"Outlier\" class=\"internal-link virtual-link-a\">Outlier</a></span> Detection","level":2,"id":"[[Boxplot]]_and_Outlier_Detection_0"},{"heading":"Components","level":3,"id":"Components_0"},{"heading":"References","level":2,"id":"References_0"}],"links":["notes/math/quantile.html#_0",".html",".html","notes/math/median.html#_0","notes/math/median.html#_0",".html","notes/math/median.html#_0",".html","notes/math/outlier.html#_0","notes/math/median.html#_0",".html","notes/math/range.html#_0","notes/math/median.html#_0","notes/math/outlier.html#_0",".html","notes/math/outlier.html#_0",".html","notes/math/median.html#_0",".html","notes/math/median.html#_0","notes/math/range.html#_0"],"author":"","coverImageURL":"HTML/resources/pasted-image-20250708093049.png","fullURL":"notes/math/quartile.html","pathToRoot":"../..","attachments":["resources/pasted-image-20250708093049.png"],"createdTime":1751928764242,"modifiedTime":1754247589230,"sourceSize":4049,"sourcePath":"NOTES/Math/Quartile.md","exportPath":"notes/math/quartile.html","showInTree":true,"treeOrder":20,"backlinks":["notes/math/mean.html","notes/math/median.html","notes/math/outlier.html","notes/math/quantile.html","notes/math/range.html","notes/math/standard-deviation.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown"},"notes/math/quantile.html":{"title":"Quantile","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-16Quantiles are points in a dataset which divide the dataset into equal parts. Some examples of quantiles include the:\n<a data-href=\"Median\" href=\"notes/math/median.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Median</a>: which divides the dataset into two equal parts;\n<br><a data-tooltip-position=\"top\" aria-label=\"Quartile\" data-href=\"Quartile\" href=\"notes/math/quartile.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Quartiles</a>: which divide the dataset into four equal parts; and <br><a data-href=\"Percentiles\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Percentiles</a>: which divide the dataset into 100 equal parts.\n<br><img alt=\"Quartiles &amp; Quantiles | Calculation, Definition &amp; Interpretation\" src=\"https://www.scribbr.com/wp-content/uploads/2022/05/Quartiles-probability-distribution.webp\" referrerpolicy=\"no-referrer\" target=\"_self\" class=\"is-unresolved\"><br>\n<a data-tooltip-position=\"top\" aria-label=\"https://www.scribbr.com/statistics/quartiles-quantiles/\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.scribbr.com/statistics/quartiles-quantiles/\" target=\"_self\">Quartiles &amp; Quantiles | Calculation, Definition &amp; Interpretation</a>Q-quantiles are the values which divide a dataset into equal (or nearly equal) parts. The 100-quantiles (percentiles), for example, divide the dataset into 100 parts.To divide a dataset into equal parts:\nSort the dataset in ascending order.\nCalculate the position of the the th quantile using: <br>If is an integer, the quantile is the value at that position in the sorted dataset. If is not an integer, <a data-href=\"Interpolate\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Interpolate</a> it, that is, round it up, and take the <a data-href=\"Mean\" href=\"notes/math/mean.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Mean</a> of the values at positions and .\nNumPy has multiple functions for computing quantiles including:\nnumpy.percentile(): Which takes a dataset and percentile (e.g. 50) as arguments, and returns the value at that percentile.\nnumpy.quantile(): Which takes a dataset and a decimal value representing the percentile (e.g. 0.50) as input, and returns the value at that quantile.\n<br>Quantiles are commonly used to summarize the distribution of a dataset, and is commonly used as both a <a data-tooltip-position=\"top\" aria-label=\"Measure of Central Tendency\" data-href=\"Measure of Central Tendency\" href=\"notes/math/measure-of-central-tendency.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">measure of central tendency</a> and a <a data-href=\"Measure of Dispersion\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Measure of Dispersion</a>.<br>Quantiles are commonly used to identify <a href=\"notes/math/outlier.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"40\" to=\"48\" origin-text=\"outliers\" class=\"internal-link virtual-link-a\">outliers</a>. One method involves flagging any observations that are more than 1.5 times the <a data-href=\"Interquartile Range\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Interquartile Range</a> (IQR) above the third <a href=\"notes/math/quartile.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"23\" to=\"31\" origin-text=\"quartile\" class=\"internal-link virtual-link-a\">quartile</a> or below the first quartile.\n<br>\n<a data-href=\"Box Plot\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Box Plot</a>: Visualizes the <a data-href=\"Five-Number Summary\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Five-Number Summary</a> of a dataset, where the box represents the IQR, the line in the box is the <a data-href=\"Median\" href=\"notes/math/median.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Median</a>, the \"whiskers\" represent the 25% of the data below and above the first and third <a href=\"notes/math/quartile.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"83\" to=\"91\" origin-text=\"quartile\" class=\"internal-link virtual-link-a\">quartile</a> respectively, and the lines at the edge of each whisker represents the minimum and maximum values. <br>\n<a data-href=\"Q-Q Plot\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Q-Q Plot</a>: A comparative visualization method which plots the quantiles of two distributions against each other, where typically, a real dataset is plotted against a theoretical dataset (usually a normal distribution). <br>\n<a data-href=\"Violin Plot\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Violin Plot</a>: Overlays density curves over a box plot, where the width of the curve indicates the density of data points at specific values. J. Han, M. Kamber, and J. Pei,&nbsp;Data Mining : Concepts and Techniques. Burlington, Ma: Elsevier, 2012.\n<br>GeeksforGeeks, “Quantiles in Machine Learning,”&nbsp;GeeksforGeeks, Mar. 12, 2024. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/data-science/quantiles-in-machine-learning/\" target=\"_self\">https://www.geeksforgeeks.org/data-science/quantiles-in-machine-learning/</a>\n<br>“Quantile,”&nbsp;Wikipedia, Dec. 12, 2020. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Quantile\" target=\"_self\">https://en.wikipedia.org/wiki/Quantile</a>\n<br>Atlassian, “A Complete Guide to Violin Plots,”&nbsp;Atlassian. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.atlassian.com/data/charts/violin-plot-complete-guide\" target=\"_self\">https://www.atlassian.com/data/charts/violin-plot-complete-guide</a>\n","aliases":["Quantiles"],"inlineTags":[],"frontmatterTags":["#math","#statistics"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Quantile","level":1,"id":"Quantile_0"},{"heading":"Q-Quantiles","level":4,"id":"Q-Quantiles_0"},{"heading":"Calculation","level":2,"id":"Calculation_0"},{"heading":"Using <a data-href=\"NumPy\" href=\"NumPy\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">NumPy</a>","level":3,"id":"Using_[[NumPy]]_0"},{"heading":"Applications","level":2,"id":"Applications_0"},{"heading":"Descriptive Statistics","level":3,"id":"Descriptive_Statistics_0"},{"heading":"<a data-href=\"Outlier\" href=\"Outlier\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Outlier</a> Detection","level":3,"id":"[[Outlier]]_Detection_0"},{"heading":"Visualization","level":3,"id":"Visualization_0"},{"heading":"References","level":2,"id":"References_0"}],"links":["notes/math/median.html#_0","notes/math/quartile.html#_0",".html",".html","notes/math/mean.html#_0",".html","notes/math/measure-of-central-tendency.html#_0",".html","notes/math/outlier.html#_0","notes/math/outlier.html#_0",".html","notes/math/quartile.html#_0",".html",".html","notes/math/median.html#_0","notes/math/quartile.html#_0",".html",".html"],"author":"","coverImageURL":"https://www.scribbr.com/wp-content/uploads/2022/05/Quartiles-probability-distribution.webp","fullURL":"notes/math/quantile.html","pathToRoot":"../..","attachments":[],"createdTime":1752697090804,"modifiedTime":1754247586326,"sourceSize":3487,"sourcePath":"NOTES/Math/Quantile.md","exportPath":"notes/math/quantile.html","showInTree":true,"treeOrder":19,"backlinks":["notes/math/quartile.html","textbooks/data-mining-concepts-and-techniques/practice-problems.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown"},"notes/math/probability.html":{"title":"Probability","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-02Probability is a branch of mathematics that deals with the analysis of events and their likelihood of occurring. The probability of an event, written is a number between 0 and 1, with 0 representing impossible and 1 representing certainty. Probability theory is a framework for making inferences of events that have elements of uncertainty or randomness within them or their outcomes. Random Experiment - any process or action that yields uncertain outcomes.Sample Space - commonly denoted as , is the set of all possible outcomes of a random experiment.Event - a subset of the sample space, representing specific outcomes.Power Set: The set of all possible subsets of a sample space, including the empty set or .The probability of an event is calculated by dividing the number of favorable outcomes by the total size of the sample space:Joint probability is the probability of 2 events happening simultaneously. If 2 events are independent, then their joint probability is:If 2 events are mutually exclusive, then the probability of one or the other occurring is equal to the sum of their probabilities:If 2 events aren't necessarily mutually exclusive, then you simply subtract their intersection in order to prevent counting the values in their intersection twice:The probability of one event occurring, given that another has already occurred, denoted as (the probability of A given B), is:Bayes' Theorem states that:\nNon-Negativity: The probability of an event can never be negative: Normalization: The probability of an entire sample space is always 1: Countable Additivity: The probability of any countable sequence of disjoint (mutually exclusive) events, is equal to the sum of the probabilities of the individual events: Complement Rule: The complement of is denoted as or and is equal to: Mutual Exclusivity: If events A and B are mutually exclusive, that is they can never occur simultaneously, then: Empty Set: The probability of the empty set is always 0: <a data-href=\"Law of Total Probability\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Law of Total Probability</a>: If events are mutually exclusive and form a partition of the sample space, and is any event, the law of total probability states: <br><a data-href=\"Central Limit Theorem\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Central Limit Theorem</a>: States that if we take random samples from any population, the <a data-href=\"Mean\" href=\"notes/math/mean.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Mean</a> of those samples will form a normal distribution as the sample size gets sufficiently large.\n<br><a data-href=\"Law of Large Numbers\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Law of Large Numbers</a>: States that the <a href=\"notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"18\" to=\"22\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a> of the outcomes obtained from a large number of independent samples will converge to the expected value of the underlying probability distribution.\nThe factorial of a non-negative integer , denoted , is the product of all positive integers less than or equal to :\nA permutation of a set is a possible arrangement of it's elements, where the order of the elements matters. The number of possible permutations of elements from a set with total elements is given by:\nA combination of a set is a selection of it's elements where the order does not matter. The number of combinations of objects from a set with total elements is given by:\nA random variable is a formalization that assigns a numerical value to the outcome of a random event or experiment. Random variables can be classified into two main types:\nDiscrete random variables take on a countable number of distinct values.\n<br>Continuous random variables take on a infinite number of values within a <a href=\"notes/math/range.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"46\" to=\"51\" origin-text=\"range\" class=\"internal-link virtual-link-a\">range</a>.\nRandom variables are typically denoted by capital letters. For example, the set can represent the random variables 'heads' and 'tails' in a coin flip experiment. Their values are represented with their corresponding lowercase letter.A probability distribution is a mathematical function which describes the likelihood of different outcomes for the domain of a random variable. The probability of each outcome is between 0 and 1 (inclusive), and the sum of probabilities of each outcome must sum to 1.Discrete probability distributions describe the probability of each possible value in the domain of a discrete random variable.<br>The <a data-href=\"Probability Mass Function\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Probability Mass Function</a> (PMF) gives the probability of each possible value of a random variable :The sum of all outcomes must sum to 1:Common Discrete Distributions:\n<br><a data-href=\"Binomial Distribution\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Binomial Distribution</a> - Models the number of successes in a fixed amount of independent Bernoulli trials.\n<br><a data-href=\"Bernoulli Distribution\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Bernoulli Distribution</a> - Models the distribution of a random variable with two possible outcomes.\n<br><a data-href=\"Poisson Distribution\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Poisson Distribution</a> - Models the probability of a number of events occurring in a fixed interval given a constant <a href=\"notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"95\" to=\"99\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a> rate .\n<br>Continuous probability distributions describe the probability of a continuous random variable. These distributions are characterized by a <a data-href=\"Probability Density Function\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Probability Density Function</a> (PDF).<br>The probability density function describes the likelihood of a random variable taking on a value in a specific <a href=\"notes/math/range.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"111\" to=\"116\" origin-text=\"range\" class=\"internal-link virtual-link-a\">range</a>:where is the PDF and must satisfy:Common Continuous Distributions:\n<br><a data-href=\"Normal Distribution\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Normal Distribution</a> - Also called the Gaussian distribution, represents a bell curve that is symmetric around the <a href=\"notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"95\" to=\"99\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a>.\n<br><a data-href=\"Exponential Distribution\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Exponential Distribution</a> - Skewed to the right, often used to model the time or space between events in a <a data-href=\"Poisson Process\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Poisson Process</a>.\n<br><a data-href=\"Uniform Distribution\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Uniform Distribution</a> - Rectangle shape, indicating that all outcomes are equally likely.\n<br>The expected value, or <a href=\"notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"23\" to=\"27\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a>, of a random variable , denoted , is a <a data-tooltip-position=\"top\" aria-label=\"Measure of Central Tendency\" data-href=\"Measure of Central Tendency\" href=\"notes/math/measure-of-central-tendency.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">measure of central tendency</a> that represents the average outcome of a random variable. This concept is crucial in various fields, including artificial intelligence, especially in the context of stochastic (random) task environments, where uncertainty plays a significant role.For discrete probability distributions, the expected value is defined as:<br>Where is the <a data-href=\"Probability Mass Function\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Probability Mass Function</a>.For continuous probability distributions, the expected value is defined as:<br>Where is the <a data-href=\"Probability Density Function\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Probability Density Function</a>.Probability is the foundation of many Machine Learning algorithms such as:\n<br><a data-href=\"Naïve Bayes Classifier\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Naïve Bayes Classifier</a> - a classification model based on Bayes' theorem with strong assumptions about feature independence.\n<br><a data-href=\"Hidden Markov Model\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Hidden Markov Model</a> - A statistical model that represents a system which is assumed to be a <a data-href=\"Markov Process\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Markov Process</a>.\nDecision theory utilizes probability to make rational decisions under uncertainty. Relevant subject include:\n<br><a data-href=\"Utility Function\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Utility Function</a> - Assigning subjective desirability to outcomes.\n<br><a data-href=\"Markov Decision Process\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Markov Decision Process</a> - A framework for modeling sequential decision-making with stochastic outcomes.\nA Bayesian Network is a probabilistic model implemented as a directed acyclic graph that represents a set of random variables and their conditional dependencies, as well as a set of conditional probability distribution tables.Game theory is a mathematical framework for modelling strategic interactions in a multi-agent interdependent environment. Probability is crucial for game theory, where agents face uncertainty over the actions of other players as well as the state of the game.<br>A Markov process is a mathematical model which models a stochastic environment that satisfies the <a data-href=\"Markov Property\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Markov Property</a>. The Markov property states that the future state depends only on the present state and not past states. Markov processes have transition probabilities between states, quantifying the chance of moving from one state to another.<br>Bayesian statistics is subfield of <a data-href=\"Statistics\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Statistics</a> that uses Bayes' theorem to update the probability of a hypothesis as new evidence becomes available. Bayesian methods are widely used in classification, regression, and decision-making algorithms.\nPeter. R. Norvig, Artificial Intelligence: A Modern Approach, Global Edition. 2021.\n<br>Wikipedia contributors, “Probability theory,” Wikipedia, Apr. 23, 2025. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Probability_theory\" target=\"_self\">https://en.wikipedia.org/wiki/Probability_theory</a>\n<br>Wikipedia Contributors, “Probability axioms,”&nbsp;Wikipedia, Dec. 05, 2019. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Probability_axioms\" target=\"_self\">https://en.wikipedia.org/wiki/Probability_axioms</a>\n<br>J. Soch, “Kolmogorov axioms of probability,”&nbsp;The Book of Statistical Proofs, Jul. 30, 2021. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://statproofbook.github.io/D/prob-ax.html\" target=\"_self\">https://statproofbook.github.io/D/prob-ax.html</a> (accessed Mar. 16, 2025)\n<br>Wikipedia Contributors, “Factorial,”&nbsp;Wikipedia, Oct. 18, 2019. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Factorial\" target=\"_self\">https://en.wikipedia.org/wiki/Factorial</a>\n<br>Wikipedia Contributors, “Permutation,”&nbsp;Wikipedia, Sep. 22, 2019. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Permutation\" target=\"_self\">https://en.wikipedia.org/wiki/Permutation</a>\n<br>“Bayesian statistics,”&nbsp;Wikipedia, May 29, 2020. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Bayesian_statistics\" target=\"_self\">https://en.wikipedia.org/wiki/Bayesian_statistics</a>\n","aliases":["Probabilities"],"inlineTags":[],"frontmatterTags":["#AI","#math","#statistics","#probability"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Probability","level":1,"id":"Probability_0"},{"heading":"Basic Definitions","level":2,"id":"Basic_Definitions_0"},{"heading":"Mathematics","level":2,"id":"Mathematics_0"},{"heading":"Joint Probability","level":3,"id":"Joint_Probability_0"},{"heading":"Conditional Probabilities","level":3,"id":"Conditional_Probabilities_0"},{"heading":"<a data-href=\"Bayes' Theorem\" href=\"Bayes' Theorem\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Bayes' Theorem</a>","level":4,"id":"[[Bayes'_Theorem]]_0"},{"heading":"Axioms of Probability","level":2,"id":"Axioms_of_Probability_0"},{"heading":"Kolmogorov's Axioms","level":3,"id":"Kolmogorov's_Axioms_0"},{"heading":"Properties of Probability","level":3,"id":"Properties_of_Probability_0"},{"heading":"<a data-href=\"Counting Techniques\" href=\"Counting Techniques\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Counting Techniques</a>","level":2,"id":"[[Counting_Techniques]]_0"},{"heading":"<a data-href=\"Factorial\" href=\"Factorial\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Factorial</a>","level":3,"id":"[[Factorial]]_0"},{"heading":"<a data-href=\"Permutation\" href=\"Permutation\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Permutation</a>","level":3,"id":"[[Permutation]]_0"},{"heading":"<a data-href=\"Combination\" href=\"Combination\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Combination</a>","level":3,"id":"[[Combination]]_0"},{"heading":"<a data-href=\"Random Variable\" href=\"Random Variable\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Random Variable</a>","level":2,"id":"[[Random_Variable]]_0"},{"heading":"<a data-href=\"Probability Distribution\" href=\"Probability Distribution\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Probability Distribution</a>","level":2,"id":"[[Probability_Distribution]]_0"},{"heading":"1. Discrete Probability Distributions","level":4,"id":"1._[[Discrete_Probability_Distributions]]_0"},{"heading":"2. Continuous Probability Distributions","level":4,"id":"2._[[Continuous_Probability_Distributions]]_0"},{"heading":"<a data-href=\"Expected Value\" href=\"Expected Value\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Expected Value</a>","level":2,"id":"[[Expected_Value]]_0"},{"heading":"Discrete Distributions","level":3,"id":"Discrete_Distributions_0"},{"heading":"Continuous Distributions","level":3,"id":"Continuous_Distributions_0"},{"heading":"Applications","level":2,"id":"Applications_0"},{"heading":"<a data-href=\"Machine Learning\" href=\"Machine Learning\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Machine Learning</a>","level":3,"id":"[[Machine_Learning]]_0"},{"heading":"<a data-href=\"Decision Theory\" href=\"Decision Theory\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Decision Theory</a>","level":3,"id":"[[Decision_Theory]]_0"},{"heading":"<a data-href=\"Bayesian Network\" href=\"Bayesian Network\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Bayesian Network</a>","level":3,"id":"[[Bayesian_Network]]_0"},{"heading":"<a data-href=\"Game Theory\" href=\"Game Theory\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Game Theory</a>","level":3,"id":"[[Game_Theory]]_0"},{"heading":"Advanced Topics","level":2,"id":"Advanced_Topics_0"},{"heading":"<a data-href=\"Markov Process\" href=\"Markov Process\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Markov Process</a>","level":3,"id":"[[Markov_Process]]_0"},{"heading":"<a data-href=\"Bayesian Statistics\" href=\"Bayesian Statistics\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Bayesian Statistics</a>","level":3,"id":"[[Bayesian_Statistics]]_0"},{"heading":"References","level":2,"id":"References_0"}],"links":[".html",".html",".html","notes/math/mean.html#_0",".html","notes/math/mean.html#_0",".html",".html",".html",".html",".html","notes/math/range.html#_0",".html",".html",".html",".html",".html",".html","notes/math/mean.html#_0",".html",".html","notes/math/range.html#_0",".html","notes/math/mean.html#_0",".html",".html",".html",".html","notes/math/mean.html#_0","notes/math/measure-of-central-tendency.html#_0",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html"],"author":"","coverImageURL":"","fullURL":"notes/math/probability.html","pathToRoot":"../..","attachments":[],"createdTime":1751495555242,"modifiedTime":1754247568321,"sourceSize":10549,"sourcePath":"NOTES/Math/Probability.md","exportPath":"notes/math/probability.html","showInTree":true,"treeOrder":17,"backlinks":["index.html","notes/math/binary-data.html","notes/math/mean.html","notes/math/variance.html","textbooks/ai-a-modern-approach/summary.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown"},"notes/math/outlier.html":{"title":"Outlier","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-08Outliers are data points that differ significantly from the other observations in a dataset. Outliers may occur due to measurement or recording error, or could possibly represent an important anomaly warranting further analysis. There is no fixed definition of what constitutes as an outlier, typically, specific domain knowledge is usually necessary to understand whether a specific observation is an outlier, or is a natural phenomenon of the dataset.\nMeasurement Error: Outliers may occur due to user error in the data collection process, or could occur due to errors in autonomous systems, such as sensor failure.\nNatural Variation: Outliers may represent perfectly legitimate values that are an inherent part of the naturally occurring variations in the underlying domain of the dataset.\nAnomaly: Outliers may represent unusual behavior, such as fraudulent transactions, which warrant further investigation and analysis. Outliers can have a significant impact on various statistical measures, such as <a data-href=\"Mean\" href=\"notes/math/mean.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Mean</a>, <a data-href=\"Standard Deviation\" href=\"notes/math/standard-deviation.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Standard Deviation</a>, or <a data-href=\"Range\" href=\"notes/math/range.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Range</a>.\n<br>Outliers could also hinder the performance of some machine learning models such as <a data-href=\"Logistic Regression\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Logistic Regression</a> or <a data-href=\"K-Nearest-Neighbors\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">K-Nearest-Neighbors</a>. Global Outliers: A global outlier deviates significantly from the entire population globally.\nCollective Outliers: A group or subset of data points that collectively deviate considerably from the overall distribution. Typically require special techniques to detect.\nContextual/Local Outliers: Data points whose value deviate significantly relative to other data points within the same \"context.\" Contextual outliers may not be considered outliers when considered globally, meaning they need special attention to be properly detected and analyzed.\nWhile the terms outliers and extreme values may be used interchangeably, they have distinct definitions in statistics:\nOutlier: is a data point that varies significantly from the rest of the dataset.\nExtreme values: values that reside at the outer edges of the dataset, representing the highest and lowest points in a dataset. Extreme values may be outliers, or they be a natural part of a distribution.\n<br>Z-score, sometimes called the standard score, measures how many <a data-tooltip-position=\"top\" aria-label=\"Standard Deviation\" data-href=\"Standard Deviation\" href=\"notes/math/standard-deviation.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Standard Deviations</a> a data object is from the <a data-tooltip-position=\"top\" aria-label=\"Mean\" data-href=\"Mean\" href=\"notes/math/mean.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">mean</a> of the distribution. A positive z-score indicates the values is greater than the <a href=\"notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"82\" to=\"86\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a>, while a negative score indicates it is less than the mean.Mathematically, the z-score is defined as:Where: is the z-score. is the given data value.\n<br> is the population <a href=\"notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"19\" to=\"23\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a>.\n<br> is the population <a href=\"notes/math/standard-deviation.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"20\" to=\"38\" origin-text=\"standard deviation\" class=\"internal-link virtual-link-a\">standard deviation</a>.\nThe z-score is commonly used to detect outliers by flagging any values that are outside of a specified threshold (commonly -3 and 3).<br>The Interquartile <a href=\"notes/math/range.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"18\" to=\"23\" origin-text=\"Range\" class=\"internal-link virtual-link-a\">Range</a> (IQR) is a measure of statistical dispersion that represents the range within which the central 50% of the data points lie.Mathematically, the IQR is defined as:Where:\n<br> is the value of the 3rd <a data-href=\"Quartile\" href=\"notes/math/quartile.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Quartile</a>.\n<br> is the value of the 1st <a href=\"notes/math/quartile.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"25\" to=\"33\" origin-text=\"Quartile\" class=\"internal-link virtual-link-a\">Quartile</a>.\nThe IQR is commonly used to detect outliers by flagging any values that are outside of a specified boundary, typically:\nLower Bound: Upper Bound: Any data point below the lower bound or above the upper bound is considered an outlier.\n<br>K-Nearest Neighbors is a supervised machine learning algorithm which can be used for both classification and regression tasks. The algorithm relies on <a data-tooltip-position=\"top\" aria-label=\"Distance Metric\" data-href=\"Distance Metric\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Distance Metrics</a> such as <a data-href=\"Euclidean Distance\" href=\"notes/math/euclidean-distance.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Euclidean Distance</a>, <a data-href=\"Manhattan Distance\" href=\"notes/math/manhattan-distance.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Manhattan Distance</a>, or <a data-href=\"Minkowski Distance\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Minkowski Distance</a> to find the \"-nearest neighbors\" of a data object, where is a parameter indicating the number of neighbors to consider when making a prediction.\n<br>Regression: For regression tasks, the algorithm takes the average values of the k-nearest-neighbors of the data object, where the neighbors come from the training set. Then the <a data-tooltip-position=\"top\" aria-label=\"Mean\" data-href=\"Mean\" href=\"notes/math/mean.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">mean</a> of the neighbors' values is the predicted value for the object.\nClassification: For classification tasks, a majority vote among the object's k-nearest-neighbors is taken to determine the category for the given data object.\nK-nearest-neighbors is used to detect outliers by assigning an outlier score to a data object, which is done by measuring the distance of an object from its nearest neighbors. Though, this approach is not effective for collective outliers.DBSCAN is a density-based clustering algorithm that clusters data based on the density of data points. DBSCAN excels at identifying an arbitrary number of clusters, and can handle nested clusters of arbitrary shapes.\nTwo parameters are chosen, represents the radius within which to classify neighbors; and minPts, which represents the minimum number of neighbors within to classify the point as a \"core point.\"\nA data point is considered a core point if the amount of other data points that falls within its radius is at least minPts.\nAfter all core points are identified, for each core point, create a cluster of the core point and all the points within its radius.\nExpand the cluster by iterating through all reachable points and adding them to the cluster if they're core points.\nPoints that are not reachable from any core points are labelled as noise or outliers.\nDBSCAN is popular algorithm for anomaly detection, since outliers are detected based on relative density of data.<br>One straightforward method for dealing with outliers is simply to remove them from the dataset. This approach if effective as long as outliers are not relevant to the analysis, such as the case where the focus is not on <a data-href=\"Anomaly Detection\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Anomaly Detection</a>.Transformations can be applied to the data to minimize the effect of outliers. Some popular techniques include:\n<br><a data-href=\"Scaling\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Scaling</a>: Adjusting the <a href=\"notes/math/range.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"16\" to=\"21\" origin-text=\"range\" class=\"internal-link virtual-link-a\">range</a> of the data to reduce the influence of extreme values. This includes methods such as <a data-href=\"Min-Max Scaling\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Min-Max Scaling</a> or <a data-href=\"Z-Score Normalization\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Z-Score Normalization</a>.\n<br><a data-href=\"Winsorization\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Winsorization</a>: Replacing outlier values with the nearest value within a specific percentile range. For example, any values outside of the middle 95% percentile are replaced with the nearest values within that range.\n<br><a data-href=\"Log Transformation\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Log Transformation</a>: Applying logarithmic transformations to reduce <a data-href=\"Variance\" href=\"notes/math/variance.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Variance</a> and make the data more normally distributed.\n<br>Another approach to handling outliers is explicitly modeling them. This can be done by adding a new <a data-href=\"Binary Data\" href=\"notes/math/binary-data.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Binary Data</a> attribute that specifies whether a given data object is an outlier or not.\nJ. Han, M. Kamber, and J. Pei,&nbsp;Data Mining : Concepts and Techniques. Burlington, Ma: Elsevier, 2012.\n<br>I. Cohen, “Outlier Detection &amp; Analysis: The Different Types of Outliers,”&nbsp;Anodot, Feb. 25, 2022. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.anodot.com/blog/quick-guide-different-types-outliers/\" target=\"_self\">https://www.anodot.com/blog/quick-guide-different-types-outliers/</a>\n<br>GeeksforGeeks, “Types of Outliers in Data Mining,”&nbsp;GeeksforGeeks, Jul. 2021. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/data-analysis/types-of-outliers-in-data-mining/\" target=\"_self\">https://www.geeksforgeeks.org/data-analysis/types-of-outliers-in-data-mining/</a> (accessed Jul. 08, 2025).\n<br>S. Glen, “Outliers: Finding Them in Data, Formula, Examples. Easy Steps and Video,”&nbsp;Statistics How To. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.statisticshowto.com/statistics-basics/find-outliers/\" target=\"_self\">https://www.statisticshowto.com/statistics-basics/find-outliers/</a>\n<br>Wikipedia Contributors, “Outlier,”&nbsp;Wikipedia, Apr. 07, 2019. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Outlier\" target=\"_self\">https://en.wikipedia.org/wiki/Outlier</a>\n<br>GeeksforGeeks, “How to Detect Outliers in Machine Learning,”&nbsp;GeeksforGeeks, Jan. 12, 2019. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/machine-learning/machine-learning-outlier/\" target=\"_self\">https://www.geeksforgeeks.org/machine-learning/machine-learning-outlier/</a> (accessed Jul. 10, 2025).\n<br>Wikipedia Contributors, “Standard score,”&nbsp;Wikipedia, Sep. 12, 2019. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Standard_score\" target=\"_self\">https://en.wikipedia.org/wiki/Standard_score</a>\n","aliases":["Outliers"],"inlineTags":[],"frontmatterTags":["#math","#statistics"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Outlier","level":1,"id":"Outlier_0"},{"heading":"Causes of Outliers","level":3,"id":"Causes_of_Outliers_0"},{"heading":"Impact","level":3,"id":"Impact_0"},{"heading":"Types of Outliers","level":3,"id":"Types_of_Outliers_0"},{"heading":"Outliers vs. Extreme Values","level":3,"id":"Outliers_vs._Extreme_Values_0"},{"heading":"Detection","level":2,"id":"Detection_0"},{"heading":"<a data-href=\"Z-Score\" href=\"Z-Score\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Z-Score</a>","level":3,"id":"[[Z-Score]]_0"},{"heading":"Formula","level":4,"id":"Formula_0"},{"heading":"Application","level":4,"id":"Application_0"},{"heading":"<a data-href=\"Interquartile Range\" href=\"Interquartile Range\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Interquartile Range</a> (IQR)","level":3,"id":"[[Interquartile_Range]]_(IQR)_0"},{"heading":"Formula","level":4,"id":"Formula_1"},{"heading":"Application","level":4,"id":"Application_1"},{"heading":"<a data-href=\"K-Nearest Neighbors\" href=\"K-Nearest Neighbors\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">K-Nearest Neighbors</a>","level":3,"id":"[[K-Nearest_Neighbors]]_0"},{"heading":"Intuition","level":4,"id":"Intuition_0"},{"heading":"Application","level":4,"id":"Application_2"},{"heading":"<a data-tooltip-position=\"top\" aria-label=\"DBSCAN\" data-href=\"DBSCAN\" href=\"DBSCAN\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Density-Based Spatial Clustering of Applications with Noise</a> (DBSCAN)","level":3,"id":"[[DBSCAN|Density-Based_Spatial_Clustering_of_Applications_with_Noise]]_(DBSCAN)_0"},{"heading":"Intuition","level":4,"id":"Intuition_1"},{"heading":"Application","level":4,"id":"Application_3"},{"heading":"Handling Outliers","level":2,"id":"Handling_Outliers_0"},{"heading":"Removal","level":3,"id":"Removal_0"},{"heading":"Transformation","level":3,"id":"Transformation_0"},{"heading":"Explicit Modeling","level":3,"id":"Explicit_Modeling_0"},{"heading":"References","level":2,"id":"References_0"}],"links":["notes/math/mean.html#_0","notes/math/standard-deviation.html#_0","notes/math/range.html#_0",".html",".html",".html","notes/math/standard-deviation.html#_0","notes/math/mean.html#_0","notes/math/mean.html#_0","notes/math/mean.html#_0","notes/math/standard-deviation.html#_0",".html","notes/math/range.html#_0","notes/math/quartile.html#_0","notes/math/quartile.html#_0",".html",".html","notes/math/euclidean-distance.html#_0","notes/math/manhattan-distance.html#_0",".html","notes/math/mean.html#_0",".html",".html",".html","notes/math/range.html#_0",".html",".html",".html",".html","notes/math/variance.html#_0","notes/math/binary-data.html#_0"],"author":"","coverImageURL":"","fullURL":"notes/math/outlier.html","pathToRoot":"../..","attachments":[],"createdTime":1752013301145,"modifiedTime":1754247552925,"sourceSize":8384,"sourcePath":"NOTES/Math/Outlier.md","exportPath":"notes/math/outlier.html","showInTree":true,"treeOrder":16,"backlinks":["notes/math/euclidean-distance.html","notes/math/least-squares.html","notes/math/manhattan-distance.html","notes/math/mean.html","notes/math/measure-of-central-tendency.html","notes/math/median.html","notes/math/quantile.html","notes/math/quartile.html","notes/math/range.html","notes/math/standard-deviation.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown"},"notes/math/ordinal-data.html":{"title":"Ordinal Data","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-07Ordinal data is a type of <a data-href=\"Categorical Data\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Categorical Data</a> that represents categories that a defined order or ranking. Unlike <a data-href=\"Nominal Data\" href=\"notes/math/nominal-data.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Nominal Data</a>, which has no intrinsic ordering, ordinal data allows for the comparison of the relative positioning of items.A key property of ordinal data is that the interval between adjacent ranks is not necessarily equal. For example, the interval between \"expensive\" and \"average\" is not necessarily equal to the interval between \"cheap\" and \"average.\"Another key property of Ordinal data is that it is non-numeric. While ordinal variables may be represented by numbers (e.g. 1, 2, 3), these numbers are only labels that indicate orders and not precise values.Examples of ordinal data include attributes like education levels (e.g. High School, Bachelor's, Masters), or survey responses (e.g. Unsatisfied, Neutral, Satisfied).The analysis of ordinal data requires specific statistical techniques that respect the inherent order of the categories while acknowledging that the intervals between them are not necessarily uniform.\n<br>The <a data-href=\"Median\" href=\"notes/math/median.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Median</a> and <a data-href=\"Mode\" href=\"notes/math/mode.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Mode</a> can be used to summarize the central tendencies and identify the most frequent categories.\n<br><a data-tooltip-position=\"top\" aria-label=\"Bar Chart\" data-href=\"Bar Chart\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Bar Charts</a> can visualize the frequency distribution of categories and illustrate the most common or uncommon values. <br>Because ordinal data does not meet the assumptions required for <a data-tooltip-position=\"top\" aria-label=\"Parametric Test\" data-href=\"Parametric Test\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Parametric Tests</a>, <a data-tooltip-position=\"top\" aria-label=\"Non-Parametric Test\" data-href=\"Non-Parametric Test\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Non-Parametric Tests</a>, such as <a data-href=\"Mann-Whitney U Test\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Mann-Whitney U Test</a> or <a data-href=\"Kruskal-Wallis Test\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Kruskal-Wallis Test</a> are often used to analyze the difference between groups.\n<br><a data-href=\"Spearman's Rank\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Spearman's Rank</a> correlation can be used to assess relationships between ordinal attributes, providing potential insights on the strength and direction of correlations. J. Han and M. Kamber,&nbsp;Data Mining : Concepts and Techniques, 3rd ed. Haryana, India ; Burlington, Ma: Elsevier, 2018.\n<br>“Types of Data | Introduction to Data Science,”&nbsp;<a data-tooltip-position=\"top\" aria-label=\"http://www.stat.lsa.umich.edu\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://www.stat.lsa.umich.edu\" target=\"_self\">www.stat.lsa.umich.edu</a>. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://dept.stat.lsa.umich.edu/~kshedden/introds/topics/types_of_data/\" target=\"_self\">https://dept.stat.lsa.umich.edu/~kshedden/introds/topics/types_of_data/</a>\n<br>“Ordinal data,”&nbsp;Wikipedia, Apr. 03, 2020. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Ordinal_data\" target=\"_self\">https://en.wikipedia.org/wiki/Ordinal_data</a>\n","aliases":["Ordinal"],"inlineTags":[],"frontmatterTags":["#math","#statistics"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Ordinal Data","level":1,"id":"Ordinal_Data_0"},{"heading":"Unequal Intervals","level":4,"id":"Unequal_Intervals_0"},{"heading":"Non-Numeric","level":4,"id":"Non-Numeric_0"},{"heading":"Examples","level":4,"id":"Examples_0"},{"heading":"Analysis","level":2,"id":"Analysis_0"},{"heading":"Descriptive Statistics","level":3,"id":"Descriptive_Statistics_0"},{"heading":"Statistical Techniques","level":3,"id":"Statistical_Techniques_0"},{"heading":"References","level":2,"id":"References_0"}],"links":[".html","notes/math/nominal-data.html#_0","notes/math/median.html#_0","notes/math/mode.html#_0",".html",".html",".html",".html",".html",".html"],"author":"","coverImageURL":"","fullURL":"notes/math/ordinal-data.html","pathToRoot":"../..","attachments":[],"createdTime":1751919439086,"modifiedTime":1754247549994,"sourceSize":2402,"sourcePath":"NOTES/Math/Ordinal Data.md","exportPath":"notes/math/ordinal-data.html","showInTree":true,"treeOrder":15,"backlinks":["notes/math/measure-of-central-tendency.html","notes/math/nominal-data.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown"},"notes/math/nominal-data.html":{"title":"Nominal Data","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-07Nominal data is a type of <a data-href=\"Categorical Data\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Categorical Data</a> that represents categories that don't have a specific order or ranking. Unlike <a data-href=\"Ordinal Data\" href=\"notes/math/ordinal-data.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Ordinal Data</a>, which has a defined order, nominal data is purely qualitative. It describes specific qualities without conveying any measure of numerical significance.<br>Since nominal data lacks numerical significance, data operations such as <a data-href=\"Mean\" href=\"notes/math/mean.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Mean</a> or <a data-href=\"Median\" href=\"notes/math/median.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Median</a> cannot be performed. However, the frequency of nominal data values can be analyzed, and a measure like the <a data-href=\"Mode\" href=\"notes/math/mode.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Mode</a> can describe the most common category within a given dataset.<br>A specific type of nominal data is <a data-href=\"Binary Data\" href=\"notes/math/binary-data.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Binary Data</a>, which consists of only two categories, typically represented as 1 and 0. Binary fields are commonly used to represent whether a certain feature is present or not in a given data object.Examples of nominal data include attributes like eye color (e.g., blue, brown, green), gender (e.g., male, female, non-binary), and nationality (e.g., American, Canadian). These categories do not imply any hierarchy or order.<br>Nominal data, lacking any numerical significance, primarily relies on frequency distribution analysis for meaningful insights. One of the most effective techniques for this is the use of a <a data-href=\"Bar Chart\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Bar Chart</a>, which visually represent the frequency of each category.\nJ. Han and M. Kamber,&nbsp;Data Mining : Concepts and Techniques, 3rd ed. Haryana, India ; Burlington, Ma: Elsevier, 2018.\n<br>“Types of Data | Introduction to Data Science,”&nbsp;_<a data-tooltip-position=\"top\" aria-label=\"http://www.stat.lsa.umich.edu.\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://www.stat.lsa.umich.edu.\" target=\"_self\">www.stat.lsa.umich.edu.</a> <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://dept.stat.lsa.umich.edu/~kshedden/introds/topics/types_of_data/\" target=\"_self\">https://dept.stat.lsa.umich.edu/~kshedden/introds/topics/types_of_data/</a>\nWikipedia Contributors, “Nominal category,”&nbsp;Wikipedia, Oct. 07, 2024.\n","aliases":["Nominal"],"inlineTags":[],"frontmatterTags":["#math","#statistics","#unfinished"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Nominal Data","level":1,"id":"Nominal_Data_0"},{"heading":"No Numerical Significance","level":4,"id":"No_Numerical_Significance_0"},{"heading":"<span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Binary Data.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"0\" to=\"11\" origin-text=\"Binary Data\" class=\"internal-link virtual-link-a\">Binary Data</a></span>","level":4,"id":"Binary_Data_0"},{"heading":"Examples","level":4,"id":"Examples_0"},{"heading":"Analysis","level":4,"id":"Analysis_0"},{"heading":"Encoding","level":2,"id":"Encoding_0"},{"heading":"References","level":2,"id":"References_0"}],"links":[".html","notes/math/ordinal-data.html#_0","notes/math/mean.html#_0","notes/math/median.html#_0","notes/math/mode.html#_0","notes/math/binary-data.html#_0","notes/math/binary-data.html#_0",".html"],"author":"","coverImageURL":"","fullURL":"notes/math/nominal-data.html","pathToRoot":"../..","attachments":[],"createdTime":1751914187819,"modifiedTime":1754247547097,"sourceSize":1919,"sourcePath":"NOTES/Math/Nominal Data.md","exportPath":"notes/math/nominal-data.html","showInTree":true,"treeOrder":14,"backlinks":["notes/math/binary-data.html","notes/math/measure-of-central-tendency.html","notes/math/ordinal-data.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown"},"notes/math/mode.html":{"title":"Mode","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-08The mode is a <a data-href=\"Measure of Central Tendency\" href=\"notes/math/measure-of-central-tendency.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Measure of Central Tendency</a> which represents the most frequently occurring value in a dataset or population. The mode is particularly versatile since it can be applied to both <a data-href=\"Categorical Data\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Categorical Data</a> and <a data-href=\"Numerical Data\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Numerical Data</a>.\nUnimodal: A dataset with only one mode.\nBimodal: A dataset with exactly two modes.\nMultimodal: A dataset with three or more modes.\n<br>In a discrete <a href=\"notes/math/probability.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"14\" to=\"25\" origin-text=\"probability\" class=\"internal-link virtual-link-a\">probability</a> distribution, the mode is the value that has the highest probability of occurring.<br>In a continuous <a href=\"notes/math/probability.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"16\" to=\"27\" origin-text=\"probability\" class=\"internal-link virtual-link-a\">probability</a> distribution, the mode is the value at which the <a data-href=\"Probability Density Function\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Probability Density Function</a> reaches it's maximum. It represents the peak of the distribution.<br>Other common <a data-tooltip-position=\"top\" aria-label=\"Measure of Central Tendency\" data-href=\"Measure of Central Tendency\" href=\"notes/math/measure-of-central-tendency.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Measures of Central Tendency</a> include the <a data-href=\"Median\" href=\"notes/math/median.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Median</a>, which represents the middle value when the data is ordered, the <a data-href=\"Mean\" href=\"notes/math/mean.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Mean</a>, which represents the average value of a population, and the <a data-href=\"Midrange\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Midrange</a>, calculated as the average of the maximum and minimum values.\nJ. Han, M. Kamber, and J. Pei,&nbsp;Data Mining : Concepts and Techniques. Burlington, Ma: Elsevier, 2012.\n<br>GeeksforGeeks, “Mode in Statistics | Definition, Formula, How to Calculate Mode,”&nbsp;GeeksforGeeks, Sep. 20, 2021. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/maths/what-is-mode/\" target=\"_self\">https://www.geeksforgeeks.org/maths/what-is-mode/</a>\n<br>Wikipedia Contributors, “Mode (statistics),”&nbsp;Wikipedia, Oct. 10, 2019. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Mode_(statistics)\" target=\"_self\">https://en.wikipedia.org/wiki/Mode_(statistics)</a>\n","aliases":[],"inlineTags":[],"frontmatterTags":["#math","#statistics"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Mode","level":1,"id":"Mode_0"},{"heading":"Types of Modes","level":3,"id":"Types_of_Modes_0"},{"heading":"Mode in <a data-href=\"Probability Distributions\" href=\"Probability Distributions\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Probability Distributions</a>","level":2,"id":"Mode_in_[[Probability_Distributions]]_0"},{"heading":"<a data-href=\"Discrete Probability Distribution\" href=\"Discrete Probability Distribution\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Discrete Probability Distribution</a>","level":4,"id":"[[Discrete_Probability_Distribution]]_0"},{"heading":"<a data-href=\"Continuous Probability Distribution\" href=\"Continuous Probability Distribution\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Continuous Probability Distribution</a>","level":4,"id":"[[Continuous_Probability_Distribution]]_0"},{"heading":"Other <span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Measure of Central Tendency.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"6\" to=\"34\" origin-text=\"Measures of Central Tendency\" class=\"internal-link virtual-link-a\">Measures of Central Tendency</a></span>","level":2,"id":"Other_Measures_of_Central_Tendency_0"},{"heading":"References","level":2,"id":"References_0"}],"links":["notes/math/measure-of-central-tendency.html#_0",".html",".html",".html",".html","notes/math/probability.html#_0",".html","notes/math/probability.html#_0",".html","notes/math/measure-of-central-tendency.html#_0","notes/math/measure-of-central-tendency.html#_0","notes/math/median.html#_0","notes/math/mean.html#_0",".html"],"author":"","coverImageURL":"","fullURL":"notes/math/mode.html","pathToRoot":"../..","attachments":[],"createdTime":1752009968333,"modifiedTime":1754247543991,"sourceSize":1720,"sourcePath":"NOTES/Math/Mode.md","exportPath":"notes/math/mode.html","showInTree":true,"treeOrder":13,"backlinks":["notes/math/binary-data.html","notes/math/mean.html","notes/math/measure-of-central-tendency.html","notes/math/median.html","notes/math/nominal-data.html","notes/math/ordinal-data.html","textbooks/data-mining-concepts-and-techniques/practice-problems.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown"},"notes/math/median.html":{"title":"Median","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-07The Median is a <a data-href=\"Measure of Central Tendency\" href=\"notes/math/measure-of-central-tendency.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Measure of Central Tendency</a> that represents the middle value of an ordered dataset. It is particularly useful in scenarios where the data is skewed or contains <a data-tooltip-position=\"top\" aria-label=\"Outlier\" data-href=\"Outlier\" href=\"notes/math/outlier.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">outliers</a>, as it provides a more accurate representation of the center of the dataset compared to other measures like the <a data-href=\"Mean\" href=\"notes/math/mean.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Mean</a>. The median can only be applied to <a data-href=\"Numerical Data\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Numerical Data</a> and not <a data-href=\"Categorical Data\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Categorical Data</a>.<br>The median is defined as the 50th <a data-href=\"Percentile\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Percentile</a> or the second <a data-href=\"Quartile\" href=\"notes/math/quartile.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Quartile</a> (Q2), which divides the dataset into two equal halves. This means that half of the data points are below the median and half are above it.\nUniqueness: In a finite dataset, the median is unique, meaning there is only one median.\n<br>Robustness: The median is robust to <a href=\"notes/math/outlier.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"26\" to=\"34\" origin-text=\"outliers\" class=\"internal-link virtual-link-a\">outliers</a>, making it a reliable measure in skewed or noisy datasets.\nNon-Parametric: The median does not assume a particular distribution of the underlying data.\nInvariance: The median remains unchanged under linear transformations of the dataset.\nIf is even, the formula for the median is:While if is odd, the formula for the median is:<br>The multivariate median extends the concept of the median to multiple dimensions. One of the most common multivariate median is the <a data-href=\"Geometric Median\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Geometric Median</a> which focuses on minimizing the <a data-tooltip-position=\"top\" aria-label=\"Euclidean Distance\" data-href=\"Euclidean Distance\" href=\"notes/math/euclidean-distance.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Euclidean distance</a> of a set of points in a Euclidean space. The geometric median is defined as:Where:\n<br> denotes the <a href=\"notes/math/euclidean-distance.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"13\" to=\"31\" origin-text=\"Euclidean distance\" class=\"internal-link virtual-link-a\">Euclidean distance</a>. represents the -th data point in -dimensional space. <br>Machine Learning: Multivariate medians are commonly employed in machine learning, particularly in clustering algorithms when dealing with centroid initialization, such as in <a data-href=\"DBSCAN\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">DBSCAN</a>.\nComputer Vision: The geometric median can be used to find a central point among pixel locations, helping in tasks such as object tracking.\n<br><a data-href=\"Robust Statistics\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Robust Statistics</a>: Multivariate medians are utilized as a <a href=\"notes/math/measure-of-central-tendency.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"41\" to=\"68\" origin-text=\"measure of central tendency\" class=\"internal-link virtual-link-a\">measure of central tendency</a> over other measures such as the <a href=\"notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"101\" to=\"105\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a>, due to median's robustness to <a href=\"notes/math/outlier.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"137\" to=\"145\" origin-text=\"outliers\" class=\"internal-link virtual-link-a\">outliers</a>.\n<br>Other common <a data-tooltip-position=\"top\" aria-label=\"Measure of Central Tendency\" data-href=\"Measure of Central Tendency\" href=\"notes/math/measure-of-central-tendency.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Measures of Central Tendency</a> include the <a data-href=\"Mean\" href=\"notes/math/mean.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Mean</a>, which represents the average value of a population, the <a data-href=\"Mode\" href=\"notes/math/mode.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Mode</a>, which identifies the most frequently occurring value in a set, and the <a data-href=\"Midrange\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Midrange</a>, calculated as the average of the maximum and minimum values.\nJ. Han, M. Kamber, and J. Pei,&nbsp;Data Mining : Concepts and Techniques. Burlington, Ma: Elsevier, 2012.\n<br>Wikipedia, “Median,”&nbsp;Wikipedia, Apr. 17, 2020. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Median\" target=\"_self\">https://en.wikipedia.org/wiki/Median</a>\n","aliases":["Medians"],"inlineTags":[],"frontmatterTags":["#math","#statistics"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Median","level":1,"id":"Median_0"},{"heading":"Definition","level":4,"id":"Definition_0"},{"heading":"Properties","level":4,"id":"Properties_0"},{"heading":"Formula","level":2,"id":"Formula_0"},{"heading":"Multivariate Median","level":2,"id":"Multivariate_Median_0"},{"heading":"Applications","level":3,"id":"Applications_0"},{"heading":"Other <span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Measure of Central Tendency.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"6\" to=\"34\" origin-text=\"Measures of Central Tendency\" class=\"internal-link virtual-link-a\">Measures of Central Tendency</a></span>","level":2,"id":"Other_Measures_of_Central_Tendency_0"},{"heading":"References","level":2,"id":"References_0"}],"links":["notes/math/measure-of-central-tendency.html#_0","notes/math/outlier.html#_0","notes/math/mean.html#_0",".html",".html",".html","notes/math/quartile.html#_0","notes/math/outlier.html#_0",".html","notes/math/euclidean-distance.html#_0","notes/math/euclidean-distance.html#_0",".html",".html","notes/math/measure-of-central-tendency.html#_0","notes/math/mean.html#_0","notes/math/outlier.html#_0","notes/math/measure-of-central-tendency.html#_0","notes/math/measure-of-central-tendency.html#_0","notes/math/mean.html#_0","notes/math/mode.html#_0",".html"],"author":"","coverImageURL":"","fullURL":"notes/math/median.html","pathToRoot":"../..","attachments":[],"createdTime":1751927785580,"modifiedTime":1754247536481,"sourceSize":3023,"sourcePath":"NOTES/Math/Median.md","exportPath":"notes/math/median.html","showInTree":true,"treeOrder":12,"backlinks":["notes/math/binary-data.html","notes/math/mean.html","notes/math/measure-of-central-tendency.html","notes/math/mode.html","notes/math/nominal-data.html","notes/math/ordinal-data.html","notes/math/quantile.html","notes/math/quartile.html","textbooks/data-mining-concepts-and-techniques/practice-problems.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown"},"notes/math/measure-of-central-tendency.html":{"title":"Measure of Central Tendency","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-06A measure of central tendency is a statistical measure that attempts to describe the center of a dataset. The measure attempts to summarize the dataset with a single value that represents the middle or \"average\" of the data. The most common measures are the <a data-href=\"Mean\" href=\"notes/math/mean.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Mean</a>, <a data-href=\"Median\" href=\"notes/math/median.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Median</a>, and <a data-href=\"Mode\" href=\"notes/math/mode.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Mode</a>. Depending on the characteristics of the underlying dataset, one measure may be more appropriate than the others.<br> <img alt=\"Pasted image 20250708171735.png\" src=\"resources/pasted-image-20250708171735.png\" target=\"_self\"><br>\n<a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.studyforfe.com/blog/measures-of-central-tendencies-and-dispersions\" target=\"_self\">https://www.studyforfe.com/blog/measures-of-central-tendencies-and-dispersions</a><br>The <a href=\"notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"4\" to=\"8\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a>, often referred to as the average, is one of the most widely used measures of central tendency. There are various types of means, with the arithmetic mean being the most common. The arithmetic mean is calculated by summing all values in a dataset and dividing by the number of values.<br>The sample <a href=\"notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"11\" to=\"15\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a>, denoted as , is defined by the formula:Where is the number of values in the dataset.\n<br>The <a href=\"notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"4\" to=\"8\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a> is sensitive to <a data-tooltip-position=\"top\" aria-label=\"Outlier\" data-href=\"Outlier\" href=\"notes/math/outlier.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">outliers</a>, so it may not provide an accurate representation of the center when the underlying dataset is asymmetric, or has many extreme <a href=\"notes/math/outlier.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"128\" to=\"136\" origin-text=\"outliers\" class=\"internal-link virtual-link-a\">outliers</a>. For symmetric distributions, the mean is a useful measure that provides the average value of the dataset.\nThe mean is often used in algorithms such as linear regression, where it helps minimize error in predictions.\n<br>The <a href=\"notes/math/median.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"4\" to=\"10\" origin-text=\"median\" class=\"internal-link virtual-link-a\">median</a> is the middle value of a dataset when it is ordered. If there is an even number of values, then the <a href=\"notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"111\" to=\"115\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a> (average) of the two middlemost values are taken.<br>If is even, the formula for the <a href=\"notes/math/median.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"30\" to=\"36\" origin-text=\"median\" class=\"internal-link virtual-link-a\">median</a> is:<br>While if is odd, the formula for the <a href=\"notes/math/median.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"29\" to=\"35\" origin-text=\"median\" class=\"internal-link virtual-link-a\">median</a> is:\n<br>The <a href=\"notes/math/median.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"4\" to=\"10\" origin-text=\"median\" class=\"internal-link virtual-link-a\">median</a> is less susceptible to <a href=\"notes/math/outlier.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"34\" to=\"42\" origin-text=\"outliers\" class=\"internal-link virtual-link-a\">outliers</a> than the <a href=\"notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"52\" to=\"56\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a>, therefore, it provides a more accurate measure of the center for skewed distributions.\n<br>In data science and machine learning, the <a href=\"notes/math/mode.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"42\" to=\"46\" origin-text=\"mode\" class=\"internal-link virtual-link-a\">mode</a> is useful for categorical variables, such as determining the most common class label in classification tasks.\n<br>The <a href=\"notes/math/mode.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"4\" to=\"8\" origin-text=\"mode\" class=\"internal-link virtual-link-a\">mode</a> of a dataset is the value that appears most frequently. A dataset can have:\n<br>One <a href=\"notes/math/mode.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"4\" to=\"8\" origin-text=\"mode\" class=\"internal-link virtual-link-a\">mode</a> (unimodal)\nTwo modes (bimodal)\nMultiple modes (multimodal)\n<br>The <a href=\"notes/math/mode.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"4\" to=\"8\" origin-text=\"mode\" class=\"internal-link virtual-link-a\">mode</a> is useful for analyzing <a data-href=\"Nominal Data\" href=\"notes/math/nominal-data.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Nominal Data</a>, as it helps identify the most \"popular\" category within a given set of values.<br>The <a href=\"notes/math/mode.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"4\" to=\"8\" origin-text=\"mode\" class=\"internal-link virtual-link-a\">mode</a> can be defined as:Where is the frequency of the value in a given dataset.\n<br>Unlike the <a href=\"notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"11\" to=\"15\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a> or <a href=\"notes/math/median.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"19\" to=\"25\" origin-text=\"median\" class=\"internal-link virtual-link-a\">median</a>, the <a href=\"notes/math/mode.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"31\" to=\"35\" origin-text=\"mode\" class=\"internal-link virtual-link-a\">mode</a> can be directly applied to <a data-href=\"Categorical Data\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Categorical Data</a>, making it one of the most simple and versatile measures of central tendency. J. Han and M. Kamber,&nbsp;Data Mining : Concepts and Techniques, 3rd ed. Amsterdam ; Boston: Elsevier/Morgan Kaufmann, 2012.\n<br>Laerd Statistics, “Measures of central tendency,”&nbsp;Laerd Statistics, 2018. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://statistics.laerd.com/statistical-guides/measures-central-tendency-mean-mode-median.php\" target=\"_self\">https://statistics.laerd.com/statistical-guides/measures-central-tendency-mean-mode-median.php</a>\n<br>“Central tendency,”&nbsp;Wikipedia, Jul. 13, 2020. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Central_tendency\" target=\"_self\">https://en.wikipedia.org/wiki/Central_tendency</a>\n","aliases":["Measures of Central Tendency"],"inlineTags":[],"frontmatterTags":["#math","#statistics"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Measure of Central Tendency","level":1,"id":"Measure_of_Central_Tendency_0"},{"heading":"<span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Mean.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"0\" to=\"4\" origin-text=\"Mean\" class=\"internal-link virtual-link-a\">Mean</a></span>","level":2,"id":"Mean_0"},{"heading":"Properties","level":3,"id":"Properties_0"},{"heading":"<span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Median.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"0\" to=\"6\" origin-text=\"Median\" class=\"internal-link virtual-link-a\">Median</a></span>","level":2,"id":"Median_0"},{"heading":"Properties","level":3,"id":"Properties_1"},{"heading":"<span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Mode.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"0\" to=\"4\" origin-text=\"Mode\" class=\"internal-link virtual-link-a\">Mode</a></span>","level":2,"id":"Mode_0"},{"heading":"Properties","level":3,"id":"Properties_2"},{"heading":"Summary of When to Use Each Measure","level":2,"id":"Summary_of_When_to_Use_Each_Measure_0"},{"heading":"References","level":2,"id":"References_0"}],"links":["notes/math/mean.html#_0","notes/math/median.html#_0","notes/math/mode.html#_0","notes/math/mean.html#_0","notes/math/mean.html#_0","notes/math/mean.html#_0","notes/math/mean.html#_0","notes/math/outlier.html#_0","notes/math/outlier.html#_0","notes/math/median.html#_0","notes/math/median.html#_0","notes/math/mean.html#_0","notes/math/median.html#_0","notes/math/median.html#_0","notes/math/median.html#_0","notes/math/outlier.html#_0","notes/math/mean.html#_0","notes/math/mode.html#_0","notes/math/mode.html#_0","notes/math/mode.html#_0","notes/math/mode.html#_0","notes/math/mode.html#_0","notes/math/nominal-data.html#_0","notes/math/mode.html#_0","notes/math/mean.html#_0","notes/math/median.html#_0","notes/math/mode.html#_0",".html","notes/math/nominal-data.html#_0","notes/math/mode.html#_0","notes/math/ordinal-data.html#_0","notes/math/median.html#_0","notes/math/mean.html#_0"],"author":"","coverImageURL":"HTML/resources/pasted-image-20250708171735.png","fullURL":"notes/math/measure-of-central-tendency.html","pathToRoot":"../..","attachments":["resources/pasted-image-20250708171735.png"],"createdTime":1751847846892,"modifiedTime":1754247539565,"sourceSize":3814,"sourcePath":"NOTES/Math/Measure of Central Tendency.md","exportPath":"notes/math/measure-of-central-tendency.html","showInTree":true,"treeOrder":11,"backlinks":["notes/math/mean.html","notes/math/median.html","notes/math/mode.html","notes/math/probability.html","notes/math/quantile.html","notes/math/variance.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown"},"notes/math/mean.html":{"title":"Mean","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-04The mean is a <a data-href=\"Measure of Central Tendency\" href=\"notes/math/measure-of-central-tendency.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Measure of Central Tendency</a> which attempts to summarize an entire dataset with a single number. It provides an illustration of the average value within a collection of values, making it essential for data analysis tasks. The mean can only be applied to <a data-href=\"Numerical Data\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Numerical Data</a> and not <a data-href=\"Categorical Data\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Categorical Data</a>.<br>There are several types of means, each suited for different applications and fields. The most commonly used type is the arithmetic mean, which is calculated by summing all values and dividing by the number of observations. Other variations include the geometric mean, often used in financial contexts, the harmonic mean, which is beneficial in situations involving rates, and <a data-href=\"Root-Mean Square\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Root-Mean Square</a>, often used to measure the average voltage of an AC source.<br>Other common <a data-tooltip-position=\"top\" aria-label=\"Measure of Central Tendency\" data-href=\"Measure of Central Tendency\" href=\"notes/math/measure-of-central-tendency.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Measures of Central Tendency</a> include the <a data-href=\"Median\" href=\"notes/math/median.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Median</a>, which represents the middle value when the data is ordered, the <a data-href=\"Mode\" href=\"notes/math/mode.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Mode</a>, which identifies the most frequently occurring value in a set, and the <a data-href=\"Midrange\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Midrange</a>, calculated as the average of the maximum and minimum values.<br>The arithmetic mean, commonly referred to as the average, is the sum of all values in a dataset divided by the number of values. It is the most widely used <a data-href=\"Measure of Central Tendency\" href=\"notes/math/measure-of-central-tendency.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Measure of Central Tendency</a>.There are two types of arithmetic means:\nSample Mean (): This represents the average value of a subset drawn from a larger population.\nGroup Mean (): This denotes the average of values within a specific category or attribute of a dataset.\nThe formula for calculating the sample mean is:Where: is the sample mean. is the number of values in the dataset.\n<br> is the value of an individual object at <a href=\"index.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"41\" to=\"46\" origin-text=\"index\" class=\"internal-link virtual-link-a\">index</a> i.\n<br>The arithmetic mean is commonly used in statistics the summarize datasets and provide a simple <a data-href=\"Measure of Central Tendency\" href=\"notes/math/measure-of-central-tendency.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Measure of Central Tendency</a>. Though an arithmetic mean is susceptible to <a href=\"notes/math/outlier.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"46\" to=\"54\" origin-text=\"outliers\" class=\"internal-link virtual-link-a\">outliers</a>, making it a less relevant metric for skewed datasets.<br>The trimmed mean is an arithmetic mean which discards a specified number of values from both ends of the value <a data-href=\"range\" href=\"notes/math/range.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">range</a>. This is done to minimize the effect of <a data-tooltip-position=\"top\" aria-label=\"Outlier\" data-href=\"Outlier\" href=\"notes/math/outlier.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">outliers</a> on the mean, which can skew the results. The trimmed mean generally gives a more accurate representation of the center making it particularly useful in datasets prone to <a href=\"notes/math/outlier.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"171\" to=\"179\" origin-text=\"outliers\" class=\"internal-link virtual-link-a\">outliers</a> or extreme variations.<br>The <a data-href=\"Interquartile\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Interquartile</a> Mean is a specific type of trimmed mean that excludes the first and last <a data-href=\"Quartile\" href=\"notes/math/quartile.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Quartile</a> of ordered data. This results in the average of the middle 50% of values, offering a robust <a data-href=\"measure of central tendency\" href=\"notes/math/measure-of-central-tendency.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">measure of central tendency</a> that is less influenced by extreme values.The formula for the trimmed mean is:The formula for the Interquartile mean is:<br>The trimmed mean is useful for analyzing skewed datasets, or datasets that have large amount of <a href=\"notes/math/outlier.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"96\" to=\"104\" origin-text=\"outliers\" class=\"internal-link virtual-link-a\">outliers</a>. The trimmed mean offers a more stable measure that is less affected by outliers, making it valuable across various disciplines.In a weighted mean, instead of each value contributing equally like in an arithmetic mean, each value is assigned a weight (or coefficient) based on its significance. This allows for a more nuanced average that reflects the importance of different contributions. The arithmetic mean is a weighted mean where all weights are equal.The formula for calculating the weighted mean is:<br>The weighted mean is used when there is a need to model the relative importance of various attributes. This is commonly seen in artificial intelligence techniques such as <a data-href=\"Ensemble Machine Learning\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Ensemble Machine Learning</a> algorithms.The harmonic mean calculates the average of a set of numbers that are defined in relation to some unit. It is calculated by taking the reciprocal of the arithmetic mean of the reciprocals of each value in the dataset.The formula for the harmonic mean is:<br>In <a data-href=\"Machine Learning\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Machine Learning</a>, the harmonic mean is commonly used to calculate the <a data-href=\"F1 Score\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">F1 Score</a> of a model. It is also commonly used when analyzing speed and rates, such as finding the average speed of multiple segments of a journey.<br>The geometric mean calculates the average of a set of values by using the product of a values rather than their sum. It involves multiplying all the values in the dataset and then taking the th root of that product, where is the total number of values in the set. The geometric mean is less susceptible to <a data-tooltip-position=\"top\" aria-label=\"Outlier\" data-href=\"Outlier\" href=\"notes/math/outlier.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">outliers</a> than the arithmetic mean since each value is part of a product rather than a sum.It is called the geometric mean because it's commonly used to find the side length of a square that has the same area as a rectangle with given side lengths. For example, if you have a rectangle with dimensions , the length of a square with equal volume is the geometric mean of and , which is .The formula for the geometric mean is:<br>The geometric mean is commonly used to calculate average rates of return on investments over time. The geometric mean can also be employed in <a data-href=\"Data Normalization\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Data Normalization</a> to normalize features, particularly when the values span multiple orders of magnitude.The root mean square (RMS), often denoted as , is an average of the magnitude of a set of values, and is and is also known as the quadratic mean. It is particularly useful in sets with values of both positive and negative numbers. RMS is calculated by taking the square root of the arithmetic average of the squared values.For a continuous function defined over the interval , the RMS is determined by squaring the function, averaging the squared values over the interval, and then taking the square root of that average.The formula for discrete RMS is:The formula for continuous RMS is:Root-mean square is widely used in signal processing the measure the power of AC currents and voltages. In regression analysis, the RMS error is a common metric for evaluation the performance of a model. RMS is also used to evaluate the performance of control systems regarding the stability and responsiveness to input signals. RMS can also be used in optimization algorithms for neural networks, such as RMSprop.<br>The mean of a <a data-href=\"Probability Distribution\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Probability Distribution</a> represents the average outcome of some <a data-href=\"Random Variable\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Random Variable</a>. It is a specific type of weighted mean where each outcome of some random variable is weighted by the <a data-href=\"Probability\" href=\"notes/math/probability.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Probability</a> of that outcome occurring.<br>The <a data-href=\"Expected Value\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Expected Value</a> of a random variable, denoted as , is the weighted average of its possible outcomes. This concept is crucial in various fields, including artificial intelligence, especially in the context of stochastic task environments, where uncertainty plays a significant role.<br>For discrete <a href=\"notes/math/probability.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"13\" to=\"24\" origin-text=\"probability\" class=\"internal-link virtual-link-a\">probability</a> distributions, the expected value is defined as:<br>Where is the <a data-href=\"Probability Mass Function\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Probability Mass Function</a>.<br>For continuous <a href=\"notes/math/probability.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"15\" to=\"26\" origin-text=\"probability\" class=\"internal-link virtual-link-a\">probability</a> distributions, the expected value is defined as:<br>Where is the <a data-href=\"Probability Density Function\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Probability Density Function</a>.<br>The expected value is a fundamental metric in decision theory for AI systems, guiding the decision-making process by allowing agents to evaluate actions based on their expected rewards. Expected value is also important in game theory for evaluating strategies based on their expected payoffs. In <a data-href=\"Machine Learning\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Machine Learning</a>, the expected value is used for supervised techniques, particularly for loss functions.<br>The mean of a continuous function over a specific interval is defined as the integral of the function divided by the length of the interval. <a data-href=\"Root-Mean Square\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Root-Mean Square</a> is a type of mean of a function.The formula for the mean of a continuous function defined over the interval is:<br>The mean of a function has many applications, such as in statistics, where it provides the <a data-href=\"measure of central tendency\" href=\"notes/math/measure-of-central-tendency.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">measure of central tendency</a> for continuous random variables. It is also commonly applied in machine learning when performing feature engineering, such as when normalizing data.\n<br>S. Glen, “Mean, <a href=\"notes/math/median.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"16\" to=\"22\" origin-text=\"Median\" class=\"internal-link virtual-link-a\">Median</a>, <a href=\"notes/math/mode.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"24\" to=\"28\" origin-text=\"Mode\" class=\"internal-link virtual-link-a\">Mode</a>: What They Are, How to Find Them,”&nbsp;Statistics How To, 2022. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/\" target=\"_self\">https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/</a>\n<br>Wikipedia contributors, “Mean,” Wikipedia, Apr. 25, 2025. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Mean\" target=\"_self\">https://en.wikipedia.org/wiki/Mean</a>\n<br>J. Frost, “What is the Mean in Statistics?,”&nbsp;Statistics By Jim, Aug. 21, 2021. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://statisticsbyjim.com/basics/mean_average/\" target=\"_self\">https://statisticsbyjim.com/basics/mean_average/</a>\nJ. Han and M. Kamber,&nbsp;Data Mining : Concepts and Techniques, 3rd ed. Amsterdam ; Boston: Elsevier/Morgan Kaufmann, 2012.\n<br>“Expectation | Mean | Average,”&nbsp;<a data-tooltip-position=\"top\" aria-label=\"http://www.probabilitycourse.com\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://www.probabilitycourse.com\" target=\"_self\">www.probabilitycourse.com</a>. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.probabilitycourse.com/chapter3/3_2_2_expectation.php\" target=\"_self\">https://www.probabilitycourse.com/chapter3/3_2_2_expectation.php</a>\n‌","aliases":[],"inlineTags":[],"frontmatterTags":["#math","#statistics"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Mean","level":1,"id":"Mean_0"},{"heading":"Other <span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Measure of Central Tendency.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"6\" to=\"34\" origin-text=\"Measures of Central Tendency\" class=\"internal-link virtual-link-a\">Measures of Central Tendency</a></span>","level":4,"id":"Other_Measures_of_Central_Tendency_0"},{"heading":"Types of Means","level":2,"id":"Types_of_Means_0"},{"heading":"Arithmetic Mean","level":3,"id":"Arithmetic_Mean_0"},{"heading":"Formula","level":4,"id":"Formula_0"},{"heading":"Application","level":4,"id":"Application_0"},{"heading":"Trimmed Mean / Interquartile Mean","level":3,"id":"Trimmed_Mean_/_Interquartile_Mean_0"},{"heading":"Formula","level":4,"id":"Formula_1"},{"heading":"Application","level":4,"id":"Application_1"},{"heading":"Weighted Mean","level":3,"id":"Weighted_Mean_0"},{"heading":"Formula","level":4,"id":"Formula_2"},{"heading":"Application","level":4,"id":"Application_2"},{"heading":"<a data-href=\"Harmonic Mean\" href=\"Harmonic Mean\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Harmonic Mean</a>","level":3,"id":"[[Harmonic_Mean]]_0"},{"heading":"Formula","level":4,"id":"Formula_3"},{"heading":"Application","level":4,"id":"Application_3"},{"heading":"<a data-href=\"Geometric Mean\" href=\"Geometric Mean\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Geometric Mean</a>","level":3,"id":"[[Geometric_Mean]]_0"},{"heading":"Formula","level":4,"id":"Formula_4"},{"heading":"Application","level":4,"id":"Application_4"},{"heading":"<a data-href=\"Root-Mean Square\" href=\"Root-Mean Square\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Root-Mean Square</a>","level":3,"id":"[[Root-Mean_Square]]_0"},{"heading":"Formula","level":4,"id":"Formula_5"},{"heading":"Application","level":4,"id":"Application_5"},{"heading":"<a data-href=\"Expected Value\" href=\"Expected Value\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Expected Value</a> / Mean of a <span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Probability.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"13\" to=\"24\" origin-text=\"Probability\" class=\"internal-link virtual-link-a\">Probability</a></span> Distribution","level":3,"id":"[[Expected_Value]]_/_Mean_of_a_Probability_Distribution_0"},{"heading":"Formula","level":4,"id":"Formula_6"},{"heading":"Application","level":4,"id":"Application_6"},{"heading":"Mean of a Function","level":3,"id":"Mean_of_a_Function_0"},{"heading":"Formula","level":4,"id":"Formula_7"},{"heading":"Application","level":4,"id":"Application_7"},{"heading":"References","level":2,"id":"References_0"}],"links":["notes/math/measure-of-central-tendency.html#_0",".html",".html",".html","notes/math/measure-of-central-tendency.html#_0","notes/math/measure-of-central-tendency.html#_0","notes/math/median.html#_0","notes/math/mode.html#_0",".html","notes/math/measure-of-central-tendency.html#_0","index.html#_0","notes/math/measure-of-central-tendency.html#_0","notes/math/outlier.html#_0","notes/math/range.html#_0","notes/math/outlier.html#_0","notes/math/outlier.html#_0",".html","notes/math/quartile.html#_0","notes/math/measure-of-central-tendency.html#_0","notes/math/outlier.html#_0",".html",".html",".html",".html",".html","notes/math/outlier.html#_0",".html",".html",".html","notes/math/probability.html#_0",".html",".html","notes/math/probability.html#_0",".html","notes/math/probability.html#_0",".html","notes/math/probability.html#_0",".html",".html",".html","notes/math/measure-of-central-tendency.html#_0","notes/math/median.html#_0","notes/math/mode.html#_0"],"author":"","coverImageURL":"","fullURL":"notes/math/mean.html","pathToRoot":"../..","attachments":[],"createdTime":1751668570756,"modifiedTime":1754247529503,"sourceSize":10024,"sourcePath":"NOTES/Math/Mean.md","exportPath":"notes/math/mean.html","showInTree":true,"treeOrder":10,"backlinks":["notes/math/binary-data.html","notes/math/measure-of-central-tendency.html","notes/math/median.html","notes/math/mode.html","notes/math/nominal-data.html","notes/math/outlier.html","notes/math/probability.html","notes/math/quantile.html","notes/math/range.html","notes/math/standard-deviation.html","notes/math/variance.html","textbooks/data-mining-concepts-and-techniques/practice-problems.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown"},"notes/math/manhattan-distance.html":{"title":"Manhattan Distance","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-31The Manhattan distance is a <a data-href=\"Distance Measure\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Distance Measure</a> defined as the sum of the absolute differences of the Cartesian coordinates of two points. It is the distance between two points using only grid-like movements (horizontal and vertical). The Manhattan distance is also the norm of the distance between two vectors in <a data-href=\"Lp Space\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Lp Space</a>.The Manhattan distance between two objects in -dimensional space is calculated as:\n<br>Positive: Like any other distance metric, the <a data-tooltip-position=\"top\" aria-label=\"Range\" data-href=\"Range\" href=\"notes/math/range.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">range</a> of the Manhattan distance is where a 0 distance indicates that the two points are at the same location.\nSymmetric: The Manhattan distance is symmetric, meaning <br><a data-href=\"Triangle Inequality\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Triangle Inequality</a>: The Manhattan distance obeys the triangle inequality, which states that the distance from to is always less than or equal to the distance from to plus the distance from to . Meaning that taking a detour through a third point cannot result in a shorter distance than a direct path from to . <br><a data-href=\"Clustering\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Clustering</a>: Manhattan distance may be used in clustering algorithms like <a data-href=\"K-Means Clustering\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">K-Means Clustering</a> to group similar data points.\n<br><a data-href=\"Classification\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Classification</a>: The Manhattan distance may be employed in distance-based classification algorithms such as <a data-href=\"K-Nearest-Neighbors\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">K-Nearest-Neighbors</a>. Measuring Pixel Differences: The Manhattan distance, or other distance measures, may be employed to compare pixel values, or features in image recognition tasks. <br>The Manhattan distance can be used for calculating the shortest path between points. It is also an <a data-href=\"Admissible\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Admissible</a> and <a data-href=\"Consistent\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Consistent</a> <a data-href=\"Heuristic\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Heuristic</a> in search algorithms such as <a data-href=\"A* Search\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">A* Search</a>.\nGrid-Navigation: The Manhattan distance is particularly useful for pathfinding in domains which only allow grid-like movements, such as a chessboard or a city network. <br><a data-href=\"Outlier\" href=\"notes/math/outlier.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Outlier</a> Detection: Manhattan distance can be used for distance based <a href=\"notes/math/outlier.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"52\" to=\"59\" origin-text=\"outlier\" class=\"internal-link virtual-link-a\">outlier</a> detection algorithms such as <a data-href=\"DBSCAN\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">DBSCAN</a>.\n<br><a data-href=\"Multidimensional Scaling\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Multidimensional Scaling</a>: Manhattan distance may be employed to visualize higher-dimensional data into lower dimensions.\nOther common distance measures include:\n<br><a data-href=\"Euclidean Distance\" href=\"notes/math/euclidean-distance.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Euclidean Distance</a>: The <a href=\"notes/math/euclidean-distance.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"6\" to=\"28\" origin-text=\"straight-line distance\" class=\"internal-link virtual-link-a\">straight-line distance</a> between 2 points in Euclidean space.\n<br><a data-href=\"Chebyshev Distance\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Chebyshev Distance</a>: The maximum absolute difference between 2 vectors across all dimensions.\n<br><a data-href=\"Minkowski Distance\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Minkowski Distance</a>: A generalized distance measure that is defined by a parameter whose common values are the Manhattan distance, Euclidean distance, and Chebyshev distance.\n<br><a data-href=\"Jaccard Index\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Jaccard Index</a>: Used to compare sets and is defined as the size of the intersection of 2 sets, over the size of their union. J. Han and M. Kamber,&nbsp;Data Mining : Concepts and Techniques, 3rd ed. Haryana, India ; Burlington, Ma: Elsevier, 2018.\n<br>“Taxicab geometry,”&nbsp;Wikipedia, Jan. 21, 2022. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Taxicab_geometry\" target=\"_self\">https://en.wikipedia.org/wiki/Taxicab_geometry</a>\n<br>GeeksforGeeks, “Clustering Distance Measures,”&nbsp;GeeksforGeeks, May 24, 2024. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/machine-learning/clustering-distance-measures/#common-distance-measures\" target=\"_self\">https://www.geeksforgeeks.org/machine-learning/clustering-distance-measures/#common-distance-measures</a> (accessed Jul. 31, 2025).\n","aliases":["City Block Distance"],"inlineTags":[],"frontmatterTags":["#math","#statistics"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Manhattan Distance","level":1,"id":"Manhattan_Distance_0"},{"heading":"Properties","level":3,"id":"Properties_0"},{"heading":"Applications","level":2,"id":"Applications_0"},{"heading":"<a data-href=\"Machine Learning\" href=\"Machine Learning\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Machine Learning</a>","level":3,"id":"[[Machine_Learning]]_0"},{"heading":"<a data-href=\"Computer Vision\" href=\"Computer Vision\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Computer Vision</a>","level":3,"id":"[[Computer_Vision]]_0"},{"heading":"Pathfinding","level":3,"id":"Pathfinding_0"},{"heading":"Statistics","level":3,"id":"Statistics_0"},{"heading":"Other Distance Measures","level":2,"id":"Other_Distance_Measures_0"},{"heading":"References","level":2,"id":"References_0"}],"links":[".html",".html","notes/math/range.html#_0",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html","notes/math/outlier.html#_0","notes/math/outlier.html#_0",".html",".html","notes/math/euclidean-distance.html#_0","notes/math/euclidean-distance.html#_0",".html",".html",".html"],"author":"","coverImageURL":"","fullURL":"notes/math/manhattan-distance.html","pathToRoot":"../..","attachments":[],"createdTime":1753987116983,"modifiedTime":1754247526013,"sourceSize":3486,"sourcePath":"NOTES/Math/Manhattan Distance.md","exportPath":"notes/math/manhattan-distance.html","showInTree":true,"treeOrder":9,"backlinks":["notes/math/euclidean-distance.html","notes/math/outlier.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown"},"notes/math/least-squares.html":{"title":"Least Squares","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-21The least squares method is an optimization technique that attempts to find a linear function which minimizes the sum of squared distances between observed and predicted values. This method is widely used in <a data-href=\"Regression\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Regression</a> analysis for finding the best-fit function between a matrix of features and one or more independent variables.<br><img alt=\"linearRegression.png\" src=\"resources/linearregression.png\" target=\"_self\"><br>In a <a data-href=\"Simple Linear Regression\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Simple Linear Regression</a>, the model is defined as:Where: is the dependent variable. is the slope of the line. is the independent variable. is the y-intercept.\n<br>The objective is to minimize the sum of squared <a data-tooltip-position=\"top\" aria-label=\"Euclidean Distance\" data-href=\"Euclidean Distance\" href=\"notes/math/euclidean-distance.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Euclidean distances</a>:Where: is the number of data points. are the observed data points.\nThis is equivalent to min\nCalculate Slope: Use the following formula to calculate the slope, : Calculate Y-Intercept: Use the slope calculated in the previous step and the following formula to calculate the y-intercept, : Formulate the Function: Construct the line of best fit in the form of <br>Sensitive to <a data-tooltip-position=\"top\" aria-label=\"Outlier\" data-href=\"Outlier\" href=\"notes/math/outlier.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Outliers</a>:The least squares method is sensitive to <a href=\"notes/math/outlier.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"42\" to=\"50\" origin-text=\"outliers\" class=\"internal-link virtual-link-a\">outliers</a>, which could unfairly skew the resulting regression line, resulting in inaccurate predictions.\n<br>Assumes Linearity: The least squares method assumes a linear relationship between the dependent and independent variables. If the underlying relationship is non-linear, this can introduce <a data-href=\"Bias\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Bias</a> in the predictions.\n<br>Assumes <a data-href=\"Homoscedasticity\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Homoscedasticity</a>: The least squares method assumes that the <a data-tooltip-position=\"top\" aria-label=\"Variance\" data-href=\"Variance\" href=\"notes/math/variance.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">variance</a> of errors is a constant (homoscedastic), rather than a function of the independent variable (<a data-href=\"Heteroscedastic\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Heteroscedastic</a>).\n<br><a data-href=\"Multicollinearity\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Multicollinearity</a>: If the independent variables are highly correlated, the least squares method has difficulty determining the effect of each attribute on the dependent variable, making the model unstable.\nThe most common form of the least squares method, commonly used in Linear Regression models. It simply minimizes the sum of squared residuals without any weights.<br>Extends ordinary least squares to handle heteroscedasticity by assigning weights to data objects based on <a data-href=\"Covariance Matrix\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Covariance Matrix</a> of the residuals.<br>A specific type of generalized least squares, used when the dataset exhibits heteroscedasticity. WLS is more robust because it weighs the influence of each data point based on its <a href=\"notes/math/variance.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"180\" to=\"188\" origin-text=\"variance\" class=\"internal-link virtual-link-a\">variance</a>. <br>Also known as L2 regularization, ridge regression is a type of linear regression that introduces a penalty term to the OLS cost function. Ridge regression adds the squared coefficients as a penalty to the cost function, thereby, punishing large coefficient values. This helps prevent overfitting and addresses any <a data-href=\"Multicollinearity\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Multicollinearity</a> in the independent variables.\nThe cost function for ridge regression defined as:Where: is the actual output. is the predicted output. are the coefficients is the regularization parameter which determines the magnitude of the penalty term.\n<br>By adjusting the ridge parameter, λ, ridge regression allows for control over the bias-<a href=\"notes/math/variance.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"87\" to=\"95\" origin-text=\"variance\" class=\"internal-link virtual-link-a\">variance</a> tradeoff. As increases, the bias of the model increases and the variance decreases. Optimizing this parameter can achieve the ideal balance between overfitting and underfitting model.Also known as L1 regularization, lasso regression is a type of linear regression which, like ridge regression, introduces a penalty term to the OLS cost function. Lasso regression adds the absolute coefficient values to the cost function, this allows the coefficients of irrelevant variables to be shrunk down to 0, thereby, simplifying and regularizing the model.\nThe cost function for lasso regression defined as:Where: is the actual output. is the predicted output. are the coefficients is the regularization parameter which determines the magnitude of the penalty term.\n<br>Unlike ridge regression, which shrinks coefficients but typically keeps all predictors in the model, lasso regression can set the coefficients of irrelevant variables exactly to zero. This makes it an effective technique for <a data-href=\"Feature Selection\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Feature Selection</a>, that is, identifying and retaining only the most important variables.\n<br>Prabhu Raghav, “Linear Regression Simplified - Ordinary Least Square vs Gradient Descent,”&nbsp;Medium, May 15, 2018. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://medium.com/data-science/linear-regression-simplified-ordinary-least-square-vs-gradient-descent-48145de2cf76\" target=\"_self\">https://medium.com/data-science/linear-regression-simplified-ordinary-least-square-vs-gradient-descent-48145de2cf76</a> (accessed Jul. 21, 2025).\n<br>GeeksforGeeks, “Least Square Method | Definition Graph and Formula,”&nbsp;GeeksforGeeks, Jul. 06, 2023. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/maths/least-square-method/\" target=\"_self\">https://www.geeksforgeeks.org/maths/least-square-method/</a>\n<br>“Least squares,”&nbsp;Wikipedia, Dec. 19, 2019. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Least_squares\" target=\"_self\">https://en.wikipedia.org/wiki/Least_squares</a>\n<br>A. Menon, “Linear Regression Using Least Squares - TDS Archive - Medium,”&nbsp;Medium, Sep. 08, 2018. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://medium.com/data-science/linear-regression-using-least-squares-a4c3456e8570\" target=\"_self\">https://medium.com/data-science/linear-regression-using-least-squares-a4c3456e8570</a> (accessed Jul. 21, 2025).\n<br>The Organic Chemistry Tutor, “Linear Regression Using Least Squares Method - Line of Best Fit Equation,”&nbsp;YouTube. Jul. 13, 2020. [YouTube Video]. Available: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.youtube.com/watch?v=P8hT5nDai6A\" target=\"_self\">https://www.youtube.com/watch?v=P8hT5nDai6A</a>\n<br>GeeksforGeeks, “Ridge Regression,”&nbsp;GeeksforGeeks, Jun. 11, 2024. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/machine-learning/what-is-ridge-regression/\" target=\"_self\">https://www.geeksforgeeks.org/machine-learning/what-is-ridge-regression/</a> (accessed Jul. 23, 2025).\n<br>GeeksforGeeks, “What is Lasso Regression?,”&nbsp;GeeksforGeeks, May 15, 2024. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/machine-learning/what-is-lasso-regression/\" target=\"_self\">https://www.geeksforgeeks.org/machine-learning/what-is-lasso-regression/</a>\n","aliases":[],"inlineTags":[],"frontmatterTags":["#math","#statistics","#machineLearning","#regression"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Least Squares","level":1,"id":"Least_Squares_0"},{"heading":"Method","level":2,"id":"Method_0"},{"heading":"Objective","level":3,"id":"Objective_0"},{"heading":"Steps","level":3,"id":"Steps_0"},{"heading":"Limitations","level":2,"id":"Limitations_0"},{"heading":"Types of Least Squares","level":2,"id":"Types_of_Least_Squares_0"},{"heading":"<a data-href=\"Ordinary Least Squares\" href=\"Ordinary Least Squares\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Ordinary Least Squares</a>","level":3,"id":"[[Ordinary_Least_Squares]]_0"},{"heading":"<a data-href=\"Generalized Least Squares\" href=\"Generalized Least Squares\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Generalized Least Squares</a>","level":3,"id":"[[Generalized_Least_Squares]]_0"},{"heading":"<a data-href=\"Weighted Least Squares\" href=\"Weighted Least Squares\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Weighted Least Squares</a>","level":3,"id":"[[Weighted_Least_Squares]]_0"},{"heading":"Additional Techniques","level":2,"id":"Additional_Techniques_0"},{"heading":"<a data-href=\"Ridge Regression\" href=\"Ridge Regression\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Ridge Regression</a>","level":3,"id":"[[Ridge_Regression]]_0"},{"heading":"<a data-href=\"Lasso Regression\" href=\"Lasso Regression\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Lasso Regression</a>","level":3,"id":"[[Lasso_Regression]]_0"},{"heading":"References","level":2,"id":"References_0"}],"links":[".html",".html","notes/math/euclidean-distance.html#_0","notes/math/outlier.html#_0","notes/math/outlier.html#_0",".html",".html","notes/math/variance.html#_0",".html",".html",".html",".html",".html",".html","notes/math/variance.html#_0",".html",".html","notes/math/variance.html#_0",".html",".html"],"author":"","coverImageURL":"resources/linearregression.png","fullURL":"notes/math/least-squares.html","pathToRoot":"../..","attachments":["resources/linearregression.png"],"createdTime":1753105505921,"modifiedTime":1754247532104,"sourceSize":6451,"sourcePath":"NOTES/Math/Least Squares.md","exportPath":"notes/math/least-squares.html","showInTree":true,"treeOrder":8,"backlinks":["notes/math/euclidean-distance.html","notes/math/variance.html"],"type":"markdown"},"notes/math/euclidean-distance.html":{"title":"Euclidean Distance","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-17The Euclidean distance is the most popular <a data-href=\"Distance Measure\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Distance Measure</a> that quantifies the dissimilarity between <a data-href=\"Numerical Data\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Numerical Data</a>. It represents the straight-line distance between two points in Euclidean space and is calculated using the <a data-href=\"Pythagorean Theorem\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Pythagorean Theorem</a>. Let and be two data objects described by numerical attributes. The Euclidean distance between these two objects is: This formula computes the square root of the sum of the squared differences of each corresponding attribute, providing a measure of the straight-line distance in the multidimensional space.\n<br>Positive: Like any other distance metric, the <a data-tooltip-position=\"top\" aria-label=\"Range\" data-href=\"Range\" href=\"notes/math/range.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">range</a> of the Euclidean distance is where a 0 distance indicates that the two points are at the same location.\nSymmetric: The Euclidean distance is symmetric, meaning <br><a data-href=\"Triangle Inequality\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Triangle Inequality</a>: The Euclidean distance obeys the triangle inequality, which states that the distance from to is always less than or equal to the distance from to plus the distance from to . Meaning that taking a detour through a third point cannot result in a shorter distance than a direct path from to .\nThe squared Euclidean distance is computed as only the sum of squared differences:<br>The squared Euclidean distance amplifies greater distances more so then the standard Euclidean distance, and is faster and easier to compute. This makes it more desirable in problems where significant distances should be penalized harsher, such as in <a data-href=\"Clustering\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Clustering</a> or <a data-href=\"Outlier Detection\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Outlier Detection</a>. Though, the squared Euclidian distance does not obey the triangle inequality.<br>Squared Euclidean distance is a <a data-href=\"Convex Function\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Convex Function</a>, which makes it more desirable in optimization theory since it permits the use of <a data-href=\"Convex Analysis\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Convex Analysis</a>.<br><a href=\"notes/math/least-squares.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"0\" to=\"13\" origin-text=\"Least squares\" class=\"internal-link virtual-link-a\">Least squares</a> is an optimization technique that attempts to find the function which minimizes the sum of the square Euclidean distances between the observed and predicted values. This method is widely used in <a data-href=\"Machine Learning\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Machine Learning</a>, particularly in <a data-href=\"Regression\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Regression</a> analysis.<br>Divergence is a kind of distance measure that applies to <a href=\"notes/math/probability.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"57\" to=\"68\" origin-text=\"probability\" class=\"internal-link virtual-link-a\">probability</a> distributions. The squared Euclidean distance is the simplest divergence measure.<br>Other common divergence measures include <a data-href=\"Kullback–Leibler Divergence\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Kullback–Leibler Divergence</a> and <a data-href=\"Jensen-Shannon Divergence\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Jensen-Shannon Divergence</a>.\n<br>Clustering: Euclidean distance is used in clustering algorithms such as <a data-href=\"K-Means Clustering\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">K-Means Clustering</a> to measure distance between data points.\n<br><a data-href=\"Classification\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Classification</a>: Classification algorithms like <a data-href=\"K-Nearest-Neighbors\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">K-Nearest-Neighbors</a> may utilize Euclidean distance to classify data points based on the label of their nearest neighbors. <br><a data-href=\"Outlier\" href=\"notes/math/outlier.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Outlier</a> Detection: The Euclidean or squared Euclidean distance can be used to identify data points which deviate significantly from the rest of the dataset.\n<br>Multivariate Analysis: Euclidean distance can be used in techniques like <a data-href=\"Principal Component Analysis\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Principal Component Analysis</a> or <a data-href=\"Multidimensional Scaling\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Multidimensional Scaling</a> to measure the distance between points with multiple dimensions.\n<br>Least Squares Method: The Euclidean distance is a key step for the method of <a href=\"notes/math/least-squares.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"57\" to=\"70\" origin-text=\"least squares\" class=\"internal-link virtual-link-a\">least squares</a>, which is commonly used to optimize regression problems.\n<br>Divergence: Squared Euclidean distance is a simple measure of divergence, allowing you to compare <a href=\"notes/math/probability.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"88\" to=\"99\" origin-text=\"probability\" class=\"internal-link virtual-link-a\">probability</a> distributions.\nOptimization: Squared Euclidean distance is preferred in optimization theory due to its smoothness and convexity, permitting the use of convex analysis. <br>Path Planning: Euclidean distance can be used for calculating the shortest path between points. It is also an <a data-href=\"Admissible\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Admissible</a> and <a data-href=\"Consistent\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Consistent</a> <a data-href=\"Heuristic\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Heuristic</a> in search algorithms such as <a data-href=\"A* Search\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">A* Search</a>.\n<br>Localization: in <a data-href=\"Simultaneous Localization and Mapping\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Simultaneous Localization and Mapping</a> problems, the Euclidean distance can be used to determine a robot's position relative to known landmarks. Image Recognition: Euclidean distance is used to compare feature vectors in images, aiding in object recognition.\nImage Segmentation: Euclidean distance is used to measure similarities between pixel values in clustering-based image segmentation.\nOther common distance measures include:\n<br><a data-href=\"Chebyshev Distance\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Chebyshev Distance</a>: The maximum absolute difference between 2 vectors across all dimensions.\n<br><a data-href=\"Minkowski Distance\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Minkowski Distance</a>: A generalized distance measure that is defined by a parameter whose common values are the <a href=\"notes/math/manhattan-distance.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"29\" to=\"47\" origin-text=\"Manhattan distance\" class=\"internal-link virtual-link-a\">Manhattan distance</a>, Euclidean distance, and Chebyshev distance.\n<br><a data-href=\"Manhattan Distance\" href=\"notes/math/manhattan-distance.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Manhattan Distance</a>: The shortest distance between 2 vectors using only 90° movements.\n<br><a data-href=\"Jaccard Index\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Jaccard Index</a>: Used to compare sets and is defined as the size of the intersection of 2 sets, over the size of their union. J. Han and M. Kamber,&nbsp;Data Mining : Concepts and Techniques, 3rd ed. Amsterdam ; Boston: Elsevier/Morgan Kaufmann, 2012.\n<br>GeeksforGeeks, “Euclidean Distance,”&nbsp;GeeksforGeeks, Mar. 13, 2024. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/maths/euclidean-distance/\" target=\"_self\">https://www.geeksforgeeks.org/maths/euclidean-distance/</a>\n<br>Wikipedia Contributors, “Euclidean distance,”&nbsp;Wikipedia, Apr. 01, 2019. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Euclidean_distance\" target=\"_self\">https://en.wikipedia.org/wiki/Euclidean_distance</a>\n<br>“<a href=\"notes/math/least-squares.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"1\" to=\"14\" origin-text=\"Least squares\" class=\"internal-link virtual-link-a\">Least squares</a>,”&nbsp;Wikipedia, Dec. 19, 2019. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Least_squares\" target=\"_self\">https://en.wikipedia.org/wiki/Least_squares</a>\n<br>Maarten Grootendorst, “9 Distance Measures in Data Science | TDS Archive,”&nbsp;Medium, Feb. 2021. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://medium.com/data-science/9-distance-measures-in-data-science-918109d069fa\" target=\"_self\">https://medium.com/data-science/9-distance-measures-in-data-science-918109d069fa</a>\n","aliases":["Straight-Line Distance","Euclidean Distances"],"inlineTags":[],"frontmatterTags":["#math","#statistics","#machineLearning","#unfinished"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Euclidean Distance","level":1,"id":"Euclidean_Distance_0"},{"heading":"Formula","level":3,"id":"Formula_0"},{"heading":"Properties","level":3,"id":"Properties_0"},{"heading":"Squared Euclidean Distance","level":2,"id":"Squared_Euclidean_Distance_0"},{"heading":"<a data-href=\"Least Squares\" href=\"Least Squares\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Least Squares</a>","level":3,"id":"[[Least_Squares]]_0"},{"heading":"<a data-href=\"Divergence\" href=\"Divergence\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Divergence</a>","level":3,"id":"[[Divergence]]_0"},{"heading":"Applications","level":2,"id":"Applications_0"},{"heading":"Machine Learning","level":3,"id":"Machine_Learning_0"},{"heading":"Statistics","level":3,"id":"Statistics_0"},{"heading":"Robotics","level":3,"id":"Robotics_0"},{"heading":"<a data-href=\"Computer Vision\" href=\"Computer Vision\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Computer Vision</a>","level":3,"id":"[[Computer_Vision]]_0"},{"heading":"Other Distance Measures","level":2,"id":"Other_Distance_Measures_0"},{"heading":"References","level":2,"id":"References_0"}],"links":[".html",".html",".html","notes/math/range.html#_0",".html",".html",".html",".html",".html","notes/math/least-squares.html#_0","notes/math/least-squares.html#_0",".html",".html",".html","notes/math/probability.html#_0",".html",".html",".html",".html",".html","notes/math/outlier.html#_0",".html",".html","notes/math/least-squares.html#_0","notes/math/probability.html#_0",".html",".html",".html",".html",".html",".html",".html",".html","notes/math/manhattan-distance.html#_0","notes/math/manhattan-distance.html#_0",".html","notes/math/least-squares.html#_0"],"author":"","coverImageURL":"","fullURL":"notes/math/euclidean-distance.html","pathToRoot":"../..","attachments":[],"createdTime":1752772872723,"modifiedTime":1754247522422,"sourceSize":6032,"sourcePath":"NOTES/Math/Euclidean Distance.md","exportPath":"notes/math/euclidean-distance.html","showInTree":true,"treeOrder":7,"backlinks":["notes/math/least-squares.html","notes/math/manhattan-distance.html","notes/math/median.html","notes/math/outlier.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown"},"notes/math/commutative-property.html":{"title":"Commutative Property","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-20The commutative property states that the sums and products of values is unaffected by the order those values come in. It allows you to transform mathematical expressions into equivalent forms without altering its value.The commutative property applies to addition. For any numbers , and :The commutative property applies to multiplication. For any numbers , and :Operations such as subtraction or division are not commutative:\nSubtraction: , subtraction is actually <a data-href=\"Anti-Commutative\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Anti-Commutative</a>, meaning, Division: Exponentiation: Matrix Multiplication: For matrices , , and with compatible dimensions: The anti-commutative property refers to specific operations where switching the order of arguments negates the result of the expression.Subtraction is an anti-commutative operation. Meaning, for any 2 values and :<br>Some <a data-href=\"Logical Connectives\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Logical Connectives</a> are commutative, allowing for the rearrangement of variables without changing the truth value of the logical expression.The commutative property applies to conjunction (AND), for any variables and :The commutative property applies to disjunction (OR), for any variables and :<br>In set theory, the commutative property refers to how the <a data-href=\"Union\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Union</a> and <a data-href=\"Intersection\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Intersection</a> of sets are unaffected by the order of the sets they are applied to.for any sets , and :for any sets , and :<br>The <a href=\"notes/math/associative-property.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"4\" to=\"24\" origin-text=\"associative property\" class=\"internal-link virtual-link-a\">associative property</a> states that the sum or product of any group of values is not affected by how the values are grouped.\nAddition: Multiplication: <br>“Commutative property,”&nbsp;Wikipedia, Dec. 04, 2020. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Commutative_property\" target=\"_self\">https://en.wikipedia.org/wiki/Commutative_property</a>\n<br>GeeksforGeeks, “Commutative Property Definition | Commutative Law and Examples,”&nbsp;GeeksforGeeks, Dec. 28, 2023. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/maths/commutative-property/\" target=\"_self\">https://www.geeksforgeeks.org/maths/commutative-property/</a> (accessed Jul. 20, 2025).\n","aliases":["Commutative"],"inlineTags":[],"frontmatterTags":["#math","#algebra"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Commutative Property","level":1,"id":"Commutative_Property_0"},{"heading":"Commutative Operations","level":3,"id":"Commutative_Operations_0"},{"heading":"Addition","level":4,"id":"Addition_0"},{"heading":"Multiplication","level":4,"id":"Multiplication_0"},{"heading":"Non-Commutative Operations","level":3,"id":"Non-Commutative_Operations_0"},{"heading":"<a data-href=\"Anti-Commutative Property\" href=\"Anti-Commutative Property\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Anti-Commutative Property</a>","level":2,"id":"[[Anti-Commutative_Property]]_0"},{"heading":"Subtraction","level":3,"id":"Subtraction_0"},{"heading":"Logical Operations","level":2,"id":"Logical_Operations_0"},{"heading":"<a data-href=\"Conjunction\" href=\"Conjunction\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Conjunction</a>","level":3,"id":"[[Conjunction]]_0"},{"heading":"<a data-href=\"Disjunction\" href=\"Disjunction\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Disjunction</a>","level":3,"id":"[[Disjunction]]_0"},{"heading":"<a data-href=\"Set Theory\" href=\"Set Theory\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Set Theory</a>","level":2,"id":"[[Set_Theory]]_0"},{"heading":"Union","level":3,"id":"Union_0"},{"heading":"Intersection","level":3,"id":"Intersection_0"},{"heading":"<a data-href=\"Associative Property\" href=\"Associative Property\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Associative Property</a>","level":2,"id":"[[Associative_Property]]_0"},{"heading":"References","level":2,"id":"References_0"}],"links":[".html",".html",".html",".html",".html",".html",".html",".html","notes/math/associative-property.html#_0","notes/math/associative-property.html#_0"],"author":"","coverImageURL":"","fullURL":"notes/math/commutative-property.html","pathToRoot":"../..","attachments":[],"createdTime":1753041273100,"modifiedTime":1754247519350,"sourceSize":2732,"sourcePath":"NOTES/Math/Commutative Property.md","exportPath":"notes/math/commutative-property.html","showInTree":true,"treeOrder":6,"backlinks":["notes/math/associative-property.html","notes/math/propositional-logic.html"],"type":"markdown"},"notes/math/binary-data.html":{"title":"Binary Data","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-08Binary data is a type of <a data-href=\"Categorical Data\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Categorical Data</a> with only two possible values, typically 1 and 0. Binary data is a specific type of <a data-href=\"Nominal Data\" href=\"notes/math/nominal-data.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Nominal Data</a>, meaning the values are non-numeric, and qualitative.\nPresence: Binary data is commonly used to represent the presence or absence of a specific attribute, 1 meaning the attribute is present, 0 meaning the attribute is absent.\nTruth: Binary data is also commonly used to represent truth, where 1 indicates truth, and 0 indicated falsehood.\nOpposing Categories: Binary data is commonly used to represent 2 opposing categories, such as male/female, or employed/unemployed.\n<br>Since <a href=\"notes/math/nominal-data.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"6\" to=\"18\" origin-text=\"nominal data\" class=\"internal-link virtual-link-a\">nominal data</a> lacks numerical significance, data operations such as <a data-href=\"Mean\" href=\"notes/math/mean.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Mean</a> or <a data-href=\"Median\" href=\"notes/math/median.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Median</a> cannot be performed. However, the frequency of nominal data values can be analyzed, and a measure like the <a data-href=\"Mode\" href=\"notes/math/mode.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Mode</a> can describe the most common category within a given dataset.A binary attribute is symmetric if each state 0 or 1 is equally valuable, such as a gender attribute. A binary attribute is asymmetric if the two states are not equally valuable, such as the results of a disease test.<br>Asymmetric binary attributes require careful consideration in <a data-href=\"Machine Learning\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Machine Learning</a> tasks to ensure the model understands the underlying implications of the data. Common strategies include:\n<br><a data-href=\"Label Encoding\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Label Encoding</a>: Label encoding assigns numerical values to binary attributes. Each binary class can be assigned a value representative of its importance, for example, assigning the true class a value of , while the false class is assigned a value of .\nClass Weighting: In binary classification tasks, assigning different weights to each state during model training can put more emphasis on a particular class.\nCalculating the similarity and dissimilarity of binary attributes involves different methods depending on whether the attribute is symmetric or not.For symmetric binary attributes, common measures include:\n<br>\n<a data-href=\"Jaccard Coefficient\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Jaccard Coefficient</a>: The size of the intersection of two sets over their union: Where: is the number of attributes where and are 1. is the number of attributes where A is 1 and B is 0 is the number of attributes where A is 0 and B is 1 <br>\n<a data-href=\"Dice Coefficient\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Dice Coefficient</a>: Twice the size of the intersection of two sets over the sum of the sets: For asymmetric binary attributes, common measures include:\nWeighted Jaccard Coefficient: The Jaccard coefficient can be modified to emphasize a particular class: Where:\n​ is 1 if the element is present in set and 0 if absent.\n​ is 1 if the element is present in set and 0 if absent.\n​ is the weight assigned to the element . <br><a data-href=\"Binary Regression\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Binary Regression</a> estimates a function that maps one or more independent variables to a single dependent binary variable. Common techniques include:\n<br>\n<a data-href=\"Logistic Regression\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Logistic Regression</a>: A statistical method used for binary classification, predicting the <a data-tooltip-position=\"top\" aria-label=\"Probability\" data-href=\"Probability\" href=\"notes/math/probability.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">probability</a> that a given input vector belongs to a certain binary category. It is essentially a <a data-href=\"Regression\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Regression</a> model with a Logistic Function applied to map the output to a value between 0 and 1. <br>\n<a data-href=\"Probit Regression\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Probit Regression</a>: Similar to logistic regression, though, it assumes that errors between the predicted and actual values are normally distributed. Probit regression also uses the <a data-href=\"Probit Link Function\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Probit Link Function</a> rather than the logistic function. J. Han, M. Kamber, and J. Pei,&nbsp;Data Mining : Concepts and Techniques. Burlington, Ma: Elsevier, 2012.\n<br>GeeksforGeeks, “Jaccard Similarity,”&nbsp;GeeksforGeeks, Mar. 17, 2025. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/python/jaccard-similarity/\" target=\"_self\">https://www.geeksforgeeks.org/python/jaccard-similarity/</a> (accessed Aug. 01, 2025).\n<br>“Binary data,”&nbsp;Wikipedia, Sep. 13, 2021. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Binary_data\" target=\"_self\">https://en.wikipedia.org/wiki/Binary_data</a>\nWikipedia Contributors, “Binary regression,”&nbsp;Wikipedia, Mar. 27, 2022.\n","aliases":[],"inlineTags":[],"frontmatterTags":["#math","#statistics","#unfinished"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Binary Data","level":1,"id":"Binary_Data_0"},{"heading":"Representation","level":3,"id":"Representation_0"},{"heading":"No Numerical Significance","level":3,"id":"No_Numerical_Significance_0"},{"heading":"Symmetry","level":2,"id":"Symmetry_0"},{"heading":"Similarity","level":2,"id":"Similarity_0"},{"heading":"Symmetric","level":3,"id":"Symmetric_0"},{"heading":"Asymmetric","level":3,"id":"Asymmetric_0"},{"heading":"Regression","level":2,"id":"Regression_0"},{"heading":"References","level":2,"id":"References_0"}],"links":[".html","notes/math/nominal-data.html#_0","notes/math/nominal-data.html#_0","notes/math/mean.html#_0","notes/math/median.html#_0","notes/math/mode.html#_0",".html",".html",".html",".html",".html",".html","notes/math/probability.html#_0",".html",".html",".html"],"author":"","coverImageURL":"","fullURL":"notes/math/binary-data.html","pathToRoot":"../..","attachments":[],"createdTime":1752017585071,"modifiedTime":1754247516543,"sourceSize":4407,"sourcePath":"NOTES/Math/Binary Data.md","exportPath":"notes/math/binary-data.html","showInTree":true,"treeOrder":5,"backlinks":["notes/math/nominal-data.html","notes/math/outlier.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown"},"notes/math/associative-property.html":{"title":"Associative Property","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-19The associative property states that the sum or product of any group of values is not affected by how the values are grouped. It allows you to transform mathematical expressions into equivalent forms without altering its value.The associative property applies to addition. For any numbers , and :The associative property applies to multiplication. For any numbers , and :The associative property applies to matrix multiplication as well. For matrices , , and with compatible dimensions:Operations such as subtraction or division are not associative:\nSubtraction: Division: In <a data-href=\"Propositional Logic\" href=\"https://emujakic.github.io/TechKB/notes/math/propositional-logic.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Propositional Logic</a>, associativity is a rule of replacement that applies to some <a data-href=\"Logical Connectives\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Logical Connectives</a>, allowing for the rearrangement of grouping symbols without changing the truth value of the logical expression.Associativity applies to conjunction (AND), for any variables , , and :Associativity applies to disjunction (OR), for any variables , , and :<br>In set theory, the associative property refers to how the <a data-href=\"Union\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Union</a> and <a data-href=\"Intersection\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Intersection</a> of sets can be grouped without altering the outcome of the expression.for any sets , , and :for any sets , , and :<br>The <a href=\"https://emujakic.github.io/TechKB/notes/math/commutative-property.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"4\" to=\"24\" origin-text=\"commutative property\" class=\"internal-link virtual-link-a\">commutative property</a> states of values does not effect their sum or product.\nAddition: Multiplication: <br>Note that the <a href=\"https://emujakic.github.io/TechKB/notes/math/commutative-property.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"14\" to=\"34\" origin-text=\"commutative property\" class=\"internal-link virtual-link-a\">commutative property</a> does not apply to matrix multiplication.\n<br>GeeksforGeeks, “Associative Property,”&nbsp;GeeksforGeeks, Sep. 29, 2023. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/maths/associative-property/\" target=\"_self\">https://www.geeksforgeeks.org/maths/associative-property/</a> (accessed Jul. 19, 2025)\n<br>“Associative property,”&nbsp;Wikipedia, Jan. 16, 2023. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Associative_property\" target=\"_self\">https://en.wikipedia.org/wiki/Associative_property</a>\n","aliases":["Associative"],"inlineTags":[],"frontmatterTags":["#math","#algebra"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Associative Property","level":1,"id":"Associative_Property_0"},{"heading":"Associative Operations","level":3,"id":"Associative_Operations_0"},{"heading":"Addition","level":4,"id":"Addition_0"},{"heading":"Multiplication","level":4,"id":"Multiplication_0"},{"heading":"<a data-href=\"Matrix Multiplication\" href=\"Matrix Multiplication\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Matrix Multiplication</a>","level":4,"id":"[[Matrix_Multiplication]]_0"},{"heading":"Non-Associative Operations","level":3,"id":"Non-Associative_Operations_0"},{"heading":"Logical Operations","level":2,"id":"Logical_Operations_0"},{"heading":"<a data-href=\"Conjunction\" href=\"Conjunction\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Conjunction</a>","level":3,"id":"[[Conjunction]]_0"},{"heading":"<a data-href=\"Disjunction\" href=\"Disjunction\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Disjunction</a>","level":3,"id":"[[Disjunction]]_0"},{"heading":"<a data-href=\"Set Theory\" href=\"Set Theory\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Set Theory</a>","level":2,"id":"[[Set_Theory]]_0"},{"heading":"Union","level":3,"id":"Union_0"},{"heading":"Intersection","level":3,"id":"Intersection_0"},{"heading":"<a data-href=\"Commutative Property\" href=\"Commutative Property\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Commutative Property</a>","level":2,"id":"[[Commutative_Property]]_0"},{"heading":"References","level":2,"id":"References_0"}],"links":[".html","notes/math/propositional-logic.html#_0",".html",".html",".html",".html",".html",".html","notes/math/commutative-property.html#_0","notes/math/commutative-property.html#_0","notes/math/commutative-property.html#_0"],"author":"Ernad Mujakic","coverImageURL":"","fullURL":"https://emujakic.github.io/TechKB/notes/math/associative-property.html","pathToRoot":"../..","attachments":[],"createdTime":1752965745243,"modifiedTime":1754360736681,"sourceSize":2549,"sourcePath":"NOTES/Math/Associative Property.md","exportPath":"notes/math/associative-property.html","showInTree":true,"treeOrder":4,"backlinks":["notes/math/commutative-property.html","notes/math/propositional-logic.html"],"type":"markdown"},"notes/ai/turing-test.html":{"title":"Turing Test","icon":"","description":"Author: Ernad Mujakic\nDate: 2025-07-02A Turing test is a benchmark proposed by Alan Turing for evaluating a machine's ability to exhibit intelligence that is indistinguishable from that of a human. The test involves three participants, a human interrogator, a human respondent, and a machine. The interrogator communicates with both the human and machine via text, if the judge is unable to confidently identify which agent is the human and which is the machine, then the machine passes the Turing test. The result does not depend on the machine's ability to respond correctly, , only on how close the machine's language resembles human natural language. The CAPTCHA system is a famous implementation of a Turing test.To pass a Turing test a machine must have <a data-href=\"Natural Language Processing\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Natural Language Processing</a> and generating abilities. The machine also needs some form of <a data-href=\"Knowledge Representation\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Knowledge Representation</a> to be able to keep track of conversation, as well as <a data-href=\"Reasoning\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Reasoning</a>, so that the machine demonstrates some form of rationality to the interrogator.<br>There is an extension of the Turing test known as the total Turing test, where the machine has to also exhibit human sensory abilities and physical interactions. In this instance, the machine would also need robust <a data-href=\"Robotics\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Robotics</a>, <a data-href=\"Computer Vision\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Computer Vision</a>, <a data-href=\"Speech Recognition\" href=\".html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Speech Recognition</a>, navigation, and physical manipulation capabilities as well as a physical appearance that is indistinguishable from a human.One of the strengths of the Turing test lies in its simplicity. The test is straightforward to implement and understand which encourages broader participation in the realm of AI research.The Turing test is particularly versatile, allowing it to be applied to various forms of AI models, making it relevant across sub-disciplines within artificial intelligence research. The evaluation metrics of the test are also simple to expand or contract if specific capabilities would like to be included or excluded- such as computer vision or robotics. This allows researchers to tailor their assessment based on the goals of their project, rather than trying to conform to specific parameters of the test.Due to its subjective nature, the Turing Test is susceptible to human variability, as different interrogators may perceive specific behaviors as \"human-like\" or intelligent in varying ways. This inherent subjectivity can lead to inconsistent results, which undermines the test's reliability as a benchmark for evaluating artificial intelligence systems. As a result, the effectiveness and usefulness of the Turing test as a measure for machine intelligence has been heavily scrutinized by various researchers in the field of AI.Since the Turing Test focuses primarily on actions and behaviors, it overlooks the underlying processes that lead to the formulation of those behaviors, raising questions about whether those processes truly reflect intelligence. Many computer scientists argue that analyzing actions and behaviors alone is insufficient for determining whether a machine possesses intelligence. This critique emphasizes the need for a more comprehensive evaluation that considers not only observable outputs but also the mechanisms which deliver those outputs.Some critics of the Turing Test argue that the focus of AI research should be on augmenting or improving human behavior rather than merely mimicking it. They point out that certain human behaviors are inherently unintelligent, while some intelligent behaviors may be fundamentally inhuman. For example, a machine might deliberately avoid providing a correct answer to a challenging mathematical question to avoid raising suspicion with the interrogator. Some believe that this approach represents a misguided use of research efforts, diverting attention from the potential of AI to enhance human capabilities and solve complex problems.\n<br>Wikipedia contributors, “Turing test,” Wikipedia, Jun. 24, 2025. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Turing_test#Tractability_and_simplicity\" target=\"_self\">https://en.wikipedia.org/wiki/Turing_test#Tractability_and_simplicity</a>\n<br>GeeksforGeeks, “Turing Test in artificial intelligence,” GeeksforGeeks, Sep. 16, 2024. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/artificial-intelligence/turing-test-artificial-intelligence/\" target=\"_self\">https://www.geeksforgeeks.org/artificial-intelligence/turing-test-artificial-intelligence/</a>\nPeter. R. Norvig, Artificial Intelligence: A Modern Approach, Global Edition. 2021.\n","aliases":[],"inlineTags":[],"frontmatterTags":["#AI"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Turing Test","level":1,"id":"Turing_Test_0"},{"heading":"Requirements","level":2,"id":"Requirements_0"},{"heading":"Strengths","level":2,"id":"Strengths_0"},{"heading":"Simplicity","level":4,"id":"Simplicity_0"},{"heading":"Flexibility","level":4,"id":"Flexibility_0"},{"heading":"Weaknesses","level":2,"id":"Weaknesses_0"},{"heading":"Non-Deterministic","level":4,"id":"Non-Deterministic_0"},{"heading":"Behavior is not Intelligence","level":4,"id":"Behavior_is_not_Intelligence_0"},{"heading":"Augmentation over Imitation","level":4,"id":"Augmentation_over_Imitation_0"},{"heading":"References","level":2,"id":"References_0"}],"links":[".html",".html",".html",".html",".html",".html"],"author":"","coverImageURL":"","fullURL":"notes/ai/turing-test.html","pathToRoot":"../..","attachments":[],"createdTime":1751502135746,"modifiedTime":1754247499532,"sourceSize":4532,"sourcePath":"NOTES/AI/Turing Test.md","exportPath":"notes/ai/turing-test.html","showInTree":true,"treeOrder":2,"backlinks":["textbooks/ai-a-modern-approach/summary.html"],"type":"markdown"},"textbooks/data-mining-concepts-and-techniques/summary.html":{"title":"Summary","icon":"","description":"\nName: Jiawei Han, Micheline Kamber, and Jian Pei\nEdition: 3rd Edition\nData Mining: Concepts and Techniques is a comprehensive resource for the field of data mining. This book covers everything from data preprocessing to clustering and <a data-tooltip-position=\"top\" aria-label=\"Outlier\" data-href=\"Outlier\" href=\"https://emujakic.github.io/TechKB/notes/math/outlier.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">outlier</a> detection. This summary attempts to breaks down the main ideas and insights from the book.\n<br>\n<a data-href=\"Data Mining\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Data Mining</a>: The process of extracting knowledge from data. Is also referred to as knowledge mining/extraction or data pattern analysis. <br>\n<a data-href=\"Data Warehouse\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Data Warehouse</a>: A centralized repository of data from multiple sources. <br>Data warehouses are typically constructed through a <a data-href=\"ETL Process\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">ETL Process</a>, where data is extracted from sources, transformed into a desirable format, and then loaded into the warehouse.\nData warehouses are typically used to store historical data, allowing for long-term trend analysis. <br>\n<a data-href=\"Data Cube\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Data Cube</a>: A multi-dimensional array, where each dimension corresponds to an attribute or set of attributes. Cells typically store aggregate measures such as count or sum. Each dimension can contain hierarchies, such as yearly or quarterly, allowing for analysis at different levels. <br>\nKnowledge discovery from data is an iterative process of the following steps (steps 1-4 are different forms of <a data-href=\"Data Preprocessing\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Data Preprocessing</a>): Data Selection: Choosing and retrieving the data that is relevant for the analysis task.\nData Integration: Where data from multiple sources is combined.\nData Cleaning: Removing noise and inconsistencies from data.\nData Transformation: Where data is transformed into a form appropriate for mining.\nData Mining: Where intelligent methods are utilized to extract patterns present in the data.\nEvaluation: Assessing the patterns discovered in the previous step based on measures to determine if they are useful.\n<br>Knowledge Presentation: Presenting the discovered knowledge/patterns using intuitive methods such as <a data-href=\"Data Visualization\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Data Visualization</a> or report generation. <br>\n<a data-href=\"Database Management System\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Database Management System</a>: Software that enables users to create, manage, and manipulate a <a data-href=\"Database\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Database</a>. <br>\nDatabase: A collection of interrelated <a data-href=\"Structured Data\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Structured Data</a> . <br>\n<a data-href=\"Relational Database\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Relational Database</a>: A collection of structured tables, each which consist of a set of attributes (represented as columns), and stores entries as tuples of data (rows). Rows are typically identified using a primary key.\n<br>Most relational databases use <a data-href=\"SQL\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">SQL</a> to query the database. These queries include commands such as INSERT, JOIN, DELETE, or aggregate operators like SUM or MIN.\n<br>Relational databases can be modeled using an <a data-href=\"Entity-Relationship Diagram\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Entity-Relationship Diagram</a>. <br>\nData warehouses made up of data cubes can inherently support <a data-href=\"Online Analytical Processing\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Online Analytical Processing</a> (OLAP). OLAP allows users to explore data from different perspectives, such as time, geography, product lines, and customer segments. This allows for data analysis across multiple dimensions at varying levels of granularity.\nOLAP allows users to drill-down into more detailed/granular data, or roll-up to get more broad, summarized information. <br>\n<a data-href=\"Transactional Database\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Transactional Database</a>: A database which stores transactions, such as purchases or clicks on a website. <br>\nThere are a number of data mining functionalities such as characterization, discrimination, frequent pattern mining, associations, correlations, <a data-href=\"Classification\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Classification</a>, <a data-href=\"Regression\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Regression</a> , <a data-href=\"Clustering\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Clustering</a>, and <a data-tooltip-position=\"top\" aria-label=\"Outlier\" data-href=\"Outlier\" href=\"https://emujakic.github.io/TechKB/notes/math/outlier.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">outlier</a> analysis. Generally, such tasks can be classified as descriptive or predictive. Descriptive mining tasks describe characteristics of a dataset, this includes clustering or association rule learning.\nPredictive mining tasks, like regression or classification, aim to make predictions on future outcomes based on historical data. Data entries can be associated with classes or concepts. Class/concept descriptions can be derived from data characterization or data discrimination methods. <br>\nData characterization is the process of summarizing the general characteristics of a dataset. The output of data characterization can be <a data-tooltip-position=\"top\" aria-label=\"Bar Chart\" data-href=\"Bar Chart\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Bar Charts</a>, curves, <a data-tooltip-position=\"top\" aria-label=\"Pie Chart\" data-href=\"Pie Chart\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Pie Charts</a>, data cubes, and multidimensional tables. The output of data characterization can also be characteristic rules.\n<br>Common techniques for data characterization include <a data-href=\"Descriptive Statistics\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Descriptive Statistics</a>, <a data-href=\"Data Visualization\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Data Visualization</a>, and clustering. Data discrimination focuses on the comparison of the general characteristics of the target class against that of one or more contrasting classes. The forms of output are similar to those of data characterization. Discrimination descriptions expressed in rule form are referred to as discriminant rules.\n<br>Common techniques for data discrimination include classification algorithms, such as <a data-tooltip-position=\"top\" aria-label=\"Decision Tree\" data-href=\"Decision Tree\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Decision Trees</a> or <a data-tooltip-position=\"top\" aria-label=\"Support Vector Machine\" data-href=\"Support Vector Machine\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Support Vector Machines</a>. Frequent patterns are patterns that appear frequently in a dataset, they include frequent itemsets, subsequences, or substructures. Frequent itemsets are sets of items that are commonly together.\nFrequent subsequences are events that commonly share a particular sequence.\nFrequent substructures are patterns in structural representations like graphs or trees. <br>\n<a data-href=\"Association Rule\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Association Rule</a>: A conditional rule that describes a relationship between variables in a dataset. An association rule is made up of an antecedent - the item or set of items that suggest the presence of another item if present, and a consequent - the item or set of items that are predicted to be present if the the antecedent is present.\nAssociation rules with a single predicate are single-dimensional, otherwise it's a multidimensional association rule.\nConfidence: Indicates how often the consequent is true, given the antecedent.\nSupport: measures how often the items appear together in the dataset.\nAssociation rules can be discarded if they don't meet a specified support and confidence threshold. <br>\n<a data-href=\"Classification\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Classification</a>: The process of finding a model that categorizes a set of data objects into predefined classes. The models are derived based on analysis performed on training data, that is, a set of pre-labelled data. <br>The model may be represented as classification rules, decision trees, neural networks, or distance based models such as <a data-href=\"K-Nearest Neighbors\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">K-Nearest Neighbors</a>. A decision tree is a tree which splits a dataset based on feature values, where a path from the root to a leaf represents a decision rule. A neural network is a collection of neurons with weighted synapses (connections) between them. Each neuron functions as a processing unit that takes inputs, applies a transformation, and \"activates\" when a feature matches a specific pattern. <br>\n<a data-href=\"Regression\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Regression</a>: The process of finding a model that predicts a continuous-valued function. Regression analysis is used when the dependent variable is numeric, rather than categorical. Relevance Analysis: The process of identifying which features (independent variables) are most relevant for the prediction of the independent variable. Relevance analysis typically preempts regression and classification.\n<br>Relevance analysis techniques include statistical tests such as the <a data-href=\"Chi-Square Test\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Chi-Square Test</a>, or correlation analysis techniques like <a data-href=\"Pearson Correlation\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Pearson Correlation</a> or <a data-href=\"Spearman Rank Correlation\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Spearman Rank Correlation</a>.\n<br>Other techniques include <a data-href=\"Recursive Feature Elimination\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Recursive Feature Elimination</a>, <a data-href=\"Principal Component Analysis\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Principal Component Analysis</a>, or <a data-href=\"Lasso Regression\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Lasso Regression</a>. <br>\n<a data-href=\"Clustering\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Clustering</a>: The Process of grouping similar data points together into 'clusters' where each cluster can be viewed as a class of data objects. <br>Clusters are formed based on <a data-href=\"Distance Measures\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Distance Measures</a>, which quantify the similarity or dissimilarity of data objects.\n<br>Popular clustering techniques include <a data-href=\"K-Means Clustering\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">K-Means Clustering</a>, <a data-href=\"DBSCAN\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">DBSCAN</a>, or <a data-href=\"Gaussian Mixture Models\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Gaussian Mixture Models</a>. <br>\n<a data-href=\"Outlier Analysis\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Outlier Analysis</a>: The process of detecting <a data-tooltip-position=\"top\" aria-label=\"Outlier\" data-href=\"Outlier\" href=\"https://emujakic.github.io/TechKB/notes/math/outlier.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">outliers</a>, or anomalies in a dataset. <br><a href=\"https://emujakic.github.io/TechKB/notes/math/outlier.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"0\" to=\"8\" origin-text=\"Outliers\" class=\"internal-link virtual-link-a\">Outliers</a> may be detected using distance-based methods like <a data-href=\"K-Nearest-Neighbors\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">K-Nearest-Neighbors</a>, statistical methods such s <a data-href=\"Z-Score\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Z-Score</a> or <a data-tooltip-position=\"top\" aria-label=\"Box Plot\" data-href=\"Box Plot\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Box Plots</a>, or density based methods like DBSCAN. <br>\n<a data-href=\"Measures of Pattern Interestingness\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Measures of Pattern Interestingness</a>: Measure the significance and usefulness of discovered patterns. <br><a data-href=\"Support\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Support</a>: The proportion of objects in the dataset that contain the pattern.\n<br><a data-href=\"Confidence\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Confidence</a>: The likelihood that a pattern is present given another pattern.\n<br><a data-href=\"Lift\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Lift</a>: Measures how much more likely the presence of one pattern is to occur with another pattern compared to their individual <a data-tooltip-position=\"top\" aria-label=\"Probability\" data-href=\"Probability\" href=\"https://emujakic.github.io/TechKB/notes/math/probability.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">probabilities</a>.\n<br><a data-href=\"Accuracy\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Accuracy</a>: The ratio of correctly predicted instances to the total number of instances.\n<br><a data-href=\"Entropy\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Entropy</a>: Measures the amount of uncertainty in a dataset or pattern.\nCoverage: The percentage of data to which the rule applies. Subjective interestingness measures are based on the user's beliefs, knowledge, and context rather than solely on statistical properties. Completeness: Referring to the ability of a data mining algorithm to generate all of the patterns that are present in the data. <br>\nStatistical Model: A mathematical representation that describes the behavior of objects in a target class in terms of <a data-tooltip-position=\"top\" aria-label=\"Random Variable\" data-href=\"Random Variable\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Random Variables</a> and their associated <a href=\"https://emujakic.github.io/TechKB/notes/math/probability.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"22\" to=\"33\" origin-text=\"probability\" class=\"internal-link virtual-link-a\">probability</a> distributions. Statistical models can be the output of a data mining task, or the foundation upon which data mining tasks are built upon. Predictive statistics models data to account for uncertainty in the observations and is used to draw inferences about the underlying process under investigation. <br>\n<a data-href=\"Confirmatory Data Analysis\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Confirmatory Data Analysis</a>: A statistical approach which aims to confirm or refute a hypothesis based on statistical evidence. A result is statistically significant if it is unlikely to have occurred by chance. <br>\n<a data-href=\"Machine Learning\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Machine Learning</a>: A branch of <a data-href=\"Artificial Intelligence\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Artificial Intelligence</a> which deals with the development of algorithms and statistical models which can learn patterns from data and make intelligent decisions. <br><a data-href=\"Supervised Learning\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Supervised Learning</a>: Training a machine learning algorithm on prelabelled data. This includes <a data-href=\"Classification\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Classification</a> or <a data-href=\"Regression\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Regression</a> algorithms.\n<br><a data-href=\"Unsupervised Learning\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Unsupervised Learning</a>: Models which identify patterns in unlabeled data. This includes <a data-href=\"Clustering\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Clustering</a> or <a data-href=\"Dimensionality Reduction\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Dimensionality Reduction</a> techniques.\n<br><a data-href=\"Semi-Supervised Learning\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Semi-Supervised Learning</a>: Combines both supervised and unsupervised learning. Useful for when there is a small amount of labeled data and a large amount of unlabeled data available.\n<br><a data-href=\"Reinforcement Learning\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Reinforcement Learning</a>: Models learn by interacting with their environment and receiving feedback as rewards or penalties.\nActive Learning: Models selectively query the user to label an example. The model identifies which unlabeled data points would provide the most valuable information if labeled and requests labels for those specific instances. Information Retrieval: The process of searching for and obtaining information from sources. This field focuses on searching through sources, such as documents or databases, and matching data points based on user queries that are mainly formed by keywords. <br>This process is often assessed with metrics such as <a data-href=\"Precision\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Precision</a> and <a data-href=\"Recall\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Recall</a>.\nCommon techniques include keyword-based searching, Boolean retrieval using logical operators, or machine learning approaches.\nA topic model is a statistical model which models the topic of a document as a probability distribution over the vocabulary of the document. Search engines are very large data mining applications that use techniques such as crawling, indexing, and searching. <br>\n<a data-href=\"Measure of Central Tendency\" href=\"https://emujakic.github.io/TechKB/notes/math/measure-of-central-tendency.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Measure of Central Tendency</a>: Measures which give an idea about the center of a distribution. Common measures include <a data-tooltip-position=\"top\" aria-label=\"Mean\" data-href=\"Mean\" href=\"https://emujakic.github.io/TechKB/notes/math/mean.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">mean</a>, <a data-tooltip-position=\"top\" aria-label=\"Median\" data-href=\"Median\" href=\"https://emujakic.github.io/TechKB/notes/math/median.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">median</a>, and <a data-tooltip-position=\"top\" aria-label=\"Mode\" data-href=\"Mode\" href=\"https://emujakic.github.io/TechKB/notes/math/mode.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">mode</a>. Mean: Also called the average, is the sum of values over the number of values in a dataset. The mean is typically very sensitive to outliers.\n<br><a href=\"https://emujakic.github.io/TechKB/notes/math/mode.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"0\" to=\"4\" origin-text=\"Mode\" class=\"internal-link virtual-link-a\">Mode</a>: The most frequently occurring value in a dataset. for unimodal, moderately skewed numerical data: Median: The middle value of an ordered dataset. Better for skewed data, since it is less affected by outliers. Dataset: A set of data objects— single entities described by attribute values. Data objects are also referred to as samples, records, or data points. If the data objects are stored in a database, they are data tuples. Attribute: Also called a feature, is a data field representing a specific characteristic of a data object. Observed values for attributes are called observations.\nA set of attributes used to describe an object are called an attribute/feature vector. The type of an attribute is determined by its domain. The most common types include. <br><a data-tooltip-position=\"top\" aria-label=\"Nominal Data\" data-href=\"Nominal Data\" href=\"https://emujakic.github.io/TechKB/notes/math/nominal-data.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Nominal</a>: Each value represents some sort of category, therefore, they are also commonly referred to as categorical. A nominal attribute may be instantiated as an integer, though this integer is only used to refer to a category and cannot be used quantitatively.\n<br><a data-tooltip-position=\"top\" aria-label=\"Binary Data\" data-href=\"Binary Data\" href=\"https://emujakic.github.io/TechKB/notes/math/binary-data.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Binary</a>: A nominal attribute with only 2 states, zero or one. Binary attributes are called Boolean if the values represent true and false.\n<br><a data-tooltip-position=\"top\" aria-label=\"Ordinal Data\" data-href=\"Ordinal Data\" href=\"https://emujakic.github.io/TechKB/notes/math/ordinal-data.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Ordinal</a>: A type of categorical data, where values have some sort of meaningful order among them. Ordinal attributes may be obtained from the discretization of a numeric attribute.\n<br><a data-tooltip-position=\"top\" aria-label=\"Numeric Data\" data-href=\"Numeric Data\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Numeric</a>: A quantitative attribute represented by integer or real values. Numeric attributes can be interval or ratio scaled. <br>\n<a data-href=\"Interval Scaled\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Interval Scaled</a>: Numeric attributes measured on a scale of equal units. Interval scaled attributes have no true zero point, that is, the value zero doesn't represent the absence of the attribute. For example, 0° does not represent no temperature. You can perform addition and subtraction on interval scaled attributes, but no multiplication or division. <br>\n<a data-href=\"Ratio Scaled\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Ratio Scaled</a>: Numeric attributes with a true zero point, allowing for the comparison of absolute magnitudes. This means all arithmetic operations are applicable. Discrete Attribute: An attribute with a finite or countable infinite set of possible values. <br>\nContinuous Attribute: An attribute which can take on any value in a given <a href=\"https://emujakic.github.io/TechKB/notes/math/range.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"54\" to=\"59\" origin-text=\"range\" class=\"internal-link virtual-link-a\">range</a>. <br>\nStatistical Descriptions: Used to infer properties of data and highlight what values should be treated as noise or <a data-tooltip-position=\"top\" aria-label=\"Outlier\" data-href=\"Outlier\" href=\"https://emujakic.github.io/TechKB/notes/math/outlier.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">outliers</a>. <br>\n<a data-href=\"Midrange\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Midrange</a>: The <a href=\"https://emujakic.github.io/TechKB/notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"6\" to=\"10\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a> of the minimum and maximum values in a dataset. <br>\nData is positively skewed if the mean is a value smaller than the <a href=\"https://emujakic.github.io/TechKB/notes/math/median.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"41\" to=\"47\" origin-text=\"median\" class=\"internal-link virtual-link-a\">median</a>. Data is negatively skewed if the mean is greater than the median. <br>\n<a data-href=\"Measures of Dispersion\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Measures of Dispersion</a>: Statistical measures which characterize the spread of data. Common measures include: <br><a data-href=\"Range\" href=\"https://emujakic.github.io/TechKB/notes/math/range.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Range</a>: The difference between the maximum and minimum values in a dataset.\n<br><a data-href=\"Quantile\" href=\"https://emujakic.github.io/TechKB/notes/math/quantile.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Quantile</a>: The data points which split an ordered dataset into equal or near-equal parts.\n<br><a data-href=\"Quartile\" href=\"https://emujakic.github.io/TechKB/notes/math/quartile.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Quartile</a>: The 4-quantiles, or the three data points which split a dataset into four equal parts.\n<br><a data-href=\"Percentile\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Percentile</a>: The 100-quantiles, or the 99 values which split a dataset into 100 equal subsets.\n<br><a data-href=\"Interquartile Range\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Interquartile Range</a>: The difference between the third and first quartile.\n<br><a data-href=\"Variance\" href=\"https://emujakic.github.io/TechKB/notes/math/variance.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Variance</a>: The average squared distance of each value from the mean value of the dataset.\n<br><a data-href=\"Standard Deviation\" href=\"https://emujakic.github.io/TechKB/notes/math/standard-deviation.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Standard Deviation</a>: The average distance of each value from the mean value of the dataset. <br>\n<a data-href=\"Chebyshev's Inequality\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Chebyshev's Inequality</a>: A statistical theorem which shows that the proportion of values that lie more than <a href=\"https://emujakic.github.io/TechKB/notes/math/standard-deviation.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"1\" to=\"20\" origin-text=\"standard deviations\" class=\"internal-link virtual-link-a\">standard deviations</a> away from the mean is at most . <br>\n<a data-href=\"Five-Number Summary\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Five-Number Summary</a>: The first and third <a href=\"https://emujakic.github.io/TechKB/notes/math/quartile.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"22\" to=\"30\" origin-text=\"quartile\" class=\"internal-link virtual-link-a\">quartile</a>, median, and least and greatest values in a dataset. A <a data-href=\"Box Plot\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Box Plot</a> is used to visualize the five-number summary. Data Visualization: Graphical representations of data using charts, graphs, or maps. <br>\n<a data-href=\"Quantile Plot\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Quantile Plot</a>: A data visualization technique where the x-axis represents the <a href=\"https://emujakic.github.io/TechKB/notes/math/quantile.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"65\" to=\"74\" origin-text=\"quantiles\" class=\"internal-link virtual-link-a\">quantiles</a> of a theoretical or real distribution, and the y-axis represents the quantiles of the observed data. Quantile plots are used to visualize the distribution of data, including any bias or skewness in the dataset. <br>\n<a data-href=\"Histogram\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Histogram</a>: Also known as a bar chart, where a bar is drawn for each known value of , and the height of the bar represents the frequency of that value in the dataset. If is numeric, the domain of is partitioned into bins—disjoint, consecutive subranges. The range of a bin is called its width. <br>\n<a data-href=\"Scatterplot\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Scatterplot</a>: A data visualization technique which visualizes the relationship between two numeric attributes. Scatterplots are useful to visualize clusters, <a data-tooltip-position=\"top\" aria-label=\"Outlier\" data-href=\"Outlier\" href=\"https://emujakic.github.io/TechKB/notes/math/outlier.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">outliers</a>, or correlations. Scatterplots can be extended to n-dimensions to create a scatter matrix. <br>\n<a data-href=\"Pixel-Oriented Visualization\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Pixel-Oriented Visualization</a>: A class of data visualization techniques which use individual pixels as the primary unit of display. <br>\n<a data-href=\"Space-Filling Curve\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Space-Filling Curve</a>: Continuous curves that pass through every point in a multidimensional space. One of the most common curves is the <a data-href=\"Hilbert Curve\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Hilbert Curve</a>. Circle-Segment Technique: Uses windows in the shape of segments of a circle, each point corresponds to one dimension of one data record. <br>\n<a data-href=\"Geometric Projection\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Geometric Projection</a>: Techniques used to represent high-dimensional objects on a two-dimensional surface. Scatterplot Matrix: An by grid of 2D scatterplots that visualize each dimension with each other dimension. <br>\n<a data-href=\"Parallel Coordinates\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Parallel Coordinates</a>: A visualization technique that can handle high-dimensional data. Each -dimensional data point is a line crossing equally spaced parallel axes, each representing one dimension. Icon-Based Visualization: A class of visualization techniques which used icons to represent multidimensional data objects. Two common techniques are Chernoff faces and \"stick figures\". Hierarchical Visualization: A class of visualization techniques which organize dimensions in a nested, hierarchical manner. Tree maps are a popular hierarchical visualization technique which visualize data as a set of nested triangles. Commonly used for visualizing changes in the stock market based on sector. <br>\nCluster: A collection of 'similar' data objects. Clustering is important for discovering potential classes of data objects, or for identifying <a href=\"https://emujakic.github.io/TechKB/notes/math/outlier.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"136\" to=\"144\" origin-text=\"outliers\" class=\"internal-link virtual-link-a\">outliers</a>. Measures of Proximity: Statistical measures which quantify the similarity or dissimilarity of data objects. Multiattribute Objects are also referred to as data samples or feature vectors. Data Matrix: A data structure which stores data objects in an matrix, where is the number of attributes. Each row represents a data object. Dissimilarity Matrix: An matrix which stores the dissimilarity values for all pairs of objects. This matrix is symmetric, meaning that . Many clustering and nearest-neighbor algorithms operate on a dissimilarity matrix. <br>\n<a data-tooltip-position=\"top\" aria-label=\"Nominal Data\" data-href=\"Nominal Data\" href=\"https://emujakic.github.io/TechKB/notes/math/nominal-data.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Nominal Attribute</a>: A type of <a data-href=\"Categorical Data\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Categorical Data</a> that represents categories that don't have a specific order or ranking. The dissimilarity between 2 objects represented by <a href=\"https://emujakic.github.io/TechKB/notes/math/nominal-data.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"124\" to=\"131\" origin-text=\"nominal\" class=\"internal-link virtual-link-a\">nominal</a> attributes is computed based on the ratio of mismatches: Where is the number of matches and is the total number of attributes. <br>\n<a data-href=\"One-Hot Encoding\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">One-Hot Encoding</a>: A technique for encoding nominal attributes into <a data-tooltip-position=\"top\" aria-label=\"Binary Data\" data-href=\"Binary Data\" href=\"https://emujakic.github.io/TechKB/notes/math/binary-data.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">binary attributes</a>, where is the number of states in the nominal attribute. A binary attribute is symmetric if each state 0 or 1 is equally valuable. A binary attribute is asymmetric if the 2 states are not equally important, such as the outcomes of a disease test. <br>One approach for computing the dissimilarity of <a href=\"https://emujakic.github.io/TechKB/notes/math/binary-data.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"48\" to=\"59\" origin-text=\"binary data\" class=\"internal-link virtual-link-a\">binary data</a> involves computing a dissimilarity matrix. If all binary attributes have equal weight, then can represent the number of attributes that are 1 for both object and . A variable can represent the number of attributes that are 1 for and 0 for , can represent the number of attributes which are 0 for and 1 for . Finally, can represent the number of attributes which are 0 for both and . If objects and are described by symmetric binary attributes, then the dissimilarity between and is .\nIf objects and are described by asymmetric binary attributes, then the dissimilarity between and is defined as . Conversely, the asymmetric binary similarity between and is . <br>The coefficient is called the <a data-href=\"Jaccard Coefficient\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Jaccard Coefficient</a>. <br>\n<a data-href=\"Distance Measures\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Distance Measures</a>: A class of statistical measures used to quantify the proximity of numeric attributes. The most common measures include the <a data-href=\"Euclidean Distance\" href=\"https://emujakic.github.io/TechKB/notes/math/euclidean-distance.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Euclidean Distance</a>, <a data-href=\"Chebyshev Distance\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Chebyshev Distance</a>, <a data-href=\"Manhattan Distance\" href=\"https://emujakic.github.io/TechKB/notes/math/manhattan-distance.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Manhattan Distance</a>, and <a data-href=\"Minkowski Distance\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Minkowski Distance</a>. <br>\nThe <a href=\"https://emujakic.github.io/TechKB/notes/math/euclidean-distance.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"4\" to=\"22\" origin-text=\"Euclidean distance\" class=\"internal-link virtual-link-a\">Euclidean distance</a> is the most popular <a data-href=\"Distance Measure\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Distance Measure</a> and it represents the straight-line distance between two points in Euclidean space and is calculated using the <a data-href=\"Pythagorean Theorem\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Pythagorean Theorem</a>. <br>\nThe <a href=\"https://emujakic.github.io/TechKB/notes/math/manhattan-distance.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"4\" to=\"22\" origin-text=\"Manhattan distance\" class=\"internal-link virtual-link-a\">Manhattan distance</a>, also called the city-block distance, is defined as . Both the Euclidean distance and the Manhattan distance satisfy the following properties: Non-Negativity: is never negative.\nIdentity of Indiscernibles: , the distance of an object to itself is always 0.\nSymmetry: <br><a data-href=\"Triangle Inequality\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Triangle Inequality</a>: , going directly from one point to another is never more than making a detour to some third point. A measure that satisfies all of these qualities is called a metric. The non-negative property is implied by the 3 other properties. The Minkowski distance is a generalization of the Euclidean and Manhattan distances, , where is some real number such that . Such a distance is also the norm where refers to . When is 1, it represents the Manhattan distance. When is 2, it represents the Euclidean distance ( norm). The Chebyshev distance (, norm, supremum distance), is a generalization of the Minkowski distance for . To compute it, we find the attribute that gives the maximum distance in values between 2 objects, . <br>\nThe treatment of <a href=\"https://emujakic.github.io/TechKB/notes/math/ordinal-data.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"17\" to=\"24\" origin-text=\"ordinal\" class=\"internal-link virtual-link-a\">ordinal</a> attributes is similar to numeric attributes when computing dissimilarity between objects. To compute the dissimilarity of objects of mixed attribute types, you process the attributes types together, then perform a single analysis. Suppose a dataset has attributes of mixed type, the dissimilarity of objects and is defined as: Where the indicator if either or is missing, or and attribute is asymmetric binary. Otherwise, . The computation of attribute to the dissimilarity of and () depends on the attribute type of : If is numeric, , where runs over all non-missing values of attribute . <br>\nIf is <a data-tooltip-position=\"top\" aria-label=\"Nominal Data\" data-href=\"Nominal Data\" href=\"https://emujakic.github.io/TechKB/notes/math/nominal-data.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">nominal</a> or binary, if , otherwise it equals 1. If is ordinal, compute the ranks and , then treat as numeric. <br>\n<a data-href=\"Data Normalization\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Data Normalization</a>: The process of adjusting the values in a dataset to a common scale without distorting the differences in the ranges of values. A document can be represented by a term-frequency vector, with each attribute recording the frequency of a particular word or phrase in the document. Term frequency vectors are typically long and sparse, meaning they have many 0 values. Traditional distance measures are incompatible with sparse data, since the many shared 0 values would suggest that the two documents are similar, even if they're vastly different. <br><a data-href=\"Cosine Similarity\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Cosine Similarity</a> can be used to compare term-frequency vectors with respect to a third vector of query words. Let and be two vectors, using the cosine similarity measure, we have: Where is the Euclidean norm of vector defined as .\nThe measure computes the cosine of the angle between vector and . A cosine value of 0 means the 2 vectors are at 90° to each other and have no match.\nCosine similarity is a non-metric measure.\nWhen attributes are binary, the cosine similarity function can be interpreted in terms of shared features. Therefore, is a measure of relative possession of shared features: <br>Which is the ratio of the number of shared attributes between and to the number of attributes possessed by or . This is known as the <a data-href=\"Tanimoto Distance\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Tanimoto Distance</a>. Data preprocessing is the process taken to improve the quality of data in order to increase the efficiency, ease, and quality of the data mining process. Data preprocessing consists of: Data Cleaning: Involves dealing with missing values, smoothing noisy data, removing outliers, and resolving inconsistencies.\nData Integration: Involves consolidating data from multiple sources into one location, typically a dataset, data cube, or file.\nData Reduction: Involves obtaining a reduced representation of the data while minimizing the loss of relevant information for the data mining process.\nData Transformation: Involves normalization (feature scaling), discretization, and concept hierarchy generation. Data quality is measured using the following metrics: Accuracy: Having the correct attribute values. Data may be inaccurate due to faulty data collection, entry, transmission, or from inconsistencies in how fields are formatted. Disguised missing data occurs when users intentionally enter incorrect data in order to preserve anonymity.\nCompleteness: Having attribute values filled out. Data may be incomplete due to failure to collect, enter, or transmit the data.\nConsistency:\nTimeliness: Having the data available when needed. Timeliness may be compromised if users fail to submit data on time.\nBelievability: How trustworthy the data is by users. If users believe that the data is of poor quality, that belief in and of itself reduces the quality of the data. This is due to users not willing to use the data or take it seriously, thus, reducing the value of the data.\nInterpretability: How easily the data is understood. A missing value may not necessarily indicate an error. Each attribute should have at least one rule for handling null values. To handle missing values you can: Ignore the tuple: This is usually done if the class label is missing in the case of a classification problem, if a single tuple has many missing values, or if they're few tuples missing values. This is generally a poor choice since useful data is also being removed.\nManually fill in the values: This is only feasible for very small datasets and is still a poor method due to arbitrary values being inserted, thereby adding noise to the data.\nUse a global constant: Missing values can be replaced with a constant such as null or , though the mining program may mistakenly think there is a relationship between tuples with the global constant present.\n<br>Use a <a data-tooltip-position=\"top\" aria-label=\"Measure of Central Tendency\" data-href=\"Measure of Central Tendency\" href=\"https://emujakic.github.io/TechKB/notes/math/measure-of-central-tendency.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">measure of central tendency</a>: A measure, such as the <a href=\"https://emujakic.github.io/TechKB/notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"25\" to=\"29\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a> or <a href=\"https://emujakic.github.io/TechKB/notes/math/median.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"33\" to=\"39\" origin-text=\"median\" class=\"internal-link virtual-link-a\">median</a> can be used to fill in the value. The mean is more appropriate for symmetric distributions, while the median is more appropriate for skewed distributions.\n<br>Use a measure of central tendency from the samples from the same class: Calculate a <a href=\"https://emujakic.github.io/TechKB/notes/math/measure-of-central-tendency.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"14\" to=\"41\" origin-text=\"measure of central tendency\" class=\"internal-link virtual-link-a\">measure of central tendency</a> using only data objects that are of the same class as the tuple with the missing value. <br>Fill in the most probable missing value: This may be calculated using <a data-href=\"Regression\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Regression</a>, Bayesian formalism, or <a data-href=\"Decision Tree\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Decision Tree</a> induction. This method uses the most information from the available data. <br>\nNoise: Random error or <a data-tooltip-position=\"top\" aria-label=\"Variance\" data-href=\"Variance\" href=\"https://emujakic.github.io/TechKB/notes/math/variance.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">variance</a> present in a dataset. <a href=\"https://emujakic.github.io/TechKB/notes/math/outlier.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"23\" to=\"31\" origin-text=\"Outliers\" class=\"internal-link virtual-link-a\">Outliers</a> commonly represent noise since they stray far most other observations. Noise can be smoothed using techniques like: <br>Binning: Performs local smoothing by discretizing attribute values using the neighborhood of observations in the sorted data. Attribute values are sorted, then partitioned into a fixed number of bins, which are consecutive disjoint sets. In smoothing by bin means, each value in a bin is replaced by that bin's mean. Smoothing by bin medians, similarly, replaces each value in the bin with the bin's median. Smoothing by bin boundaries replaces each value in the bin with the value of the nearest bin boundary. Bins may be equal-frequency, where each bin has the same number of values; or they may be equal width, that is, they have the same <a href=\"https://emujakic.github.io/TechKB/notes/math/range.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"30\" to=\"35\" origin-text=\"range\" class=\"internal-link virtual-link-a\">range</a>. Binning is also a form of data discretization.\n<br><a data-href=\"Regression\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Regression</a>: Data can be smoothed using regression—the process of finding a function which best fits a dataset. The values of the attribute can be replaced with the regression models's predicted values.\n<br><a data-href=\"Outlier Analysis\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Outlier Analysis</a>: Techniques like clustering, scatterplots, or z-score analysis can be used to identify and remove outliers from a dataset. The first step in data cleaning is discrepancy detection. Discrepancies may arise due to human error, deliberate errors, or data decay (data becoming less accurate over time). Inconsistent data representations are also a source of discrepancies, such as inconsistent encoding schemas or formatting. Nested discrepancies occur when the correction of one discrepancy reveals the presence of another. Metadata is data about data. Metadata is useful for discrepancy detection for providing information about an attribute's data type, acceptable range, or dependencies.\nField overloading is another source of discrepancies where fields can have the same name but be of different data types.\nUnique rules say that each value in a dataset must be unique. Consecutive rules say that there can be no missing values between the range of an attribute. Null rules specify the handling of null conditions, such as whether they are permitted or not. Data scrubbing tools, such as spell-checking, make use of domain knowledge to detect and correct errors. Data auditing tools analyze the data to find rules or relationships, as well as identifying objects that violate such rules. Data migration tools allow simple transformations, such as renaming of attributes. Extraction/Transformation/Loading (ETL) tools provide users with a GUI for performing transformations. Careful data integration can help reduce the amount of data cleaning needed by avoiding redundancies and inconsistencies. One of the biggest problems in data integration is the entity identification problem, which is the problem of identifying which real-world entities correspond to which data objects. This includes the schema integration and object matching processes. Redundancy Question other relevant information J. Han and M. Kamber,&nbsp;Data Mining : Concepts and Techniques, 3rd ed. Haryana, India ; Burlington, Ma: Elsevier, 2018.\n","aliases":[],"inlineTags":[],"frontmatterTags":["#textbook","#AI","#data","#machineLearning","#statistics"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Textbook Summary: Data Mining: Concepts and Techniques","level":1,"id":"Textbook_Summary_Data_Mining_Concepts_and_Techniques_0"},{"heading":"Author(s):","level":3,"id":"Author(s)_0"},{"heading":"Introduction","level":2,"id":"Introduction_0"},{"heading":"Chapter Overview","level":2,"id":"Chapter_Overview_0"},{"heading":"Ch.1 Introduction:","level":3,"id":"Ch.1_Introduction_0"},{"heading":"Ch.2 Getting to Know Your Data","level":3,"id":"Ch.2_Getting_to_Know_Your_Data_0"},{"heading":"Ch.3 Data Preprocessing","level":3,"id":"Ch.3_Data_Preprocessing_0"},{"heading":"Questions for Further Study","level":2,"id":"Questions_for_Further_Study_0"},{"heading":"Additional Notes","level":2,"id":"Additional_Notes_0"},{"heading":"References","level":2,"id":"References_0"}],"links":["notes/math/outlier.html#_0",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html","notes/math/outlier.html#_0",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html","notes/math/outlier.html#_0","notes/math/outlier.html#_0",".html",".html",".html",".html",".html",".html",".html","notes/math/probability.html#_0",".html",".html",".html","notes/math/probability.html#_0",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html","notes/math/measure-of-central-tendency.html#_0","notes/math/mean.html#_0","notes/math/median.html#_0","notes/math/mode.html#_0","notes/math/mode.html#_0","notes/math/nominal-data.html#_0","notes/math/binary-data.html#_0","notes/math/ordinal-data.html#_0",".html",".html",".html","notes/math/range.html#_0","notes/math/outlier.html#_0",".html","notes/math/mean.html#_0","notes/math/median.html#_0",".html","notes/math/range.html#_0","notes/math/quantile.html#_0","notes/math/quartile.html#_0",".html",".html","notes/math/variance.html#_0","notes/math/standard-deviation.html#_0",".html","notes/math/standard-deviation.html#_0",".html","notes/math/quartile.html#_0",".html",".html","notes/math/quantile.html#_0",".html",".html","notes/math/outlier.html#_0",".html",".html",".html",".html",".html","notes/math/outlier.html#_0","notes/math/nominal-data.html#_0",".html","notes/math/nominal-data.html#_0",".html","notes/math/binary-data.html#_0","notes/math/binary-data.html#_0",".html",".html","notes/math/euclidean-distance.html#_0",".html","notes/math/manhattan-distance.html#_0",".html","notes/math/euclidean-distance.html#_0",".html",".html","notes/math/manhattan-distance.html#_0",".html","notes/math/ordinal-data.html#_0","notes/math/nominal-data.html#_0",".html",".html",".html","notes/math/measure-of-central-tendency.html#_0","notes/math/mean.html#_0","notes/math/median.html#_0","notes/math/measure-of-central-tendency.html#_0",".html",".html","notes/math/variance.html#_0","notes/math/outlier.html#_0","notes/math/range.html#_0",".html",".html"],"author":"Ernad Mujakic","coverImageURL":"","fullURL":"https://emujakic.github.io/TechKB/textbooks/data-mining-concepts-and-techniques/summary.html","pathToRoot":"../..","attachments":[],"createdTime":1751226225017,"modifiedTime":1754407904126,"sourceSize":32714,"sourcePath":"TEXTBOOKS/Data Mining-Concepts and Techniques/Summary.md","exportPath":"textbooks/data-mining-concepts-and-techniques/summary.html","showInTree":true,"treeOrder":31,"backlinks":[],"type":"markdown"},"textbooks/data-mining-concepts-and-techniques/practice-problems.html":{"title":"Practice Problems","icon":"","description":" Data mining is the application of analytical techniques to extract patterns and knowledge from data. Data mining is a essential and impactful consequence of the recent exponential availability of digital data. It is both the result of the evolution of database technology, which stores and organizes data in a structured way, allowing for efficient access for data analysis, as well as a consequence of machine learning evolution which requires large datasets to train reliable, accurate models. A database is a single schema consisting of related data stored in a single place. A data warehouse, on the other hand, stores data from multiple sources and timeframes, and allows multidimensional analysis which can make use of data from different levels of abstraction. Characterization is the summarization of the general characteristics or features of a target class of data. Discrimination is the comparison of the general characteristics of the target class of data to a set of contrasting classes. Association is the analysis of correlations between data objects or attribute-value pairs. <a data-href=\"Classification\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Classification</a> is the process of finding a model that distinguishes data classes from one another. <a data-href=\"Regression\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Regression</a> is the process of finding a model which accurately predicts a continuous, dependent feature of the data. <a data-href=\"Clustering\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Clustering</a> is the process of grouping and separating data objects based on statistical similarities. <a href=\"https://emujakic.github.io/TechKB/notes/math/outlier.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"91\" to=\"98\" origin-text=\"Outlier\" class=\"internal-link virtual-link-a\">Outlier</a> analysis is the discovery of outliers in a dataset. Data mining may be crucial to the success of a business when it comes to quality control analysis, which would allow an organization to better detect shortcomings in quality control systems and be able to produce a more consistent product. X X <br>Two methods that could be used to detect outliers is the <a data-href=\"Z-Score\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Z-Score</a> method and DBSCAN. Z-score methods detect outliers based on the number of <a href=\"https://emujakic.github.io/TechKB/notes/math/standard-deviation.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"75\" to=\"94\" origin-text=\"standard deviations\" class=\"internal-link virtual-link-a\">standard deviations</a> a data point is from the <a href=\"https://emujakic.github.io/TechKB/notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"120\" to=\"124\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a>. DBSCAN groups points based on density and points in low-density regions are labelled as outliers. Z-score is more reliable because it is based on global rather than local relationships. 3 challenges to data mining are data quality and preprocessing, computational complexity, and complex data visualization. Large amounts of data may be difficult to mine if inefficient algorithms are used, as well as potential needs for cluster or distributed computing which significantly increases complexity and overhead. <br>The major research challenge in data mining, specifically in the sensor/signal analysis domain is the wide <a href=\"https://emujakic.github.io/TechKB/notes/math/range.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"107\" to=\"112\" origin-text=\"range\" class=\"internal-link virtual-link-a\">range</a> of potential domain value mappings. <br><a href=\"https://emujakic.github.io/TechKB/notes/math/variance.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"0\" to=\"8\" origin-text=\"Variance\" class=\"internal-link virtual-link-a\">Variance</a> is the average of the square differences of the <a href=\"https://emujakic.github.io/TechKB/notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"57\" to=\"61\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a>, or the square of the <a data-href=\"Standard Deviation\" href=\"https://emujakic.github.io/TechKB/notes/math/standard-deviation.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Standard Deviation</a>.\nthe Mean Absolute Deviation is the average of the absolute differences between each data point and the mean. The coefficient of variation is expressed as a percentage of the mean: <br>\n<a data-href=\"Mean\" href=\"https://emujakic.github.io/TechKB/notes/math/mean.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Mean</a> - 25.37 <br>\n<a data-href=\"Median\" href=\"https://emujakic.github.io/TechKB/notes/math/median.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Median</a> - 14 <br>\n<a data-href=\"Mode\" href=\"https://emujakic.github.io/TechKB/notes/math/mode.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Mode</a> - 35 &amp; 25, the data is bimodal since there are 2 modes <br>\n<a data-href=\"Midrange\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Midrange</a> - Q1 = 20 Q3 = 35 five-number summary is 13, 20, 25, 35, 70 <br>\nA <a data-href=\"Quantile\" href=\"https://emujakic.github.io/TechKB/notes/math/quantile.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Quantile</a> plot compares the <a href=\"https://emujakic.github.io/TechKB/notes/math/quantile.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"19\" to=\"28\" origin-text=\"quantiles\" class=\"internal-link virtual-link-a\">quantiles</a> of a dataset against the quantiles of a theoretical distribution, such as a normal distribution. A <a data-href=\"Quantile-Quantile Plot\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Quantile-Quantile Plot</a> plot compares the quantiles of 2 datasets against one another. <br>\nTo compute an approximate <a href=\"https://emujakic.github.io/TechKB/notes/math/median.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"26\" to=\"32\" origin-text=\"median\" class=\"internal-link virtual-link-a\">median</a>: Calculate the cumulative frequency\nDetermine the median class\n<br>Use the <a data-href=\"Median Formula\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Median Formula</a> The median class is 21-50 where L is the lower boundary of the median class, N is the total frequency, CF is the cumulative frequency of the class before the median class, f is the frequency of the median class, and c is the class width. <br>\n<a href=\"https://emujakic.github.io/TechKB/notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"0\" to=\"4\" origin-text=\"Mean\" class=\"internal-link virtual-link-a\">Mean</a> (age)- 46.44 <br>\n<a href=\"https://emujakic.github.io/TechKB/notes/math/median.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"0\" to=\"6\" origin-text=\"Median\" class=\"internal-link virtual-link-a\">Median</a> (age)- 51 <br><a href=\"https://emujakic.github.io/TechKB/notes/math/nominal-data.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"0\" to=\"7\" origin-text=\"Nominal\" class=\"internal-link virtual-link-a\">Nominal</a> dissimilarity is where i and j are 2 objects, p is the total number of attributes, and m is the number of matching values.\nasymmetric binary dissimilarity is where r is the number of attributes that are 1 for i and 0 for j, s is the number of attributes that are 0 for i and 1 for j, and q is the number of attributes that are 1 for both i and j.\n<br><a href=\"https://emujakic.github.io/TechKB/notes/math/euclidean-distance.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"0\" to=\"18\" origin-text=\"Euclidean distance\" class=\"internal-link virtual-link-a\">Euclidean distance</a> for numeric dissimilarity is Dissimilarity for term-frequency vectors is Euclidean - 6.71\nManhattan - 11\nMinkowski - 233 (q = 3)\nSupremum - 6 X X (a) Use smoothing by bin means to smooth these data, using a bin depth of 3. Illustrate your steps. Comment on the effect of this technique for the given data. <br>\n(b) How might you determine <a href=\"https://emujakic.github.io/TechKB/notes/math/outlier.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"28\" to=\"36\" origin-text=\"outliers\" class=\"internal-link virtual-link-a\">outliers</a> in the data? (c) What other methods are there for data smoothing? X X (a) min-max normalization (b) z-score normalization <br>\n(c) z-score normalization using the <a data-tooltip-position=\"top\" aria-label=\"Mean\" data-href=\"Mean\" href=\"https://emujakic.github.io/TechKB/notes/math/mean.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">mean</a> absolute deviation instead of <a data-tooltip-position=\"top\" aria-label=\"Standard Deviation\" data-href=\"Standard Deviation\" href=\"https://emujakic.github.io/TechKB/notes/math/standard-deviation.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">standard deviation</a> (d) normalization by decimal scaling X (a) min-max normalization by setting min = 0 and max = 1 (b) z-score normalization <br>\n(c) z-score normalization using the <a href=\"https://emujakic.github.io/TechKB/notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"36\" to=\"40\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a> absolute deviation instead of <a href=\"https://emujakic.github.io/TechKB/notes/math/standard-deviation.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"71\" to=\"89\" origin-text=\"standard deviation\" class=\"internal-link virtual-link-a\">standard deviation</a> (d) normalization by decimal scaling X <br>\n(a) Use min-max normalization to transform the value 35 for age onto the <a href=\"https://emujakic.github.io/TechKB/notes/math/range.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"73\" to=\"78\" origin-text=\"range\" class=\"internal-link virtual-link-a\">range</a> (0.0,1.0). (b) Use z-score normalization to transform the value 35 for age, where the standard deviation of age is 12.94 years. (c) Use normalization by decimal scaling to transform the value 35 for age. (d) Comment on which method you would prefer to use for the given data, giving reasons as to why. X X ","aliases":[],"inlineTags":[],"frontmatterTags":["#AI","#statistics","#data","#machineLearning","#practice"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Data Mining: Concepts and Techniques Practice Problems","level":1,"id":"Data_Mining_Concepts_and_Techniques_Practice_Problems_0"},{"heading":"Problems","level":2,"id":"Problems_0"},{"heading":"Ch.1 - Introduction","level":2,"id":"Ch.1_-_Introduction_0"},{"heading":"1.1 What is data mining?","level":4,"id":"1.1_What_is_data_mining?_0"},{"heading":"1.2 How is a data warehouse different from a database? How are they similar?","level":4,"id":"1.2_How_is_a_data_warehouse_different_from_a_database?_How_are_they_similar?_0"},{"heading":"1.3 Define each of the following data mining functionalities: characterization, discrimination, association and correlation analysis, classification, regression, clustering, and <span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Outlier.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"178\" to=\"185\" origin-text=\"outlier\" class=\"internal-link virtual-link-a\">outlier</a></span> analysis.","level":4,"id":"1.3_Define_each_of_the_following_data_mining_functionalities_characterization,_discrimination,_association_and_correlation_analysis,_classification,_regression,_clustering,_and_outlier_analysis._0"},{"heading":"1.4 Present an example where data mining is crucial to the success of a business.","level":4,"id":"1.4_Present_an_example_where_data_mining_is_crucial_to_the_success_of_a_business._0"},{"heading":"1.5 Explain the difference and similarity between discrimination and classification, between characterization and clustering, and between classification and regression.","level":4,"id":"1.5_Explain_the_difference_and_similarity_between_discrimination_and_classification,_between_characterization_and_clustering,_and_between_classification_and_regression._0"},{"heading":"1.6 Based on your observations, describe another possible kind of knowledge that needs to be discovered by data mining methods but has not been listed in this chapter. Does it require a mining methodology that is quite different from those outlined in this chapter?","level":4,"id":"1.6_Based_on_your_observations,_describe_another_possible_kind_of_knowledge_that_needs_to_be_discovered_by_data_mining_methods_but_has_not_been_listed_in_this_chapter._Does_it_require_a_mining_methodology_that_is_quite_different_from_those_outlined_in_this_chapter?_0"},{"heading":"1.7 Using fraudulence detection as an example, propose two methods that can be used to detect <span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Outlier.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"94\" to=\"102\" origin-text=\"outliers\" class=\"internal-link virtual-link-a\">outliers</a></span> and discuss which one is more reliable.","level":4,"id":"1.7_Using_fraudulence_detection_as_an_example,_propose_two_methods_that_can_be_used_to_detect_outliers_and_discuss_which_one_is_more_reliable._0"},{"heading":"1.8 Describe three challenges to data mining regarding data mining methodology and user interaction issues.","level":4,"id":"1.8_Describe_three_challenges_to_data_mining_regarding_data_mining_methodology_and_user_interaction_issues._0"},{"heading":"1.9 What are the major challenges of mining a huge amount of data (e.g., billions of tuples) in comparison with mining a small amount of data (e.g., data set of a few hundred tuple)?","level":4,"id":"1.9_What_are_the_major_challenges_of_mining_a_huge_amount_of_data_(e.g.,_billions_of_tuples)_in_comparison_with_mining_a_small_amount_of_data_(e.g.,_data_set_of_a_few_hundred_tuple)?_0"},{"heading":"Outline the major research challenges of data mining in one specific application domain, such as stream/sensor data analysis, spatiotemporal data analysis, or bioinformatics.","level":4,"id":"Outline_the_major_research_challenges_of_data_mining_in_one_specific_application_domain,_such_as_stream/sensor_data_analysis,_spatiotemporal_data_analysis,_or_bioinformatics._0"},{"heading":"Ch.2 - Getting to Know Your Data","level":2,"id":"Ch.2_-_Getting_to_Know_Your_Data_0"},{"heading":"2.1 Give three additional commonly used statistical measures that are not already illustrated in this chapter for the characterization of data dispersion. Discuss how they can be computed efficiently in large databases.****","level":4,"id":"2.1_Give_three_additional_commonly_used_statistical_measures_that_are_not_already_illustrated_in_this_chapter_for_the_characterization_of_data_dispersion._Discuss_how_they_can_be_computed_efficiently_in_large_databases.****_0"},{"heading":"2.2 Suppose that the data for analysis includes the attribute age. The age values for the data tuples are (in increasing order) 13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70.","level":4,"id":"2.2_Suppose_that_the_data_for_analysis_includes_the_attribute_age._The_age_values_for_the_data_tuples_are_(in_increasing_order)_13,_15,_16,_16,_19,_20,_20,_21,_22,_22,_25,_25,_25,_25,_30,_33,_33,_35,_35,_35,_35,_36,_40,_45,_46,_52,_70._0"},{"heading":"(a) What is the <span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Mean.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"16\" to=\"20\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a></span> of the data? What is the <span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Median.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"46\" to=\"52\" origin-text=\"median\" class=\"internal-link virtual-link-a\">median</a></span>?","level":4,"id":"(a)_What_is_the_mean_of_the_data?_What_is_the_median?_0"},{"heading":"(b) What is the <span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Mode.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"16\" to=\"20\" origin-text=\"mode\" class=\"internal-link virtual-link-a\">mode</a></span> of the data? Comment on the data’s modality (i.e., bimodal, trimodal, etc.).","level":4,"id":"(b)_What_is_the_mode_of_the_data?_Comment_on_the_data’s_modality_(i.e.,_bimodal,_trimodal,_etc.)._0"},{"heading":"(c) What is the midrange of the data?","level":4,"id":"(c)_What_is_the_midrange_of_the_data?_0"},{"heading":"(d) Can you find (roughly) the first <span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Quartile.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"37\" to=\"45\" origin-text=\"quartile\" class=\"internal-link virtual-link-a\">quartile</a></span> (Q1) and the third quartile (Q3) of the data? (e) Give the five-number summary of the data.","level":4,"id":"(d)_Can_you_find_(roughly)_the_first_quartile_(Q1)_and_the_third_quartile_(Q3)_of_the_data?_(e)_Give_the_five-number_summary_of_the_data._0"},{"heading":"(f) Show a boxplot of the data.","level":4,"id":"(f)_Show_a_boxplot_of_the_data._0"},{"heading":"(g) How is a <span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Quantile.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"13\" to=\"21\" origin-text=\"quantile\" class=\"internal-link virtual-link-a\">quantile</a></span>–quantile plot different from a quantile plot?","level":4,"id":"(g)_How_is_a_quantile–quantile_plot_different_from_a_quantile_plot?_0"},{"heading":"2.3 Suppose that the values for a given set of data are grouped into intervals. The intervals and corresponding frequencies are as follows:","level":4,"id":"2.3_Suppose_that_the_values_for_a_given_set_of_data_are_grouped_into_intervals._The_intervals_and_corresponding_frequencies_are_as_follows_0"},{"heading":"Compute an approximate <span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Median.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"23\" to=\"29\" origin-text=\"median\" class=\"internal-link virtual-link-a\">median</a></span> value for the data.","level":4,"id":"Compute_an_approximate_median_value_for_the_data._0"},{"heading":"Suppose that a hospital tested the age and body fat data for 18 randomly selected adults with the following results:","level":4,"id":"Suppose_that_a_hospital_tested_the_age_and_body_fat_data_for_18_randomly_selected_adults_with_the_following_results_0"},{"heading":"(a) Calculate the <span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Mean.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"18\" to=\"22\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a></span>, <span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Median.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"24\" to=\"30\" origin-text=\"median\" class=\"internal-link virtual-link-a\">median</a></span>, and <span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Standard Deviation.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"36\" to=\"54\" origin-text=\"standard deviation\" class=\"internal-link virtual-link-a\">standard deviation</a></span> of age and %fat.","level":4,"id":"(a)_Calculate_the_mean,_median,_and_standard_deviation_of_age_and_%fat._0"},{"heading":"(b) Draw the boxplots for age and %fat.","level":4,"id":"(b)_Draw_the_boxplots_for_age_and_%fat._0"},{"heading":"(c) Draw a scatter plot and a q-q plot based on these two variables","level":4,"id":"(c)_Draw_a_scatter_plot_and_a_q-q_plot_based_on_these_two_variables_0"},{"heading":"2.5 Briefly outline how to compute the dissimilarity between objects described by the following","level":4,"id":"2.5_Briefly_outline_how_to_compute_the_dissimilarity_between_objects_described_by_the_following_0"},{"heading":"(a) <span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Nominal Data.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"4\" to=\"11\" origin-text=\"Nominal\" class=\"internal-link virtual-link-a\">Nominal</a></span> attributes","level":4,"id":"(a)_Nominal_attributes_0"},{"heading":"(b) Asymmetric binary attributes","level":4,"id":"(b)_Asymmetric_binary_attributes_0"},{"heading":"(c) Numeric attributes","level":4,"id":"(c)_Numeric_attributes_0"},{"heading":"(d) Term-frequency vectors","level":4,"id":"(d)_Term-frequency_vectors_0"},{"heading":"2.6 Given two objects represented by the tuples (22, 1, 42, 10) and (20, 0, 36, 8): (a) Compute the <span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Euclidean Distance.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"100\" to=\"118\" origin-text=\"Euclidean distance\" class=\"internal-link virtual-link-a\">Euclidean distance</a></span> between the two objects. (b) Compute the <span class=\"glossary-entry virtual-link virtual-link-span virtual-link-default\"><a href=\"NOTES/Math/Manhattan Distance.md\" target=\"_blank\" rel=\"noopener noreferrer\" from=\"160\" to=\"178\" origin-text=\"Manhattan distance\" class=\"internal-link virtual-link-a\">Manhattan distance</a></span> between the two objects. (c) Compute the Minkowski distance between the two objects, using q = 3. (d) Compute the supremum distance between the two objects.","level":4,"id":"2.6_Given_two_objects_represented_by_the_tuples_(22,_1,_42,_10)_and_(20,_0,_36,_8)_(a)_Compute_the_Euclidean_distance_between_the_two_objects._(b)_Compute_the_Manhattan_distance_between_the_two_objects._(c)_Compute_the_Minkowski_distance_between_the_two_objects,_using_q_=_3._(d)_Compute_the_supremum_distance_between_the_two_objects._0"},{"heading":"Ch.3 - Data Preprocessing","level":2,"id":"Ch.3_-_Data_Preprocessing_0"},{"heading":"3.1 Data quality can be assessed in terms of several issues, including accuracy, completeness, and consistency. For each of the above three issues, discuss how data quality assessment can depend on the intended use of the data, giving examples. Propose two other dimensions of data quality.","level":4,"id":"3.1_Data_quality_can_be_assessed_in_terms_of_several_issues,_including_accuracy,_completeness,_and_consistency._For_each_of_the_above_three_issues,_discuss_how_data_quality_assessment_can_depend_on_the_intended_use_of_the_data,_giving_examples._Propose_two_other_dimensions_of_data_quality._0"},{"heading":"3.2 In real-world data, tuples with missing values for some attributes are a common occurrence. Describe various methods for handling this problem.","level":4,"id":"3.2_In_real-world_data,_tuples_with_missing_values_for_some_attributes_are_a_common_occurrence._Describe_various_methods_for_handling_this_problem._0"},{"heading":"3.3 Exercise 2.2 gave the following data (in increasing order) for the attribute age: 13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70.","level":4,"id":"3.3_Exercise_2.2_gave_the_following_data_(in_increasing_order)_for_the_attribute_age_13,_15,_16,_16,_19,_20,_20,_21,_22,_22,_25,_25,_25,_25,_30,_33,_33,_35,_35,_35,_35,_36,_40,_45,_46,_52,_70._0"},{"heading":"3.4 Discuss issues to consider during data integration.","level":4,"id":"3.4_Discuss_issues_to_consider_during_data_integration._0"},{"heading":"3.5 What are the value ranges of the following normalization methods?","level":4,"id":"3.5_What_are_the_value_ranges_of_the_following_normalization_methods?_0"},{"heading":"Use these methods to normalize the following group of data: 200,300,400,600,1000.","level":4,"id":"Use_these_methods_to_normalize_the_following_group_of_data_200,300,400,600,1000._0"},{"heading":"3.7 Using the data for age given in Exercise 3.3, answer the following:","level":4,"id":"3.7_Using_the_data_for_age_given_in_Exercise_3.3,_answer_the_following_0"},{"heading":"Question","level":4,"id":"Question_0"}],"links":[".html",".html",".html","notes/math/outlier.html#_0",".html","notes/math/standard-deviation.html#_0","notes/math/mean.html#_0","notes/math/range.html#_0","notes/math/variance.html#_0","notes/math/mean.html#_0","notes/math/standard-deviation.html#_0","notes/math/median.html#_0","notes/math/mode.html#_0","notes/math/quartile.html#_0","notes/math/mean.html#_0","notes/math/median.html#_0","notes/math/mode.html#_0",".html","notes/math/quantile.html#_0","notes/math/quantile.html#_0",".html","notes/math/median.html#_0","notes/math/median.html#_0",".html","notes/math/mean.html#_0","notes/math/median.html#_0","notes/math/standard-deviation.html#_0","notes/math/mean.html#_0","notes/math/median.html#_0","notes/math/nominal-data.html#_0","notes/math/euclidean-distance.html#_0","notes/math/manhattan-distance.html#_0","notes/math/outlier.html#_0","notes/math/mean.html#_0","notes/math/standard-deviation.html#_0","notes/math/mean.html#_0","notes/math/standard-deviation.html#_0","notes/math/range.html#_0"],"author":"Ernad Mujakic","coverImageURL":"","fullURL":"https://emujakic.github.io/TechKB/textbooks/data-mining-concepts-and-techniques/practice-problems.html","pathToRoot":"../..","attachments":[],"createdTime":1751221818773,"modifiedTime":1754341489535,"sourceSize":11343,"sourcePath":"TEXTBOOKS/Data Mining-Concepts and Techniques/Practice Problems.md","exportPath":"textbooks/data-mining-concepts-and-techniques/practice-problems.html","showInTree":true,"treeOrder":30,"backlinks":[],"type":"markdown"},"textbooks/ai-a-modern-approach/summary.html":{"title":"Summary","icon":"","description":"\nName: Stuart Russell and Peter Norvig\nEdition: 4th Edition\nAI: A Modern Approach by Norvig and Russel is a foundational text in the dynamic field of artificial intelligence. The book covers a wide <a href=\"https://emujakic.github.io/TechKB/notes/math/range.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"117\" to=\"122\" origin-text=\"range\" class=\"internal-link virtual-link-a\">range</a> of topics, from Markov models and evolutionary algorithms, to natural language processing. This summary attempts to breaks down the main ideas and insights from the book. The field of AI is concerned with the understanding, engineering, and implementation of intelligent agents. Intelligence can be defined through several perspectives: <br>Acting Humanly- The ability to pass a <a data-href=\"Turing Test\" href=\"https://emujakic.github.io/TechKB/notes/ai/turing-test.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Turing Test</a>.\nThinking Humanly- Thinking like a human.\nThinking Rationally- Acting in such a way as to achieve the 'best' possible outcome.\nBeneficial Machines- Acting in a way beneficial to humans. <br>\n<a data-href=\"Utility\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Utility</a> is the subjective value of an outcome. <br>\n<a data-href=\"First-Order Logic\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">First-Order Logic</a> is a branch of logic made up of predicates that return either true or false, universal/existential quantifiers, terms, and logical connectives such as 'and' 'or' 'not'. <br>\n<a data-href=\"Probability\" href=\"https://emujakic.github.io/TechKB/notes/math/probability.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Probability</a> extends logic to scenarios involving uncertainty, allowing for the modeling of real-world conditions where information is incomplete or ambiguous. <br>\n<a data-href=\"Decision Theory\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Decision Theory</a> deals with the principles and methods for making rational decisions under uncertainty. <br>\n<a data-href=\"Game Theory\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Game Theory</a> is a framework for analyzing interdependent multi-agent environments. <br>\n<a data-href=\"Control Theory\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Control Theory</a> is the design of systems that can automatically adjust their behavior to achieve desired outcomes. A cost function quantifies the cost of a particular action or sequence of actions. Something is 'stochastic' if it exhibits uncertainty. <br>\nA <a data-href=\"Markov Model\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Markov Model</a> is a system where the following state depends only on the current state. A Markov Process is a stochastic process that satisfies the Markov Model. A <a data-href=\"Hidden Markov Model\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Hidden Markov Model</a> is a statistical model that represents systems that transitions between different hidden states, while perceiving observable outputs. <br>\nA <a data-href=\"Bayesian Network\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Bayesian Network</a> is a probabilistic model implemented as a directed acyclic graph that represents a set of <a data-tooltip-position=\"top\" aria-label=\"Random Variable\" data-href=\"Random Variable\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Random Variables</a> and their conditional dependencies, as well as a set of conditional <a href=\"https://emujakic.github.io/TechKB/notes/math/probability.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"69\" to=\"80\" origin-text=\"probability\" class=\"internal-link virtual-link-a\">probability</a> distribution tables. <br>\n<a data-href=\"Agent\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Agent</a> - Anything that can perceive its environment with sensors and act upon it with actuators. <br>\n<a data-href=\"Percept Sequence\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Percept Sequence</a> - Everything an agent has perceived. <br>\n<a data-href=\"Agent Function\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Agent Function</a> - An abstract mathematical function that maps percept sequences to actions. Agent Program - A concrete implementation of some abstract agent function. Performance Measure - Evaluates the desirability of an outcome. Information Gathering - Performing potentially sub-optimal actions in order the perceive new information about the environment. Rationality maximizes expected performance based on the knowledge available. What is rational at a given time-step is based on the agent's performance measure, its available actions, its prior knowledge of the environment, as well as its percept sequence up until that time-step. <br>\n<a data-href=\"Task Environment\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Task Environment</a> - The factors that determine how an agent operates. The task environment consists of the performance measure, sensors, actuators, and external environment. A task environment can be fully observable, meaning the agent's sensors give it complete knowledge about all relevant variables of the environment at all times. A Task environment is partially observable if the sensors provide a noisy or incomplete description of the actual state at any time step. A task environment is single agent if there is only one actor that influences the environment, else, it is a multi-agent environment. <br>\nA task environment is deterministic if the next state is completely determined by the current state and the action taken. Otherwise, the environment is nondeterministic or stochastic. An environment is uncertain if there is uncertainty in either the <a data-href=\"Sensor Model\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Sensor Model</a> - which returns the perceived output of the current state, or if there is uncertainty in the <a data-href=\"Transition Model\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Transition Model</a> - which returns the next state based on the current state and the action taken. Together the transition and sensor model allow the agent to keep track of the state of the world . A task environment is episodic if the sequence of states are divided into atomic episodes, that is, the next state (episode) does not depend on the actions from previous states. A task environment is sequential if actions at any given state can impact the actions taken in future states. A task environment is dynamic if it can change while the agent is deliberating, otherwise, the environment is static. If the environment itself doesn't change but the passage of time impacts the agent's performance measure, the environment is considered semi-dynamic. <br>\nAn environment is known if the outcomes (or <a data-tooltip-position=\"top\" aria-label=\"Probability\" data-href=\"Probability\" href=\"https://emujakic.github.io/TechKB/notes/math/probability.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">probabilities</a> of outcomes) of actions are given. Otherwise, the environment is unknown and the agent will have to learn the result of its actions. Discrete variables can take on distinct, separate values. <br>\nContinuous variables can take on any value in a <a href=\"https://emujakic.github.io/TechKB/notes/math/range.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"28\" to=\"33\" origin-text=\"range\" class=\"internal-link virtual-link-a\">range</a>. Simple Reflex Agent - An agent that selects an action that selects based only on the current percept. Is susceptible to infinite loops in partially-observable environments which can be avoided if the agent selects actions with an element of randomization. Model-Based Reflex Agent - An agent which maintains an internal state which is constructed based on the percept history. The agent's internal state keeps track of the result of actions, as well as how the world evolves independent of itself. Goal-Based Agent - An agent that maintains a current state description as well as goal information which determines the desirability of a state or action. Though, goals treat all non-goal states with equal desirability which leads to poor performance. <br>\nUtility-Based Agent - Instead of a goal, the agent has a <a data-href=\"Utility Function\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Utility Function</a> which assigns a continuous value to a state or action, allowing the agent to act in such a way that maximizes its utility function. The utility function is simply and internalization of the agent's performance measure, in the case where all an agent's current actions lead to non-goal states, a utility-based agent will choose the action that leads to the \"best\" state, rather than treating each action as equal like a goal-based agent would. Utility also allows an agent to rationally deal with multiple goals by selecting the goal with the maximum utility. <br>\n<a data-href=\"Learning Agent\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Learning Agent</a> - An agent that can learn and improve from experience or training. A learning agent can be of any of the mentioned types. Learning agents typically consist of four main components: Learning Element - The element responsible for making improvements.\nPerformance Element - The element responsible for selecting external actions. The performance element is what we previously considered as the entire agent.\nCritic - The element which provides feedback to the learning element on how well the agent is performing. Problem-solving agent: an agent that needs to plan, and consider a sequence of actions to reach a goal. Search: the computational process a problem-solving agent undertakes. <br>\nAn algorithm is informed if the agent can estimate its proximity to the goal using a <a data-href=\"Heuristic\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Heuristic</a>. Otherwise, the algorithm is uninformed. A problem solving agent: Formulates its goal\nDevises a description of the states and actions needed to reach its goal.\nSimulates a sequence of actions until it finds a solution.\nExecutes the solution, if one is found. An system is considered open-loop if it ignores its percepts while executing it's solution. This is done so if the environment is known and deterministic, meaning the agent needs not to keep track of its percepts. <br>A system is closed-loop if it keeps track of its percepts during execution. The term closed loop is used to <a href=\"https://emujakic.github.io/TechKB/notes/math/mean.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"85\" to=\"89\" origin-text=\"mean\" class=\"internal-link virtual-link-a\">mean</a> that the loop between the agent and its environment is not broken. A search problem is defined by the following five components: <br>The set of possible states (<a data-href=\"State Space\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">State Space</a>).\nThe initial state.\nA set of one or more goals states.\nThe set of actions available to the agent.\n<br>The <a data-href=\"Transition Model\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Transition Model</a>, which describes the result of the agent taking a action in its current state. A sequence of actions forms a path within the state space, and a path is considered a solution if it reaches a goal state from the initial state. A solution is considered optimal if it has the lowest cost among all possible solutions. <br>\nThe state space can be represented as a <a data-href=\"Graph\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Graph</a> where the vertices represent states, and edges represent actions which lead to other states. The diameter of a graph is the greatest distance between any 2 nodes. A state can be abstracted by removing as much detail as possible, while still retaining validity. <br>\n<a data-href=\"Search Algorithm\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Search Algorithm</a>: Takes a search problem and returns a solution or failure. There are four ways to evaluate the performance of a search algorithm: Completeness: Does the algorithm always find a solution if one exists?\nOptimality: Does the algorithm find the optimal solution?\n<br><a data-href=\"Time Complexity\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Time Complexity</a>: How does the search time scale as the size of the problem scales?\n<br><a data-href=\"Space Complexity\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Space Complexity</a>: How does the memory usage scale as the size of the problem scales? In bounded suboptimal search we look for a solution within some constant factor of the optimal solution. In bounded-cost search we look for a solution who's cost is less than some constant. In unbounded-cost search we accept any solution and prioritize the speed of the algorithm. For a search algorithm to be complete in an infinite state space, it needs to systematically explore the space. <br>\n<a data-href=\"Search Tree\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Search Tree</a>: Describes the path between states. The root node represents the initial state, and the current node can be expanded by considering the actions available, then generating a new child node for each resulting state. <br>\nFrontier: The collection of nodes that have been generated but not yet explored and is typically implemented as a <a data-href=\"Queue\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Queue</a> or a <a data-href=\"Stack\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Stack</a>. The functions available on the frontier are: is-Empty, which checks if the frontier is empty.\npop, which returns and removes the first element.\ntop, which returns the first element but does not remove it.\nadd, which adds a new element to the end of the queue. A best-first search, also known as a greedy search, chooses the node with the lowest cost in the frontier. A node is made up of the following four components: The state that it represents.\nIts parent node, which allows us to backtrack from a goal state back to the initial state.\nThe action which generated it.\nIts total path cost from the root. An uninformed search algorithm is completely unaware of how far any state is from the goal. Uninformed algorithms are differ in the way that they expand nodes in the frontier. Some common uninformed search algorithms include: <br><a data-href=\"Breadth-First Search\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Breadth-First Search</a>: Expands all the nodes in the current depth, starting at the root, before moving deeper into the tree. This search in implemented using a queue, which is a FIFO data structure. BFS is a systematic search, meaning, it is complete even in infinite state spaces.\nBFS is cost optimal if the edges are unweighted or have a global constant weight.\nThe goal test may be early, meaning each node is checked to be a goal when it is first generated; or the goal test may be late, meaning each node is checked when it is popped from the queue.\nThe time and space complexity of BFS is where is the number of children and is the depth of the tree. <br><a data-href=\"Dijkstra's Algorithm\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Dijkstra's Algorithm</a>: is a greedy search algorithm that uses a late goal test to ensure optimality. It maintains a list of the shortest distances to each node from the root where, initially, the distance to the root is 0 and the distance to all other nodes is . The frontier is implemented as a priority queue, where the node with the smallest distance is expanded first. Then all of the current node's neighbors are examined and if a shorter distance is found, the distance to the corresponding node is updated. This process is iterated until the optimal path is found. Dijkstra's is complete and optimal for weighted graphs with non-negative weights.\nThe time complexity of Dijkstra's is where is the number of vertices, and is the number of edges. The space complexity of Dijkstra's is . <br><a data-href=\"Depth-First Search\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Depth-First Search</a>: Expands the deepest node in the frontier. DFS uses a stack as opposed to a queue, which is a LIFO data structure. This algorithm is commonly implemented as a tree search, meaning, it doesn't keep track of the states that it has reached. DFS continues down a path until either a goal is found, or a leaf node is found. In the latter case, the algorithm backtracks to the next deepest node and expands its children if any. DFS is not cost-optimal. DFS is complete in finite state spaces though not in infinite state spaces.\nThe time complexity of DFS is and the space complexity is .\n<br><a data-href=\"Backtracking Search\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Backtracking Search</a> is a variation of DFS where only one successor is generated at a time, and the current state description is updated in place, rather than allocating memory for a new state. <br><a data-href=\"Depth-Limited Search\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Depth-Limited Search</a>: Treats nodes at depth as if they are leaf nodes. This prevents the node from traveling down an infinite path. <br>The time complexity of depth-limited search is where is the <a data-href=\"Branching Factor\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Branching Factor</a> and is the depth limit.\nThe diameter of the state space is a good limit value, though, it is rarely known beforehand. <br><a data-href=\"Iterative Deepening DFS\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Iterative Deepening DFS</a>: A depth limited search which iteratively increments the depth limit until a solution is found or a failure is returned. This combines the benefits of depth first and breadth first search. Optimal for unweighted paths.\nComplete for finite, acyclic state spaces.\nTime complexity of <br><a data-href=\"Bidirectional Search\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Bidirectional Search</a>: Searches forward from the initial state and backward from the goal state until the 2 meet. The goal test returns true when the two paths meet. Since 2 frontiers need to be tracked, the space requirements are higher than most of the previously discussed algorithms. An informed (heuristic) search algorithm uses a heuristic function to estimate the distance of any given node from the goal. A search is considered greedy if it expands the node with the lowest always. Greedy searches are complete on finite spaces, but not on infinite spaces.\n<br><a data-href=\"A* Search\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">A* Search</a>: The evaluation function, , is the path cost from the current state to node plus . A* is complete and optimal if the heuristic is admissible or optimistic - meaning it never overestimates the cost to the goal.\nA* prunes nodes that are unnecessary for finding a solution.\nIf we allow A* to use an inadmissible heuristic, there is a risk of finding a suboptimal solution, but the heuristic may be more accurate, thus, reducing the number of expanded nodes.\nA variant of A, called **weighted A search** emphasizes the heuristic by multiplying by some constant . Weighted A finds a solution somewhere between and . A heuristic is consistent if for every node , its successor : , where represents the cost from to .\n<br>This means that the cost of reaching the goal from node is never greater than the cost of getting to from , plus the estimated cost of reaching the goal from . This is a version of the <a data-href=\"Triangle Inequality\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Triangle Inequality</a>.\nEvery consistent heuristic is admissible, but not every admissible heuristic is consistent.\nA composite heuristic combines multiple heuristics, and for certain complex problems, can be more effective than a single heuristic. A node is surely expanded if it can be reached from the initial state on a path where every node has , where is the optimal cost.\n<br><a data-href=\"Beam Search\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Beam Search</a>: Keeps track of only the nodes with the best f-scores. Is incomplete and suboptimal, but very fast and memory efficient. <br><a data-href=\"Iterative Deepening A*\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Iterative Deepening A*</a>: Combines the advantages of A* and iterative deepening DFS, where, at each iteration, the cutoff values is the smallest f-score of any node that exceeds the cutoff of the previous iteration. The initial cutoff is the heuristic value of the root node. <br>Can be visualized using <a data-href=\"Search Contours\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Search Contours</a>. <br><a data-href=\"Recursive Best-First Search\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Recursive Best-First Search</a>: Uses a f-limit variable to keep track of of the f-value of the best alternative path from any ancestor of the current node. If the current node exceeds that f-limit, the recursion unwinds to the alternative path. Is optimal if the heuristic is admissible. Both RBFS and IDA use too little* memory, meaning they forget most of what they've done.\n<br><a data-href=\"Memory-Bounded A*\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Memory-Bounded A*</a>: And <a data-href=\"Simplified Memory-Bounded A*\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Simplified Memory-Bounded A*</a> uses all of the memory that is allocated to it. SMA proceeds like A, expanding the node with the best f-score until memory is full. In that case, it drops the node with the highest f-score and passes its value to the parent, similar to RBFS. If all nodes have the same f-score, the oldest node is deleted and the newest is expanded.\nIs complete and optimal if the optimal solution can fit in memory. <br>\nOne way to analyze the quality of a heuristic is the <a data-href=\"Effective Branching Factor\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Effective Branching Factor</a> . <br>If is the total number of nodes generated by A and the solution depth is , $b^$ is the branching factor a <a data-href=\"Uniform Tree\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Uniform Tree</a> of depth must have to contain nodes. A well designed heuristic should have a branching factor close to 1. <br>\n<a data-href=\"Relaxed Problems\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Relaxed Problems</a>: problems with fewer restrictions on available actions. For example, allowing illegal moves in the <a data-href=\"N-Queens Problem\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">N-Queens Problem</a>. The cost of an optimal solution in a relaxed problem is an admissible and consistent heuristic for the original problem.\nAdmissible heuristics can also be derived from subproblems of the original problem where the optimal cost in the subproblem is a lower bound on the cost of the complete problem. <br>\n<a data-href=\"Pattern Database\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Pattern Database</a>: Stores the exact solution costs for possible subproblem instances. Commonly used to store <a data-href=\"Endgame Tablebase\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Endgame Tablebase</a>s for an AI <a href=\"https://emujakic.github.io/TechKB/projects/ai-chess-robot/chess-engine.html\" target=\"_self\" rel=\"noopener noreferrer\" from=\"12\" to=\"24\" origin-text=\"chess engine\" class=\"internal-link virtual-link-a\">chess engine</a>, for example. When searching for a solution, the agent can use the pattern database to quickly estimate the cost for a given state.\nDisjoint pattern databases ignore rather than abstract the rest of the problem. Pre-computation stores the optimal path between pairs of vertices. Landmark points can be precomputed and used as an efficient but inadmissible heuristic. <br>\n<a data-href=\"Metalevel State Space\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Metalevel State Space</a>: Allows an agent to reason about its own reasoning process by evaluating potential strategies and their outcomes. Metalevel state spaces allow an agent to learn from experience and dynamically adjust their strategy based on observed output. Local search algorithms search the state space without keeping track of a path or previously reached states. These algorithms are not systematic, though they use very little memory, and usually have constant space complexity.\nLocal search is good for problems where only the final state is desired and not so much the path to get there. The most common application of local search is solving optimization problems, where the goal is to find the best state according to some objective function. Consider the states of a problem laid out in a state-space landscape—a line chart where each state has an elevation defined by the objective function. <br>If the goal is to find the highest peak, then the problem is called <a data-href=\"Hill Climbing\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Hill Climbing</a>.\n<br>If the goal is to find the lowest valley, the problem is called <a data-href=\"Gradient Descent\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Gradient Descent</a>. Hill Climbing: An optimization algorithm which keeps track of the current state, and on each iteration, moves to the neighboring state which provides the steepest ascent, that is, it moves uphill. Hill climbing terminates when it reaches a peak, or local maximum, a point where no neighboring state has a higher value.\nSince it doesn't look ahead past it's immediate neighbors, nor does it consider more than the next move, hill climbing is considered a greedy local search.\nHill climbing is also susceptible to plateaus, a flat region of the state space landscape. A plateau is a shoulder if it immediately neighbors a better state. <br>\n<a data-href=\"Stochastic Hill Climbing\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Stochastic Hill Climbing</a>: Chooses randomly from the set of available uphill moves. Typically, the <a data-tooltip-position=\"top\" aria-label=\"Probability\" data-href=\"Probability\" href=\"https://emujakic.github.io/TechKB/notes/math/probability.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">probability</a> of a move being selected is linear to the steepness of the move. <br>\n<a data-href=\"First-Choice Hill Climbing\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">First-Choice Hill Climbing</a>: Randomly generates successors until a state better than the current one is generated. <br>\n<a data-href=\"Random-Restart Hill Climbing\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Random-Restart Hill Climbing</a>: Conducts a series of searches from random initial states, until a goal is found. Random-restart is complete, since it will eventually generate a goal as the initial state.\nIf each search has a probability of success, the expected number of restarts is . <br>\n<a data-href=\"Simulated Annealing\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Simulated Annealing</a>: A local search algorithm with a temperature which determines the <a href=\"https://emujakic.github.io/TechKB/notes/math/probability.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"22\" to=\"33\" origin-text=\"probability\" class=\"internal-link virtual-link-a\">probability</a> of the algorithm accepting a downhill move. The algorithms starts with a high temperature, which decreases over iterations. The probability of a move being selected decreases with the \"badness\" of the move.\nSince simulated annealing can make downhill moves, this allows it to escape local maxima that other algorithms can't. <br>\n<a data-href=\"Local Beam Search\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Local Beam Search</a>: A local search algorithm which keeps track of states rather than just one. It begins with randomly generated states, and at each iteration, generates all the neighbors of all states. If one of the successors is a goal state, then the algorithm halts; else, the algorithm selects the best successors and repeats. A variant of local beam search, called stochastic beam search, chooses successors with probabilities proportional to their values. <br>\n<a data-href=\"Genetic Algorithms\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Genetic Algorithms</a>: A variant of local beam search inspired by the principles of natural selection and genetics. Successor states are generated by a process called recombination, where two parent states are combined to make a new state. One approach to recombination is to randomly select a crossover point, which splits each of the parent strings and recombines them to form a child. Genetic algorithms starts with a \"population\" of states, where each state is evaluated based on its fitness level, which determines the likelihood of that state being selected for reproduction. The mutation rate determines the probability that each bit in the offspring string is flipped. This ensures diversity in the population. Elitism is where a number of high-scoring individuals from the previous generation are propagated forward into the current generation without modification. This ensures that high-quality solutions are preserved while still maintaining diversity. Culling is where individuals performing under a certain threshold are removed from the population. Schema: A substring where some positions are left unspecified. Schemas can be thought of as templates or patterns for a full solution. Genetic algorithms work best when schemas correspond to meaningful components of a solution. Continuous Space: A state space with an infinite branching factor. Most real-world environments are continuous. You can discretize a continuous space be limiting values to fixed intervals.\nAlternatively, you can make the branching factor finite by sampling successor states randomly, in a random direction by some small amount . <br>\n<a data-href=\"Gradient\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Gradient</a>: A <a data-href=\"Vector\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Vector</a> that contains all the <a data-tooltip-position=\"top\" aria-label=\"Partial Derivative\" data-href=\"Partial Derivative\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">partial derivatives</a> of a function. Empirical Gradient: The gradient of a function based on observed data rather than analytical calculation. <br>\n<a data-href=\"Line Search\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Line Search</a>: An optimization technique that is used to find a satisfactory step size along a specific direction to minimize a function. <br>\n<a data-href=\"Newton-Raphson Algorithm\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Newton-Raphson Algorithm</a>: A general method for finding roots of functions, that is, solutions to equations of the form . Constrained Optimization Problem: An optimization problem where solutions must satisfy some constraints on the value of variables. <br>The most well-known category of constrained optimization problems are <a data-href=\"Linear Programming\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Linear Programming</a> problems.\n<br>Linear programming is a case of the more general problem of <a data-href=\"Convex Optimization\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Convex Optimization</a>. Search with Nondeterministic Actions: When the environment is partially-observable or is nondeterministic, the agent either doesn't exactly know it's current state (sensor model), or the next state (transition model) respectively. <br>\n<a data-href=\"Belief State\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Belief State</a>: The set of states that the agent believes it could be in. In nondeterministic or partially-observable environments, the solution to a problem is a conditional plan rather than a sequence of actions. The action to take is conditionally dependent on the percepts recieved. In a nondeterministic environment, the transition model returns a set of possible states, rather than a single state. The conditional plan can contain if-else statements, which allows solutions to be represented as trees rather than sequences. In a deterministic environment, the only branching is introduced by the agent's actions. These are called 'or' nodes. In a non-deterministic environment, branching is also introduced by the environments choice of outcome for each action. These are called 'and' nodes. <br>\nThese two nodes alternate to create an <a data-href=\"And-Or Tree\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">And-Or Tree</a>. A solution for an and-or search problem is a subtree where: Every leaf is a goal node.\nSpecifies an action at each OR node.\nIncludes each outcome of each AND node. And-or graphs can be explored using breadth-first or depth-first search. Cycles can arise in nondeterministic problems, one case is where an action has no effect on the current state. If an agent is in an environment where actions can fail, there are no acyclic solutions.\nOne workaround is to use a while construct, where an action is repeated until it succeeds. This is only useful if repeating the action increases the probability that it succeeds. Sensorless Problems: A problem where an agent's percepts provide no information on the state of the environment. The solution to sensorless problems is a sequence of actions, not a conditional plan, since there are no possible percepts to condition on.\nIf problem has states, the belief-state problem has states.\nThe initial state is typically all the states of .\nIf the agent is unsure about what state it is in, and if some actions are only legal in particular states, then the agent is unsure about what actions it can legally perform. If illegal actions have no consequence on the environment, then the agent can take the union of all actions. Though, if performing illegal actions can be detrimental, then it is safer to take the intersection of available actions.\nThe transition model for belief states results in a new belief state with all the possible results of the action for each state in the current belief state.\nThe agent is possible in a goal state if the current belief state contains a goal state. The agent is necessarily in a goal state if every state in the current belief state is a goal state.\nThe path cost could be one of several values if the same action has different costs in different states.\nIn ordinary graph search, newly reached states are tested to see if they've been visited previously, this can be done for belief states as well. If the current belief state is a superset of a previous belief state, we can discard the superset belief state since a solution to the superset must be a solution for each state in the corresponding subset.\nAdditionally, if the superset has been proven to be solvable, then any of its subsets are guaranteed to be solvable. This extra level of pruning can dramatically increase the efficiency of sensorless solutions. Though, even with this pruning, sensorless problems are still too vast to be solved efficiently.\nOne alternative is to avoid standard search algorithms, and use algorithms that look within belief states and develop incremental belief-state search algorithms. Partially Observable Problems: Problems where the agent has a PERCEPTS function which returns the percept recieved by the agent's sensors in a given state. If sensing is nondeterministic, the PERCEPTS function can return a set of possible percepts.\nWith partial observability, many states can produce the same percepts.\nThe two main differences between agents in partially observable environments and fully observable deterministic environments is: The solution is a conditional plan rather than a sequence.\nThe agent needs to maintain a belief state. Online Search: A search algorithm which interleaves action and planning. This is useful in dynamic or semi-dynamic environments, where the environment can change while the agent is deliberating. Online search is also useful in nondeterministic environments by focusing on contingencies which actually occur, rather than the set of all possible contingencies.\nThe competitive ratio is the discovered solution cost to the optimal cost if the environment was known.\nOnline agents can get stuck in dead-ends, states from which no goal is reachable.\n<br>One common example of online search are <a data-href=\"Simultaneous Localization and Mapping\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Simultaneous Localization and Mapping</a> or SLAM problems, where an agent must build a map of an unknown environment while simultaneously keeping track of its position within that environment.\nAn offline algorithm explores a model of the state space, while an online algorithm explores the real world. Therefore, an online algorithm can only discover successors for the state it is currently in, while offline algorithms can jump around the state space.\nIn an online DFS, when an agent has tried all the actions in a given state, it must backtrack in the physical world. This can be done by keeping track of the predecessor states of the current state. Though this only works if the actions in the state-space are reversible. Online Local Search: Algorithms like hill climbing or gradient descent are already considered online algorithms. Though, these algorithms are not as good for exploration due to getting stuck at local maxima/minima. A random walk simply selects one of the actions available at random. Preference can be given to actions that have yet to be tried.\nHill climbing can be augmented with memory, where a heuristic value is stored for each visited state.\nOptimism under uncertainty encourages the agent to explore new paths, rather than go down already explored paths.\n<br><a data-href=\"Learning Real-Time A*\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Learning Real-Time A*</a>* updates the cost element for the state it just left, then chooses the best move according to it's current cost element. Question other relevant information Peter. R. Norvig, Artificial Intelligence: A Modern Approach, Global Edition. 2021.\n","aliases":[],"inlineTags":[],"frontmatterTags":["#textbook"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Textbook Summary: AI-A Modern Approach","level":1,"id":"Textbook_Summary_AI-A_Modern_Approach_0"},{"heading":"Author(s):","level":3,"id":"Author(s)_0"},{"heading":"Introduction","level":2,"id":"Introduction_0"},{"heading":"Chapter Overview","level":2,"id":"Chapter_Overview_0"},{"heading":"Ch.1 Introduction:","level":3,"id":"Ch.1_Introduction_0"},{"heading":"Ch.2 Intelligent Agents:","level":3,"id":"Ch.2_Intelligent_Agents_0"},{"heading":"Ch.3 Solving by Searching:","level":3,"id":"Ch.3_Solving_by_Searching_0"},{"heading":"Ch.4 Search in Complex Environments:","level":3,"id":"Ch.4_Search_in_Complex_Environments_0"},{"heading":"Ch.5 Constraint Satisfaction Problems","level":3,"id":"Ch.5_Constraint_Satisfaction_Problems_0"},{"heading":"Questions for Further Study","level":2,"id":"Questions_for_Further_Study_0"},{"heading":"Additional Notes","level":2,"id":"Additional_Notes_0"},{"heading":"References","level":2,"id":"References_0"}],"links":["notes/math/range.html#_0","notes/ai/turing-test.html#_0",".html",".html","notes/math/probability.html#_0",".html",".html",".html",".html",".html",".html",".html","notes/math/probability.html#_0",".html",".html",".html",".html",".html",".html","notes/math/probability.html#_0","notes/math/range.html#_0",".html",".html",".html","notes/math/mean.html#_0",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html","projects/ai-chess-robot/chess-engine.html",".html",".html",".html",".html","notes/math/probability.html#_0",".html",".html",".html","notes/math/probability.html#_0",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html"],"author":"Ernad Mujakic","coverImageURL":"","fullURL":"https://emujakic.github.io/TechKB/textbooks/ai-a-modern-approach/summary.html","pathToRoot":"../..","attachments":[],"createdTime":1751236285335,"modifiedTime":1754355077347,"sourceSize":32625,"sourcePath":"TEXTBOOKS/AI-A Modern Approach/Summary.md","exportPath":"textbooks/ai-a-modern-approach/summary.html","showInTree":true,"treeOrder":28,"backlinks":[],"type":"markdown"},"textbooks/ai-a-modern-approach/practice-problems.html":{"title":"Practice Problems","icon":"","description":" Intelligence is the ability to define and utilize knowledge.\nRationality is the understanding, prioritizing, and doing the ‘right’ thing.\nArtificial intelligence is a program that acts rationally given its knowledge base.\nAn agent is anything that can act upon its environment, sometimes an agent can observe its environment with sensors.\nLogical reasoning is coming to sound conclusions based on given knowledge and the application of logical axioms and rules to derive new conclusions. X X X ","aliases":[],"inlineTags":[],"frontmatterTags":["#practice"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"AI: A Modern Approach Practice Problems","level":1,"id":"AI_A_Modern_Approach_Practice_Problems_0"},{"heading":"Problems","level":2,"id":"Problems_0"},{"heading":"Ch.1 - Introduction","level":2,"id":"Ch.1_-_Introduction_0"},{"heading":"1.1 Define in your own words: (a) intelligence, (b) artificial intelligence, (c) agent, (d) rationality, (e) logical reasoning.","level":4,"id":"1.1_Define_in_your_own_words_(a)_intelligence,_(b)_artificial_intelligence,_(c)_agent,_(d)_rationality,_(e)_logical_reasoning._0"},{"heading":"Question","level":4,"id":"Question_0"},{"heading":"Question","level":4,"id":"Question_1"},{"heading":"Question","level":4,"id":"Question_2"}],"links":[],"author":"","coverImageURL":"","fullURL":"textbooks/ai-a-modern-approach/practice-problems.html","pathToRoot":"../..","attachments":[],"createdTime":1751236319998,"modifiedTime":1754013327718,"sourceSize":849,"sourcePath":"TEXTBOOKS/AI-A Modern Approach/Practice Problems.md","exportPath":"textbooks/ai-a-modern-approach/practice-problems.html","showInTree":true,"treeOrder":27,"backlinks":[],"type":"markdown"},"index.html":{"title":"Index","icon":"","description":"<a href=\"https://emujakic.github.io/TechKB/notes/math/propositional-logic.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"0\" to=\"19\" origin-text=\"Propositional Logic\" class=\"internal-link virtual-link-a\">Propositional Logic</a><br><a href=\"https://emujakic.github.io/TechKB/notes/math/manhattan-distance.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"0\" to=\"18\" origin-text=\"Manhattan Distance\" class=\"internal-link virtual-link-a\">Manhattan Distance</a><br><a href=\"https://emujakic.github.io/TechKB/notes/math/transitive-property.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"0\" to=\"19\" origin-text=\"Transitive Property\" class=\"internal-link virtual-link-a\">Transitive Property</a>Number of Notes Total: 22\n<br><a data-href=\"Computer Architecture\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Computer Architecture</a>\n<br><a data-href=\"Digital Logic Design\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Digital Logic Design</a> Boolean Algebra <br><a data-href=\"Operating Systems\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Operating Systems</a>\nEmbedded Systems\nComputer Networks <br><a data-href=\"Programming Basics\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Programming Basics</a>\n<br><a data-href=\"Data Structures and Algorithms\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Data Structures and Algorithms</a>\n<br><a data-href=\"Software Engineering\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Software Engineering</a>\n<br><a data-href=\"Web Development\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Web Development</a>\n<br><a data-href=\"Database Systems\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Database Systems</a>\n<br><a data-href=\"Theory of Computation\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Theory of Computation</a> <br><a data-href=\"Machine Learning\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Machine Learning</a> Supervised Learning\nUnsupervised Learning\nReinforcement Learning <br><a data-href=\"Deep Learning\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Deep Learning</a> <br><a data-href=\"Neural Network\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Neural Network</a>\n<br><a data-href=\"Convolutional Neural Network\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Convolutional Neural Network</a>\n<br><a data-href=\"Recurrent Neural Network\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Recurrent Neural Network</a>\nSelf-Organizing Maps AI Tools <br><a data-href=\"PyTorch\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">PyTorch</a>\n<br><a data-href=\"Scikit-Learn\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Scikit-Learn</a>\n<br><a data-href=\"OpenCV\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">OpenCV</a>\n<br><a data-href=\"Natural Language Processing\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Natural Language Processing</a>\n<br><a data-href=\"Computer Vision\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Computer Vision</a>\n<br><a data-href=\"Robotics\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Robotics</a> AI Algorithms Genetic Algorithms &amp; Evolutionary Methods\nBayesian Methods\nGraph Algorithms Data Mining Data Preprocessing\nData Exploration\nData Mining Techniques <br><a data-href=\"Algebra\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Algebra</a>\nGeometry\nTrigonometry\nCalculus\n<br><a data-href=\"Probability\" href=\"https://emujakic.github.io/TechKB/notes/math/probability.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Probability</a>\nStatistics\nSet Theory\nLogics <br><a data-href=\"Propositional Logic\" href=\"https://emujakic.github.io/TechKB/notes/math/propositional-logic.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Propositional Logic</a>\nFirst-Order Logic\nPredicate Logic\nTemporal Logic\nModal Logic Linear Algebra\nDiscrete Mathematics Basic Electrical Concepts\nOhm's Law and Kirchhoff's Laws\nCircuit Analysis\nElectromagnetism\nAnalog Electronics\nDigital Electronics\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Table of Contents","level":1,"id":"Table_of_Contents_0"},{"heading":"Recently Updated","level":9,"id":"Recently_Updated_0"},{"heading":"Computer Engineering","level":2,"id":"Computer_Engineering_0"},{"heading":"Computer Science","level":2,"id":"Computer_Science_0"},{"heading":"AI","level":2,"id":"AI_0"},{"heading":"Mathematics","level":2,"id":"Mathematics_0"},{"heading":"Electrical Engineering","level":2,"id":"Electrical_Engineering_0"}],"links":["notes/math/propositional-logic.html#_0","notes/math/manhattan-distance.html#_0","notes/math/transitive-property.html#_0",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html","notes/math/probability.html#_0","notes/math/propositional-logic.html#_0"],"author":"Ernad Mujakic","coverImageURL":"","fullURL":"https://emujakic.github.io/TechKB/index.html","pathToRoot":".","attachments":[],"createdTime":1753993773434,"modifiedTime":1754409881723,"sourceSize":1688,"sourcePath":"Index.md","exportPath":"index.html","showInTree":true,"treeOrder":32,"backlinks":[],"type":"markdown"},"notes/math/propositional-logic.html":{"title":"Propositional Logic","icon":"","description":"Author: <a href=\"https://emujakic.github.io/TechKB/contact.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"2\" to=\"15\" origin-text=\"Ernad Mujakic\" class=\"internal-link virtual-link-a\">Ernad Mujakic</a>\nDate: 2025-08-02<br>Propositional logic is a branch of <a data-href=\"Logic\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Logic</a> that deals with propositions, which are statements that are either true or false. An example of a proposition is the sentence, \"Tomorrow is Thursday\", since it's either true or false.Propositional logic is made up of:\nPropositional Symbols: Symbols that start with an uppercase letter and refer to a proposition. For example, , , and are examples of propositional symbols. Each symbol represents a distinct statement that can be true or false. Individual symbols are commonly referred to as literals. A literal is negative if there is a negation applied to it (e.g. ), else, it's a positive literal.\nLogical Connectives: Operators which combine propositional symbols to create complex sentences. The primary logical connectives include: Negation (NOT, ¬)\nConjunction (AND, ∧)\nDisjunction (OR, ∨)\nImplication (IMPLIES, →)\nBiconditional (IF AND ONLY IF, ↔) Atomic Sentence: An atomic sentence consists of a single propositional symbol and represents a basic assertion.\nComplex Sentence: A complex sentence is made up of propositional symbols connected by parenthesis and logical connectives. They are also called formulas.\n<br>Negation flips the truth value of the propositional symbol it is applied to. Negating gives , which can be understood in English as \"not \". The <a data-href=\"Truth Table\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Truth Table</a> for negation is as follows:A conjunction of two conjuncts is true only if both of its conjuncts is true. This can be understood in English as \"and\", and is denoted in propositional logic with the symbol . The truth table for conjunction is as follows:A disjunction of two disjuncts is true if either of its disjuncts are true. This can be understood as the English word \"or\", meaning if one or the other is true, then the disjunction is true. Disjunction is denoted with the symbol . The truth table for disjunction is as follows:An Implication is a binary operator with it's left-hand side being the premise, and the right-hand side being the conclusion. An implication states that if the premise is true, then the premise must be true. Implications can be thought of as if-then statements and are denoted with the symbols or . An implication is only false if its premise is true, while the conclusion is false. The truth table for implication is as follows:A biconditional can be thought of as double implication, meaning both sides imply the other. Biconditionals are true only if both symbols are the same truth value. The symbols used to denote biconditional is or , and can be thought of in English as \"if and only if\". The truth table for a biconditional is:A NAND can be thought of as a negation applied to a conjunction, . A NAND is true as long as one of its operands it false. It's represented symbolically as or as , and has the following truth table:A XOR, or exclusive or, is true when exactly one of its operands is true. It can be thought of as the English word 'or', in the context of when only one thing or the other is the case. XOR is denoted using the symbol, and its truth table is as follows:A NOR can be thought of as a negation applied to an OR operator, . A NOR is true only when both of its operands are false and is represented symbolically as . The truth table for NOR is as follows: A XNOR can be thought of as a negation applied to the XOR gate, . Where a XOR is true only when both of its operands are different, a XNOR is true only when both of its operands are the same. It has the same truth table as a biconditional, which is as follows:The order of precedence of logical connectives is as follows:\nNegation (NOT, ¬)\nConjunction (AND, ∧)\nDisjunction (OR, ),\nImplication (IMPLIES, →)\nBiconditional (IF AND ONLY IF, ↔)\n<br>Two sentences are said to be logically equivalent if they have the same truth table. This is denoted using the symbol. A <a data-href=\"Rule of Replacement\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Rule of Replacement</a> is a logical principle that allows for the substitution of one logical expression, for another, logically equivalent expression. Rules of replacement are used to construct proofs, simply logical expressions, and verify the correctness of logical statements. Propositional equivalence is vital for many domains, including:\n<br>Digital Logic Design: Used to simplify expressions in <a data-href=\"Boolean Algebra\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Boolean Algebra</a>, thereby optimizing and simplifying digital circuits.\n<br>Artificial Intelligence: Used to in <a data-href=\"Knowledge Representation\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Knowledge Representation</a> as well as inference algorithms, to derive new facts from an existing <a data-href=\"Knowledge Base\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Knowledge Base</a>.\n<br><a data-href=\"Control Theory\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Control Theory</a>: Used for simplifying systems equations, analyze conditions, and modeling states and transitions.\nMathematical Proofs: Used to transform sentences into equivalent forms which are easier to prove.\nThe double negation law is a rule of replacement which states that, when negation is applied twice to an expression, the truth table for that expression remains the same. This is because the second negation 'cancels out' the first one, leaving only the original expression left.\nThe identity laws state that, any proposition conjoined with a true value, is logically equivalent to the proposition alone. For disjunction, any proposition disjoined with a false value, is logically equivalent to the proposition alone. This is because it both cases, the expression's truth value relies entirely on the value of the proposition.\nThe domination laws state that, any proposition conjoined with a false value is always false, and any proposition disjoined with a true value is always true. This is because, in the case of conjunction, the AND operation requires both operands to be true for the result to be true. Since one operand is false, the entire expression cannot be true, regardless of the value of the other operand. In the case of disjunction, the OR operation requires at least one operand to be true for the result to be true. Since one operand is true, the entire expression will always evaluate to true.\nThe idempotent laws state that a proposition conjoined or disjoined with itself is logically equivalent to the proposition alone. This means that applying conjunction or disjunction to a proposition with itself multiple times yields the same truth table as the original proposition.\nThe distributive laws describe how conjunction (AND) and disjunction (OR) interact with each other in propositional logic, allowing us to distribute one operation over the other.\nThe first distributive law states that a conjunction of with disjunction of and is logically equivalent to the disjunction of AND with AND R.\nThe second distributive law states that the disjunction of with the conjunction of and is logically equivalent to the conjunction of OR with OR .\nDe Morgan's Laws consist of two rules of replacement which define the relationship between disjunctions and conjunctions through negation.\nDe Morgan's first law states that the negation of a conjunction is logically equivalent to the disjunction of the negated conjuncts. This is directly equivalent to the definition of the NAND operator and can be seen as a direct implementation of De Morgan's first law.\nDe Morgan's second law states that the negation of a disjunction is logically equivalent to the conjunction of the negated disjuncts. This is directly equivalent to the definition of the NOR operator and can be seen as a direct implementation of De Morgan's second law.\n<br>The <a href=\"https://emujakic.github.io/TechKB/notes/math/commutative-property.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"4\" to=\"15\" origin-text=\"commutative\" class=\"internal-link virtual-link-a\">commutative</a> laws state that the order of operands in a conjunction and disjunction are logically equivalent.\n<br>The <a href=\"https://emujakic.github.io/TechKB/notes/math/associative-property.html#_0\" target=\"_self\" rel=\"noopener noreferrer\" from=\"4\" to=\"15\" origin-text=\"associative\" class=\"internal-link virtual-link-a\">associative</a> laws state that the grouping of operands in a conjunction or disjunction does not affect the truth value of expression.\nA clause is a disjunction/conjunction of literals. When talking about clauses, usually it refers to a disjunctive clause, which is a logical expression formed by connecting literals with the OR operator.\nThe empty clause is a clause with no literals, commonly denoted as , , or . An empty disjunctive clause is always false, making it analogous to a contradiction. This is an important concept in proof by contradiction, as reaching an empty clause indicates that a contradiction has been proven.A Horn clause is a disjunctive clause with at most one positive literal.\nDefinite Clause: If a Horn clause has exactly one positive literal, it is a definite clause. For example, .\nGoal Clause: If it has no positive literals, it is a goal clause. For example, .\nA Horn clause can be represented in disjunctive form:\nA Horn clause can also be represented in implicative form:\nIn both cases, is the only positive literal.<br>Horn clauses are computationally efficient for algorithms such as resolution theorem proving, or for <a data-href=\"Forward/Backward Chaining\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Forward/Backward Chaining</a>. This makes them the basis of many <a data-href=\"Logic Programming\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Logic Programming</a> languages, as well as for automated theorem proving or database querying.A sentence is considered to be in conjunctive normal form (CNF) if it's a conjunction (AND) of one or more clauses. Every sentence in propositional logic can be converted into an equivalent CNF form<br>In logic, an argument consists of a set of sentences called premises, which, if all are true, the conclusion sentence must follow. This is a form of reasoning called <a data-href=\"Deductive Reasoning\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Deductive Reasoning</a> which uses general statements to derive specific conclusions.\nValid: An argument is valid if its conclusion must follow its premises. Meaning that if the argument's premises are true, then the conclusion must be true as well, otherwise, the argument is invalid.\nSound: An argument is sound if it's premises are true. If any of its premises are false, then the argument is unsound.\nIf an argument is valid and sound, then its conclusion must be true. A sound argument is always valid, while a valid argument may be unsound.\nArgument forms are patterns that represent valid structures for arguments.\n<br><a data-tooltip-position=\"top\" aria-label=\"Syllogism\" data-href=\"Syllogism\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Syllogisms</a> are argument forms that consist of two premises which support a conclusion.\nModus Ponens is a syllogistic argument form and rule of inference. It has the following structure:\nIf then .\n.\nTherefore, .\n<br>The first two sentences are the arguments premises, and the final sentence is the conclusion. Modus Ponens is represented in <a data-href=\"Gentzen Notation\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Gentzen Notation</a> as:\nModus Tollens is another valid syllogistic argument form, similar to Modus Ponens. Though, unlike Modus Ponens, Modus Tollens affirms the negation of antecedent from the negation of the consequent. The structure of Modus Tollens is as follows:\nIf then .\n.\nTherefore, .\nLike Modus Ponens, the first two sentences are the arguments premises, and the final sentence is the conclusion. Modus Tollens is represented symbolically as:\nA disjunctive syllogism is a syllogistic argument form with a disjunction as one of its premises. It says that if one of the two disjuncts is true, and we know that one is false, then the other must be true. The structure of disjunctive syllogisms is as follows: or .\n.\nTherefore, .\nIn Gentzen notation:\n<br>A hypothetical syllogism, also called the chain rule, is an argument form which uses the <a data-tooltip-position=\"top\" aria-label=\"Transitive Property\" data-href=\"Transitive Property\" href=\"https://emujakic.github.io/TechKB/notes/math/transitive-property.html#_0\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">transitive property</a> of implication. The structure of a hypothetical syllogism is as follows:\nIf then .\nIf then .\nTherefore, if then .\nHypothetical syllogisms are represented in Gentzen notation as:\nRules of inference are logical rules that dictate derivations of conclusions from premises. Rules of inference differ from rules of replacement which state that two expressions are logically equivalent and can be freely swapped for one another.Implication Elimination is a rule of inferenceIn propositional logic, proofs are essential for establishing the truth of a statement or the validity of an argument. A proof is a demonstration that a conclusion follows from a set of premises.<br><a data-tooltip-position=\"top\" aria-label=\"Proof System\" data-href=\"Proof System\" href=\"https://emujakic.github.io/TechKB/.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">Proof Systems</a> are frameworks which provide the structure and methodology for constructing proofs. Proof systems are made up of:\nAxioms: Foundational propositions that are accepted as true without having been proven.\nRules of Inference: Rules which provide the structure for how new statements can be derived from existing ones.\nSyntax: The symbols used to represent statements.\nSemantics: The interpretation of the meaning of the statements within the proof system.\nThe most common proof systems include Natural Deduction, Sequent Calculus, and Resolution.A direct proof is one of the most common and straightforward methods for deriving conclusions. The structure of a direct proof is as follows:\nIdentify the premises: Clearly state the premises which you will use to derive the conclusion.\nState the conclusion: State the conclusion you wish to prove from the premises.\nLogical deduction: Use the rules of inference to derive the conclusion from the premises step by step.\nProof by contradiction is an indirect proof that assumes the conclusion is false, then proves that this assumption leads to a contradiction. If assuming the conclusion is false does lead to a contradiction, then the conclusion must be true. The structure of a proof by contradiction is as follows:\nIdentify the premises: Clearly state the premises which you will use to derive the conclusion.\nState the conclusion: State the conclusion you wish to prove from the premises.\nAssume the negation: Assume that the conclusion is false.\nLogical deduction: Use the rules of inference to derive a contradiction. A contradiction typically manifests as the empty sentence , which is a statement that is always false.\nConclude the Conclusion: If the assumption that the conclusion is false did lead to a contradiction, then the original conclusion must be true.\nProof by contrapositive is an indirect proof which proves a conclusion by proving its contrapositive. Since a sentence and its contrapositive are logically equivalent, proving one proves the other. The structure for a proof by contrapositive is as follows:\nState the conclusion: State the conclusion you wish to prove, for example, .\nIdentify the contrapositive: Identify the contrapositive of the conclusion, for example, .\nLogical deduction: Use the rules of inference to prove the contrapositive.\nConclude the Original Implication: If the contrapositive can be prove, then the original conclusion is also proven. Vacuous Truth (proof)\nCompleteness (proof)\nentailment (proof)\ntautologies (proof)\ntheorem proving (proof)\nsatisfiability (proof)\nmonotonicity S. J. Russell and P. Norvig,&nbsp;Artificial Intelligence: a Modern Approach, 4th ed. Upper Saddle River: Pearson, 2020.\nWikipedia Contributors, “Propositional calculus,”&nbsp;Wikipedia, Oct. 21, 2024.\n<br>GeeksforGeeks, “Propositional Logic,”&nbsp;GeeksforGeeks, Jun. 19, 2015. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/engineering-mathematics/proposition-logic/\" target=\"_self\">https://www.geeksforgeeks.org/engineering-mathematics/proposition-logic/</a>\n<br>C. Franks, “Propositional Logic,”&nbsp;Stanford Encyclopedia of Philosophy, 2023. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://plato.stanford.edu/entries/logic-propositional/\" target=\"_self\">https://plato.stanford.edu/entries/logic-propositional/</a>\n<br>GeeksforGeeks, “Propositional Equivalences,”&nbsp;GeeksforGeeks, Jun. 22, 2015. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/engineering-mathematics/mathematical-logic-propositional-equivalences/\" target=\"_self\">https://www.geeksforgeeks.org/engineering-mathematics/mathematical-logic-propositional-equivalences/</a> (accessed Aug. 03, 2025).\n<br>“Propositional Logic: Part I -Semantics 12-0.” Available: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.cas.mcmaster.ca/~lawford/2F03/Notes/prop.pdf\" target=\"_self\">https://www.cas.mcmaster.ca/~lawford/2F03/Notes/prop.pdf</a>\nWikipedia Contributors, “Rule of replacement,”&nbsp;Wikipedia, Mar. 03, 2025.\n<br>GeeksforGeeks, “Idempotent Laws,”&nbsp;GeeksforGeeks, Sep. 08, 2024. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.geeksforgeeks.org/maths/idempotent-laws/\" target=\"_self\">https://www.geeksforgeeks.org/maths/idempotent-laws/</a> (accessed Aug. 03, 2025).\n<br>Wikipedia Contributors, “De Morgan’s laws,”&nbsp;Wikipedia, Sep. 05, 2019. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/De_Morgan%27s_laws\" target=\"_self\">https://en.wikipedia.org/wiki/De_Morgan%27s_laws</a>\n<br>“1.2: Basic Notions - Propositions and Arguments,”&nbsp;Humanities LibreTexts, Sep. 26, 2019. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://human.libretexts.org/Bookshelves/Philosophy/Fundamental_Methods_of_Logic_(Knachel)/01%3A_The_Basics_of_Logical_Analysis/1.02%3A_Basic_Notions_-_Propositions_and_Arguments\" target=\"_self\">https://human.libretexts.org/Bookshelves/Philosophy/Fundamental_Methods_of_Logic_(Knachel)/01%3A_The_Basics_of_Logical_Analysis/1.02%3A_Basic_Notions_-_Propositions_and_Arguments</a>\n<br>Dr. Trefor Bazett, “Logical Arguments - Modus Ponens &amp; Modus Tollens,”&nbsp;YouTube. May 23, 2017. Accessed: Sep. 02, 2022. [Online]. Available: <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.youtube.com/watch?v=NTSZMdGlo4g\" target=\"_self\">https://www.youtube.com/watch?v=NTSZMdGlo4g</a>\n<br>Wikipedia Contributors, “Modus ponens,”&nbsp;Wikipedia, Jan. 20, 2019. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Modus_ponens\" target=\"_self\">https://en.wikipedia.org/wiki/Modus_ponens</a>\n<br>“Disjunctive syllogism,”&nbsp;Wikipedia, Mar. 03, 2024. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Disjunctive_syllogism\" target=\"_self\">https://en.wikipedia.org/wiki/Disjunctive_syllogism</a>\n<br>TrevTutor, “[Logic] Proofs and Rules #1,”&nbsp;<a data-tooltip-position=\"top\" aria-label=\"http://www.youtube.com\" rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://www.youtube.com\" target=\"_self\">www.youtube.com</a>. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://www.youtube.com/watch?v=m2j0TX-e8NY\" target=\"_self\">https://www.youtube.com/watch?v=m2j0TX-e8NY</a> (accessed Aug. 4, 2025).\nWikipedia Contributors, “Resolution (logic),”&nbsp;Wikipedia, May 28, 2025.\n<br>“Introduction to Logic - Chapter 6,”&nbsp;Stanford.edu, 2025. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"http://intrologic.stanford.edu/chapters/chapter_06.html\" target=\"_self\">http://intrologic.stanford.edu/chapters/chapter_06.html</a> (accessed Aug. 05, 2025).\n<br>Wikipedia Contributors, “Horn clause,”&nbsp;Wikipedia, Dec. 18, 2019. <a rel=\"noopener nofollow\" class=\"external-link is-unresolved\" href=\"https://en.wikipedia.org/wiki/Horn_clause\" target=\"_self\">https://en.wikipedia.org/wiki/Horn_clause</a>\n","aliases":[],"inlineTags":[],"frontmatterTags":["#math","#logic","#propositionalLogic","#unfinished"],"headers":[{"heading":"","level":1,"id":"_0"},{"heading":"Propositional Logic","level":1,"id":"Propositional_Logic_0"},{"heading":"Syntax","level":3,"id":"Syntax_0"},{"heading":"Sentences","level":3,"id":"Sentences_0"},{"heading":"Logical Connectives","level":2,"id":"Logical_Connectives_0"},{"heading":"Negation (NOT, ¬)","level":3,"id":"Negation_(NOT,_¬)_0"},{"heading":"Conjunction (AND, ∧)","level":3,"id":"Conjunction_(AND,_∧)_0"},{"heading":"Disjunction (OR, <span class=\"math math-inline is-loaded\"><mjx-container class=\"MathJax\" jax=\"CHTML\"><mjx-math class=\"MJX-TEX\"><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c2228\"></mjx-c></mjx-mo></mjx-math></mjx-container></span>)","level":3,"id":"Disjunction_(OR,_$\\lor$)_0"},{"heading":"Implication (IMPLIES, →)","level":3,"id":"Implication_(IMPLIES,_→)_0"},{"heading":"Biconditional (IF AND ONLY IF, ↔)","level":3,"id":"Biconditional_(IF_AND_ONLY_IF,_↔)_0"},{"heading":"NAND (NOT AND, <span class=\"math math-inline is-loaded\"><mjx-container class=\"MathJax\" jax=\"CHTML\"><mjx-math class=\"MJX-TEX\"><mjx-texatom texclass=\"ORD\"><mjx-mover><mjx-over style=\"padding-bottom: 0.105em; padding-left: 0.334em; margin-bottom: -0.544em;\"><mjx-mo class=\"mjx-n\" style=\"width: 0px; margin-left: -0.25em;\"><mjx-c class=\"mjx-cAF\"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c2227\"></mjx-c></mjx-mo></mjx-base></mjx-mover></mjx-texatom></mjx-math></mjx-container></span>)","level":3,"id":"NAND_(NOT_AND,_$\\bar{\\land}$)_0"},{"heading":"XOR (EXCLUSIVE OR, <span class=\"math math-inline is-loaded\"><mjx-container class=\"MathJax\" jax=\"CHTML\"><mjx-math class=\"MJX-TEX\"><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c2295\"></mjx-c></mjx-mo></mjx-math></mjx-container></span>)","level":3,"id":"XOR_(EXCLUSIVE_OR,_$\\oplus$)_0"},{"heading":"NOR (NOT OR, <span class=\"math math-inline is-loaded\"><mjx-container class=\"MathJax\" jax=\"CHTML\"><mjx-math class=\"MJX-TEX\"><mjx-texatom texclass=\"ORD\"><mjx-mover><mjx-over style=\"padding-bottom: 0.105em; padding-left: 0.334em; margin-bottom: -0.544em;\"><mjx-mo class=\"mjx-n\" style=\"width: 0px; margin-left: -0.25em;\"><mjx-c class=\"mjx-cAF\"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c2228\"></mjx-c></mjx-mo></mjx-base></mjx-mover></mjx-texatom></mjx-math></mjx-container></span>)","level":3,"id":"NOR_(NOT_OR,_$\\bar{\\lor}$)_0"},{"heading":"XNOR (EXCLUSIVE NOT OR, <span class=\"math math-inline is-loaded\"><mjx-container class=\"MathJax\" jax=\"CHTML\"><mjx-math class=\"MJX-TEX\"><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c2299\"></mjx-c></mjx-mo></mjx-math></mjx-container></span>)","level":3,"id":"XNOR_(EXCLUSIVE_NOT_OR,_$\\odot$)_0"},{"heading":"Precedence of Logical Connectives","level":3,"id":"Precedence_of_Logical_Connectives_0"},{"heading":"Logical Equivalence","level":2,"id":"Logical_Equivalence_0"},{"heading":"Properties and Laws","level":3,"id":"Properties_and_Laws_0"},{"heading":"Double Negation Law","level":4,"id":"Double_Negation_Law_0"},{"heading":"Identity Laws","level":4,"id":"Identity_Laws_0"},{"heading":"Domination Laws","level":4,"id":"Domination_Laws_0"},{"heading":"<a data-tooltip-position=\"top\" aria-label=\"Idempotent Property\" data-href=\"Idempotent Property\" href=\"Idempotent Property\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Idempotent</a> Laws","level":4,"id":"[[Idempotent_Property|Idempotent]]_Laws_0"},{"heading":"<a data-tooltip-position=\"top\" aria-label=\"Distributive Property\" data-href=\"Distributive Property\" href=\"Distributive Property\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Distributive</a> Laws","level":4,"id":"[[Distributive_Property|_Distributive]]_Laws_0"},{"heading":"<a data-href=\"De Morgan's Laws\" href=\"De Morgan's Laws\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">De Morgan's Laws</a>","level":4,"id":"[[De_Morgan's_Laws]]_0"},{"heading":"<a data-tooltip-position=\"top\" aria-label=\"Commutative Property\" data-href=\"Commutative Property\" href=\"Commutative Property\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Commutative</a> Laws","level":4,"id":"[[Commutative_Property|Commutative]]_Laws_0"},{"heading":"<a data-tooltip-position=\"top\" aria-label=\"Associative Property\" data-href=\"Associative Property\" href=\"Associative Property\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Associative</a> Laws","level":4,"id":"[[Associative_Property|Associative]]_Laws_0"},{"heading":"Clauses and Normal Form","level":2,"id":"Clauses_and_Normal_Form_0"},{"heading":"Horn Clause","level":3,"id":"Horn_Clause_0"},{"heading":"<a data-href=\"Conjunctive Normal Form\" href=\"Conjunctive Normal Form\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Conjunctive Normal Form</a>","level":3,"id":"[[Conjunctive_Normal_Form]]_0"},{"heading":"Disjunctive Normal Form","level":3,"id":"Disjunctive_Normal_Form_0"},{"heading":"Arguments","level":2,"id":"Arguments_0"},{"heading":"Validity and Soundness","level":3,"id":"Validity_and_Soundness_0"},{"heading":"<a data-tooltip-position=\"top\" aria-label=\"Argument Form\" data-href=\"Argument Form\" href=\"Argument Form\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Argument Forms</a>","level":3,"id":"[[Argument_Form|Argument_Forms]]_0"},{"heading":"<a data-href=\"Modus Ponens\" href=\"Modus Ponens\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Modus Ponens</a>","level":4,"id":"[[Modus_Ponens]]_0"},{"heading":"<a data-href=\"Modus Tollens\" href=\"Modus Tollens\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Modus Tollens</a>","level":4,"id":"[[Modus_Tollens]]_0"},{"heading":"<a data-href=\"Disjunctive Syllogism\" href=\"Disjunctive Syllogism\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Disjunctive Syllogism</a>","level":4,"id":"[[Disjunctive_Syllogism]]_0"},{"heading":"<a data-href=\"Hypothetical Syllogism\" href=\"Hypothetical Syllogism\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Hypothetical Syllogism</a>","level":4,"id":"[[Hypothetical_Syllogism]]_0"},{"heading":"<a data-href=\"Rules of Inference\" href=\"Rules of Inference\" class=\"internal-link\" target=\"_blank\" rel=\"noopener nofollow\">Rules of Inference</a>","level":3,"id":"[[Rules_of_Inference]]_0"},{"heading":"Implication Elimination / Introduction","level":4,"id":"Implication_Elimination_/_Introduction_0"},{"heading":"Biconditional Elimination / Introduction","level":4,"id":"Biconditional_Elimination_/_Introduction_0"},{"heading":"Conjunction Elimination / Introduction","level":4,"id":"Conjunction_Elimination_/_Introduction_0"},{"heading":"Disjunction Elimination / Introduction","level":4,"id":"Disjunction_Elimination_/_Introduction_0"},{"heading":"Proofs","level":2,"id":"Proofs_0"},{"heading":"Common Proof Techniques","level":3,"id":"Common_Proof_Techniques_0"},{"heading":"Direct Proof","level":4,"id":"Direct_Proof_0"},{"heading":"Proof by Contradiction","level":4,"id":"Proof_by_Contradiction_0"},{"heading":"Proof by Contrapositive","level":4,"id":"Proof_by_Contrapositive_0"},{"heading":"Resolution","level":4,"id":"Resolution_0"},{"heading":"Mathematical Induction","level":4,"id":"Mathematical_Induction_0"},{"heading":"TO-ADD","level":4,"id":"TO-ADD_0"},{"heading":"Applications","level":2,"id":"Applications_0"},{"heading":"Computer Science","level":3,"id":"Computer_Science_0"},{"heading":"Computer Engineering","level":3,"id":"Computer_Engineering_0"},{"heading":"Mathematics","level":3,"id":"Mathematics_0"},{"heading":"Game Theory","level":3,"id":"Game_Theory_0"},{"heading":"Control Theory","level":3,"id":"Control_Theory_0"},{"heading":"References","level":2,"id":"References_0"}],"links":["contact.html#_0",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html","notes/math/commutative-property.html#_0","notes/math/commutative-property.html#_0","notes/math/associative-property.html#_0","notes/math/associative-property.html#_0",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html",".html","notes/math/transitive-property.html#_0",".html",".html"],"author":"Ernad Mujakic","coverImageURL":"","fullURL":"https://emujakic.github.io/TechKB/notes/math/propositional-logic.html","pathToRoot":"../..","attachments":[],"createdTime":1754173984748,"modifiedTime":1754405691700,"sourceSize":21563,"sourcePath":"NOTES/Math/Propositional Logic.md","exportPath":"notes/math/propositional-logic.html","showInTree":true,"treeOrder":18,"backlinks":["index.html","notes/math/associative-property.html"],"type":"markdown"}},"fileInfo":{"notes/math/variance.html":{"createdTime":1752165687104,"modifiedTime":1754247600661,"sourceSize":6070,"sourcePath":"NOTES/Math/Variance.md","exportPath":"notes/math/variance.html","showInTree":true,"treeOrder":24,"backlinks":["notes/math/least-squares.html","notes/math/outlier.html","notes/math/standard-deviation.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown","data":null},"notes/math/transitive-property.html":{"createdTime":1753469739589,"modifiedTime":1754247595889,"sourceSize":3717,"sourcePath":"NOTES/Math/Transitive Property.md","exportPath":"notes/math/transitive-property.html","showInTree":true,"treeOrder":23,"backlinks":[],"type":"markdown","data":null},"notes/math/standard-deviation.html":{"createdTime":1752158879614,"modifiedTime":1754340040519,"sourceSize":2848,"sourcePath":"NOTES/Math/Standard Deviation.md","exportPath":"notes/math/standard-deviation.html","showInTree":true,"treeOrder":22,"backlinks":["notes/math/outlier.html","notes/math/variance.html","textbooks/data-mining-concepts-and-techniques/practice-problems.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown","data":null},"notes/math/range.html":{"createdTime":1752606554931,"modifiedTime":1754247593125,"sourceSize":1540,"sourcePath":"NOTES/Math/Range.md","exportPath":"notes/math/range.html","showInTree":true,"treeOrder":21,"backlinks":["notes/math/euclidean-distance.html","notes/math/manhattan-distance.html","notes/math/mean.html","notes/math/outlier.html","notes/math/standard-deviation.html","notes/math/variance.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown","data":null},"notes/math/quartile.html":{"createdTime":1751928764242,"modifiedTime":1754247589230,"sourceSize":4049,"sourcePath":"NOTES/Math/Quartile.md","exportPath":"notes/math/quartile.html","showInTree":true,"treeOrder":20,"backlinks":["notes/math/mean.html","notes/math/median.html","notes/math/outlier.html","notes/math/quantile.html","notes/math/range.html","notes/math/standard-deviation.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown","data":null},"notes/math/quantile.html":{"createdTime":1752697090804,"modifiedTime":1754247586326,"sourceSize":3487,"sourcePath":"NOTES/Math/Quantile.md","exportPath":"notes/math/quantile.html","showInTree":true,"treeOrder":19,"backlinks":["notes/math/quartile.html","textbooks/data-mining-concepts-and-techniques/practice-problems.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown","data":null},"notes/math/probability.html":{"createdTime":1751495555242,"modifiedTime":1754247568321,"sourceSize":10549,"sourcePath":"NOTES/Math/Probability.md","exportPath":"notes/math/probability.html","showInTree":true,"treeOrder":17,"backlinks":["index.html","notes/math/binary-data.html","notes/math/mean.html","notes/math/variance.html","textbooks/ai-a-modern-approach/summary.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown","data":null},"notes/math/outlier.html":{"createdTime":1752013301145,"modifiedTime":1754247552925,"sourceSize":8384,"sourcePath":"NOTES/Math/Outlier.md","exportPath":"notes/math/outlier.html","showInTree":true,"treeOrder":16,"backlinks":["notes/math/euclidean-distance.html","notes/math/least-squares.html","notes/math/manhattan-distance.html","notes/math/mean.html","notes/math/measure-of-central-tendency.html","notes/math/median.html","notes/math/quantile.html","notes/math/quartile.html","notes/math/range.html","notes/math/standard-deviation.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown","data":null},"notes/math/ordinal-data.html":{"createdTime":1751919439086,"modifiedTime":1754247549994,"sourceSize":2402,"sourcePath":"NOTES/Math/Ordinal Data.md","exportPath":"notes/math/ordinal-data.html","showInTree":true,"treeOrder":15,"backlinks":["notes/math/measure-of-central-tendency.html","notes/math/nominal-data.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown","data":null},"notes/math/nominal-data.html":{"createdTime":1751914187819,"modifiedTime":1754247547097,"sourceSize":1919,"sourcePath":"NOTES/Math/Nominal Data.md","exportPath":"notes/math/nominal-data.html","showInTree":true,"treeOrder":14,"backlinks":["notes/math/binary-data.html","notes/math/measure-of-central-tendency.html","notes/math/ordinal-data.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown","data":null},"notes/math/mode.html":{"createdTime":1752009968333,"modifiedTime":1754247543991,"sourceSize":1720,"sourcePath":"NOTES/Math/Mode.md","exportPath":"notes/math/mode.html","showInTree":true,"treeOrder":13,"backlinks":["notes/math/binary-data.html","notes/math/mean.html","notes/math/measure-of-central-tendency.html","notes/math/median.html","notes/math/nominal-data.html","notes/math/ordinal-data.html","textbooks/data-mining-concepts-and-techniques/practice-problems.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown","data":null},"notes/math/median.html":{"createdTime":1751927785580,"modifiedTime":1754247536481,"sourceSize":3023,"sourcePath":"NOTES/Math/Median.md","exportPath":"notes/math/median.html","showInTree":true,"treeOrder":12,"backlinks":["notes/math/binary-data.html","notes/math/mean.html","notes/math/measure-of-central-tendency.html","notes/math/mode.html","notes/math/nominal-data.html","notes/math/ordinal-data.html","notes/math/quantile.html","notes/math/quartile.html","textbooks/data-mining-concepts-and-techniques/practice-problems.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown","data":null},"notes/math/measure-of-central-tendency.html":{"createdTime":1751847846892,"modifiedTime":1754247539565,"sourceSize":3814,"sourcePath":"NOTES/Math/Measure of Central Tendency.md","exportPath":"notes/math/measure-of-central-tendency.html","showInTree":true,"treeOrder":11,"backlinks":["notes/math/mean.html","notes/math/median.html","notes/math/mode.html","notes/math/probability.html","notes/math/quantile.html","notes/math/variance.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown","data":null},"notes/math/mean.html":{"createdTime":1751668570756,"modifiedTime":1754247529503,"sourceSize":10024,"sourcePath":"NOTES/Math/Mean.md","exportPath":"notes/math/mean.html","showInTree":true,"treeOrder":10,"backlinks":["notes/math/binary-data.html","notes/math/measure-of-central-tendency.html","notes/math/median.html","notes/math/mode.html","notes/math/nominal-data.html","notes/math/outlier.html","notes/math/probability.html","notes/math/quantile.html","notes/math/range.html","notes/math/standard-deviation.html","notes/math/variance.html","textbooks/data-mining-concepts-and-techniques/practice-problems.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown","data":null},"notes/math/manhattan-distance.html":{"createdTime":1753987116983,"modifiedTime":1754247526013,"sourceSize":3486,"sourcePath":"NOTES/Math/Manhattan Distance.md","exportPath":"notes/math/manhattan-distance.html","showInTree":true,"treeOrder":9,"backlinks":["notes/math/euclidean-distance.html","notes/math/outlier.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown","data":null},"notes/math/least-squares.html":{"createdTime":1753105505921,"modifiedTime":1754247532104,"sourceSize":6451,"sourcePath":"NOTES/Math/Least Squares.md","exportPath":"notes/math/least-squares.html","showInTree":true,"treeOrder":8,"backlinks":["notes/math/euclidean-distance.html","notes/math/variance.html"],"type":"markdown","data":null},"notes/math/euclidean-distance.html":{"createdTime":1752772872723,"modifiedTime":1754247522422,"sourceSize":6032,"sourcePath":"NOTES/Math/Euclidean Distance.md","exportPath":"notes/math/euclidean-distance.html","showInTree":true,"treeOrder":7,"backlinks":["notes/math/least-squares.html","notes/math/manhattan-distance.html","notes/math/median.html","notes/math/outlier.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown","data":null},"notes/math/commutative-property.html":{"createdTime":1753041273100,"modifiedTime":1754247519350,"sourceSize":2732,"sourcePath":"NOTES/Math/Commutative Property.md","exportPath":"notes/math/commutative-property.html","showInTree":true,"treeOrder":6,"backlinks":["notes/math/associative-property.html","notes/math/propositional-logic.html"],"type":"markdown","data":null},"notes/math/binary-data.html":{"createdTime":1752017585071,"modifiedTime":1754247516543,"sourceSize":4407,"sourcePath":"NOTES/Math/Binary Data.md","exportPath":"notes/math/binary-data.html","showInTree":true,"treeOrder":5,"backlinks":["notes/math/nominal-data.html","notes/math/outlier.html","textbooks/data-mining-concepts-and-techniques/summary.html"],"type":"markdown","data":null},"notes/math/associative-property.html":{"createdTime":1752965745243,"modifiedTime":1754360736681,"sourceSize":2549,"sourcePath":"NOTES/Math/Associative Property.md","exportPath":"notes/math/associative-property.html","showInTree":true,"treeOrder":4,"backlinks":["notes/math/commutative-property.html","notes/math/propositional-logic.html"],"type":"markdown","data":null},"notes/ai/turing-test.html":{"createdTime":1751502135746,"modifiedTime":1754247499532,"sourceSize":4532,"sourcePath":"NOTES/AI/Turing Test.md","exportPath":"notes/ai/turing-test.html","showInTree":true,"treeOrder":2,"backlinks":["textbooks/ai-a-modern-approach/summary.html"],"type":"markdown","data":null},"textbooks/data-mining-concepts-and-techniques/summary.html":{"createdTime":1751226225017,"modifiedTime":1754407904126,"sourceSize":32714,"sourcePath":"TEXTBOOKS/Data Mining-Concepts and Techniques/Summary.md","exportPath":"textbooks/data-mining-concepts-and-techniques/summary.html","showInTree":true,"treeOrder":31,"backlinks":[],"type":"markdown","data":null},"textbooks/data-mining-concepts-and-techniques/practice-problems.html":{"createdTime":1751221818773,"modifiedTime":1754341489535,"sourceSize":11343,"sourcePath":"TEXTBOOKS/Data Mining-Concepts and Techniques/Practice Problems.md","exportPath":"textbooks/data-mining-concepts-and-techniques/practice-problems.html","showInTree":true,"treeOrder":30,"backlinks":[],"type":"markdown","data":null},"textbooks/ai-a-modern-approach/summary.html":{"createdTime":1751236285335,"modifiedTime":1754355077347,"sourceSize":32625,"sourcePath":"TEXTBOOKS/AI-A Modern Approach/Summary.md","exportPath":"textbooks/ai-a-modern-approach/summary.html","showInTree":true,"treeOrder":28,"backlinks":[],"type":"markdown","data":null},"textbooks/ai-a-modern-approach/practice-problems.html":{"createdTime":1751236319998,"modifiedTime":1754013327718,"sourceSize":849,"sourcePath":"TEXTBOOKS/AI-A Modern Approach/Practice Problems.md","exportPath":"textbooks/ai-a-modern-approach/practice-problems.html","showInTree":true,"treeOrder":27,"backlinks":[],"type":"markdown","data":null},"index.html":{"createdTime":1753993773434,"modifiedTime":1754409881723,"sourceSize":1688,"sourcePath":"Index.md","exportPath":"index.html","showInTree":true,"treeOrder":32,"backlinks":[],"type":"markdown","data":null},"site-lib/scripts/graph-wasm.wasm":{"createdTime":1753995693438,"modifiedTime":1753989145137.2212,"sourceSize":23655,"sourcePath":"","exportPath":"site-lib/scripts/graph-wasm.wasm","showInTree":false,"treeOrder":0,"backlinks":[],"type":"other","data":null},"site-lib/fonts/94f2f163d4b698242fef.otf":{"createdTime":1754410548223,"modifiedTime":1754410548223,"sourceSize":66800,"sourcePath":"","exportPath":"site-lib/fonts/94f2f163d4b698242fef.otf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/72505e6a122c6acd5471.woff2":{"createdTime":1754410548223,"modifiedTime":1754410548223,"sourceSize":104232,"sourcePath":"","exportPath":"site-lib/fonts/72505e6a122c6acd5471.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/2d5198822ab091ce4305.woff2":{"createdTime":1754410548223,"modifiedTime":1754410548223,"sourceSize":104332,"sourcePath":"","exportPath":"site-lib/fonts/2d5198822ab091ce4305.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/c8ba52b05a9ef10f4758.woff2":{"createdTime":1754410548224,"modifiedTime":1754410548224,"sourceSize":98868,"sourcePath":"","exportPath":"site-lib/fonts/c8ba52b05a9ef10f4758.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/cb10ffd7684cd9836a05.woff2":{"createdTime":1754410548224,"modifiedTime":1754410548224,"sourceSize":106876,"sourcePath":"","exportPath":"site-lib/fonts/cb10ffd7684cd9836a05.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/b5f0f109bc88052d4000.woff2":{"createdTime":1754410548224,"modifiedTime":1754410548224,"sourceSize":105804,"sourcePath":"","exportPath":"site-lib/fonts/b5f0f109bc88052d4000.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/cbe0ae49c52c920fd563.woff2":{"createdTime":1754410548224,"modifiedTime":1754410548224,"sourceSize":106108,"sourcePath":"","exportPath":"site-lib/fonts/cbe0ae49c52c920fd563.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/535a6cf662596b3bd6a6.woff2":{"createdTime":1754410548224,"modifiedTime":1754410548224,"sourceSize":111708,"sourcePath":"","exportPath":"site-lib/fonts/535a6cf662596b3bd6a6.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/70cc7ff27245e82ad414.ttf":{"createdTime":1754410548224,"modifiedTime":1754410548224,"sourceSize":192740,"sourcePath":"","exportPath":"site-lib/fonts/70cc7ff27245e82ad414.ttf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/454577c22304619db035.ttf":{"createdTime":1754410548224,"modifiedTime":1754410548224,"sourceSize":161376,"sourcePath":"","exportPath":"site-lib/fonts/454577c22304619db035.ttf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/52ac8f3034507f1d9e53.ttf":{"createdTime":1754410548224,"modifiedTime":1754410548224,"sourceSize":191568,"sourcePath":"","exportPath":"site-lib/fonts/52ac8f3034507f1d9e53.ttf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/05b618077343fbbd92b7.ttf":{"createdTime":1754410548225,"modifiedTime":1754410548225,"sourceSize":155288,"sourcePath":"","exportPath":"site-lib/fonts/05b618077343fbbd92b7.ttf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/4bb6ac751d1c5478ff3a.woff2":{"createdTime":1754410548211,"modifiedTime":1754410548211,"sourceSize":7876,"sourcePath":"","exportPath":"site-lib/fonts/4bb6ac751d1c5478ff3a.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/media/6155340132a851f6089e.svg":{"createdTime":1754410548212,"modifiedTime":1754410548212,"sourceSize":315,"sourcePath":"","exportPath":"site-lib/media/6155340132a851f6089e.svg","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"site-lib/media/2308ab1944a6bfa5c5b8.svg":{"createdTime":1754410548211,"modifiedTime":1754410548211,"sourceSize":278,"sourcePath":"","exportPath":"site-lib/media/2308ab1944a6bfa5c5b8.svg","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"site-lib/fonts/mathjax_zero.woff":{"createdTime":1754409106445,"modifiedTime":1754409106445,"sourceSize":1368,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_zero.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_main-regular.woff":{"createdTime":1754409106446,"modifiedTime":1754409106446,"sourceSize":34160,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_main-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_main-bold.woff":{"createdTime":1754409106446,"modifiedTime":1754409106446,"sourceSize":34464,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_main-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_math-italic.woff":{"createdTime":1754409106446,"modifiedTime":1754409106446,"sourceSize":19360,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_math-italic.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_main-italic.woff":{"createdTime":1754409106446,"modifiedTime":1754409106446,"sourceSize":20832,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_main-italic.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_math-bolditalic.woff":{"createdTime":1754409106446,"modifiedTime":1754409106446,"sourceSize":19776,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_math-bolditalic.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_size1-regular.woff":{"createdTime":1754409106447,"modifiedTime":1754409106447,"sourceSize":5792,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_size1-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_size2-regular.woff":{"createdTime":1754409106446,"modifiedTime":1754409106446,"sourceSize":5464,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_size2-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_size3-regular.woff":{"createdTime":1754409106447,"modifiedTime":1754409106447,"sourceSize":3244,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_size3-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_size4-regular.woff":{"createdTime":1754409106446,"modifiedTime":1754409106446,"sourceSize":5148,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_size4-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_ams-regular.woff":{"createdTime":1754409106447,"modifiedTime":1754409106447,"sourceSize":40808,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_ams-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_calligraphic-regular.woff":{"createdTime":1754409106447,"modifiedTime":1754409106447,"sourceSize":9600,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_calligraphic-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_calligraphic-bold.woff":{"createdTime":1754409106447,"modifiedTime":1754409106447,"sourceSize":9908,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_calligraphic-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_fraktur-regular.woff":{"createdTime":1754409106447,"modifiedTime":1754409106447,"sourceSize":21480,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_fraktur-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_fraktur-bold.woff":{"createdTime":1754409106447,"modifiedTime":1754409106447,"sourceSize":22340,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_fraktur-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_sansserif-regular.woff":{"createdTime":1754409106447,"modifiedTime":1754409106447,"sourceSize":12660,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_sansserif-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_sansserif-bold.woff":{"createdTime":1754409106447,"modifiedTime":1754409106447,"sourceSize":15944,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_sansserif-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_sansserif-italic.woff":{"createdTime":1754409106447,"modifiedTime":1754409106447,"sourceSize":14628,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_sansserif-italic.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_script-regular.woff":{"createdTime":1754409106447,"modifiedTime":1754409106447,"sourceSize":11852,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_script-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_typewriter-regular.woff":{"createdTime":1754409106448,"modifiedTime":1754409106448,"sourceSize":17604,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_typewriter-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_vector-regular.woff":{"createdTime":1754409106448,"modifiedTime":1754409106448,"sourceSize":1136,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_vector-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_vector-bold.woff":{"createdTime":1754409106448,"modifiedTime":1754409106448,"sourceSize":1116,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_vector-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/html/file-tree-content.html":{"createdTime":1754410548470,"modifiedTime":1754410548470,"sourceSize":16081,"sourcePath":"","exportPath":"site-lib/html/file-tree-content.html","showInTree":false,"treeOrder":0,"backlinks":[],"type":"html","data":null},"site-lib/scripts/webpage.js":{"createdTime":1754336536701,"modifiedTime":1754336536701,"sourceSize":110729,"sourcePath":"","exportPath":"site-lib/scripts/webpage.js","showInTree":false,"treeOrder":0,"backlinks":[],"type":"script","data":null},"site-lib/scripts/graph-wasm.js":{"createdTime":1754336536701,"modifiedTime":1754336536701,"sourceSize":12885,"sourcePath":"","exportPath":"site-lib/scripts/graph-wasm.js","showInTree":false,"treeOrder":0,"backlinks":[],"type":"script","data":null},"site-lib/scripts/graph-render-worker.js":{"createdTime":1754336536701,"modifiedTime":1754336536701,"sourceSize":5681,"sourcePath":"","exportPath":"site-lib/scripts/graph-render-worker.js","showInTree":false,"treeOrder":0,"backlinks":[],"type":"script","data":null},"site-lib/media/favicon.png":{"createdTime":1754410548102,"modifiedTime":1754410548102,"sourceSize":1105,"sourcePath":"","exportPath":"site-lib/media/favicon.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"site-lib/styles/snippets.css":{"createdTime":1754410548220,"modifiedTime":1754410548220,"sourceSize":488,"sourcePath":"","exportPath":"site-lib/styles/snippets.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"site-lib/styles/obsidian.css":{"createdTime":1754410548249,"modifiedTime":1754410548249,"sourceSize":163797,"sourcePath":"","exportPath":"site-lib/styles/obsidian.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"site-lib/styles/theme.css":{"createdTime":1754336536902,"modifiedTime":1754336536902,"sourceSize":2121232,"sourcePath":"","exportPath":"site-lib/styles/theme.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"site-lib/styles/global-variable-styles.css":{"createdTime":1754410548158,"modifiedTime":1754410548158,"sourceSize":305,"sourcePath":"","exportPath":"site-lib/styles/global-variable-styles.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"site-lib/styles/supported-plugins.css":{"createdTime":1754410548221,"modifiedTime":1754410548221,"sourceSize":2020,"sourcePath":"","exportPath":"site-lib/styles/supported-plugins.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"site-lib/styles/main-styles.css":{"createdTime":1754336536716,"modifiedTime":1754336536716,"sourceSize":19521,"sourcePath":"","exportPath":"site-lib/styles/main-styles.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"resources/linearregression.png":{"createdTime":1753106412811,"modifiedTime":1753106412812,"sourceSize":19594,"sourcePath":"RESOURCES/linearRegression.png","exportPath":"resources/linearregression.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"resources/highvslowstd.png":{"createdTime":1752162585317,"modifiedTime":1752162585318,"sourceSize":27029,"sourcePath":"RESOURCES/HighVsLowSTD.png","exportPath":"resources/highvslowstd.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"resources/pasted-image-20250708171735.png":{"createdTime":1752013056598,"modifiedTime":1752013055594,"sourceSize":23535,"sourcePath":"RESOURCES/Pasted image 20250708171735.png","exportPath":"resources/pasted-image-20250708171735.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"resources/pasted-image-20250708093049.png":{"createdTime":1751985050065,"modifiedTime":1751985049041,"sourceSize":33853,"sourcePath":"RESOURCES/Pasted image 20250708093049.png","exportPath":"resources/pasted-image-20250708093049.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"site-lib/rss.xml":{"createdTime":1754410548584,"modifiedTime":1754410548584,"sourceSize":279590,"sourcePath":"","exportPath":"site-lib/rss.xml","showInTree":false,"treeOrder":0,"backlinks":[],"type":"other","data":null},"notes/math/propositional-logic.html":{"createdTime":1754173984748,"modifiedTime":1754405691700,"sourceSize":21563,"sourcePath":"NOTES/Math/Propositional Logic.md","exportPath":"notes/math/propositional-logic.html","showInTree":true,"treeOrder":18,"backlinks":["index.html","notes/math/associative-property.html"],"type":"markdown","data":null},"site-lib/html/custom-head-content-content.html":{"createdTime":1754358392671,"modifiedTime":1754409583543,"sourceSize":335,"sourcePath":"HTML/site-lib/html/custom.html","exportPath":"site-lib/html/custom-head-content-content.html","showInTree":false,"treeOrder":0,"backlinks":[],"type":"html","data":null}},"sourceToTarget":{"NOTES/Math/Variance.md":"notes/math/variance.html","NOTES/Math/Transitive Property.md":"notes/math/transitive-property.html","NOTES/Math/Standard Deviation.md":"notes/math/standard-deviation.html","NOTES/Math/Range.md":"notes/math/range.html","NOTES/Math/Quartile.md":"notes/math/quartile.html","NOTES/Math/Quantile.md":"notes/math/quantile.html","NOTES/Math/Probability.md":"notes/math/probability.html","NOTES/Math/Outlier.md":"notes/math/outlier.html","NOTES/Math/Ordinal Data.md":"notes/math/ordinal-data.html","NOTES/Math/Nominal Data.md":"notes/math/nominal-data.html","NOTES/Math/Mode.md":"notes/math/mode.html","NOTES/Math/Median.md":"notes/math/median.html","NOTES/Math/Measure of Central Tendency.md":"notes/math/measure-of-central-tendency.html","NOTES/Math/Mean.md":"notes/math/mean.html","NOTES/Math/Math MOC.md":"notes/math/math-moc.html","NOTES/Math/Manhattan Distance.md":"notes/math/manhattan-distance.html","NOTES/Math/Least Squares.md":"notes/math/least-squares.html","NOTES/Math/Euclidean Distance.md":"notes/math/euclidean-distance.html","NOTES/Math/Commutative Property.md":"notes/math/commutative-property.html","NOTES/Math/Binary Data.md":"notes/math/binary-data.html","NOTES/Math/Associative Property.md":"notes/math/associative-property.html","NOTES/AI/Turing Test.md":"notes/ai/turing-test.html","TEXTBOOKS/Data Mining-Concepts and Techniques/Summary.md":"textbooks/data-mining-concepts-and-techniques/summary.html","TEXTBOOKS/Data Mining-Concepts and Techniques/Practice Problems.md":"textbooks/data-mining-concepts-and-techniques/practice-problems.html","TEXTBOOKS/AI-A Modern Approach/Summary.md":"textbooks/ai-a-modern-approach/summary.html","TEXTBOOKS/AI-A Modern Approach/Practice Problems.md":"textbooks/ai-a-modern-approach/practice-problems.html","Index.md":"index.html","":"site-lib/rss.xml","RESOURCES/linearRegression.png":"resources/linearregression.png","RESOURCES/HighVsLowSTD.png":"resources/highvslowstd.png","RESOURCES/Pasted image 20250708171735.png":"resources/pasted-image-20250708171735.png","RESOURCES/Pasted image 20250708093049.png":"resources/pasted-image-20250708093049.png","NOTES/Math/Propositional Logic.md":"notes/math/propositional-logic.html","HTML/site-lib/html/custom.html":"site-lib/html/custom-head-content-content.html","Contact.md":"contact.html"},"featureOptions":{"backlinks":{"featureId":"backlinks","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":".footer","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Backlinks","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""}},"tags":{"featureId":"tags","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":".header .data-bar","type":"end","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"showInlineTags":true,"showFrontmatterTags":true,"info_showInlineTags":{"show":true,"name":"","description":"Show tags defined inside the document at the top of the page.","placeholder":""},"info_showFrontmatterTags":{"show":true,"name":"","description":"Show tags defined in the frontmatter of the document at the top of the page.","placeholder":""}},"alias":{"featureId":"aliases","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":".header .data-bar","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Aliases","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""}},"properties":{"featureId":"properties","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":".header","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Properties","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"info_hideProperties":{"show":true,"name":"","description":"A list of properties to hide from the properties view","placeholder":""}},"fileNavigation":{"featureId":"file-navigation","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#left-sidebar-content","type":"end","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"info_includePath":{"show":false,"name":"","description":"","placeholder":""},"showCustomIcons":false,"showDefaultFolderIcons":false,"showDefaultFileIcons":false,"defaultFolderIcon":"lucide//folder","defaultFileIcon":"lucide//file","defaultMediaIcon":"lucide//file-image","exposeStartingPath":true,"info_showCustomIcons":{"show":true,"name":"","description":"Show custom icons for files and folders","placeholder":""},"info_showDefaultFolderIcons":{"show":true,"name":"","description":"Show a default icon of a folder for every folder in the tree","placeholder":""},"info_showDefaultFileIcons":{"show":true,"name":"","description":"Show a default icon of a file for every file in the tree","placeholder":""},"info_defaultFolderIcon":{"show":true,"name":"","description":"The icon to use for folders. Prefix with 'lucide//' to use a Lucide icon","placeholder":""},"info_defaultFileIcon":{"show":true,"name":"","description":"The icon to use for files. Prefix with 'lucide//' to use a Lucide icon","placeholder":""},"info_defaultMediaIcon":{"show":true,"name":"","description":"The icon to use for media files. Prefix with 'lucide//' to use a Lucide icon","placeholder":""},"info_exposeStartingPath":{"show":true,"name":"","description":"Whether or not to show the current file in the file tree when the page is first loaded","placeholder":""},"includePath":"site-lib/html/file-tree.html"},"search":{"featureId":"search","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#left-sidebar .topbar-content","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Search...","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""}},"outline":{"featureId":"outline","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#right-sidebar-content","type":"end","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Outline","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"startCollapsed":false,"minCollapseDepth":0,"info_startCollapsed":{"show":true,"name":"","description":"Should the outline start collapsed?","placeholder":""},"info_minCollapseDepth":{"show":true,"name":"","description":"Only allow outline items to be collapsed if they are at least this many levels deep in the tree.","placeholder":"","dropdownOptions":{"1":1,"2":2,"No Collapse":100}}},"themeToggle":{"featureId":"theme-toggle","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#right-sidebar .topbar-content","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""}},"graphView":{"featureId":"graph-view","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#right-sidebar-content","type":"end","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Graph View","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"showOrphanNodes":true,"showAttachments":false,"allowGlobalGraph":true,"allowExpand":true,"attractionForce":1,"linkLength":15,"repulsionForce":80,"centralForce":2,"edgePruning":100,"minNodeRadius":3,"maxNodeRadius":7,"info_showOrphanNodes":{"show":true,"name":"","description":"Show nodes that are not connected to any other nodes.","placeholder":""},"info_showAttachments":{"show":true,"name":"","description":"Show attachments like images and PDFs as nodes in the graph.","placeholder":""},"info_allowGlobalGraph":{"show":true,"name":"","description":"Allow the user to view the global graph of all nodes.","placeholder":""},"info_allowExpand":{"show":true,"name":"","description":"Allow the user to pop-out the graph view to take up the whole screen","placeholder":""},"info_attractionForce":{"show":true,"name":"","description":"How much should linked nodes attract each other? This will make the graph appear more clustered.","placeholder":""},"info_linkLength":{"show":true,"name":"","description":"How long should the links between nodes be? The shorter the links the more connected nodes will cluster together.","placeholder":""},"info_repulsionForce":{"show":true,"name":"","description":"How much should nodes repel each other? This will make disconnected parts more spread out.","placeholder":""},"info_centralForce":{"show":true,"name":"","description":"How much should nodes be attracted to the center? This will make the graph appear more dense and circular.","placeholder":""},"info_edgePruning":{"show":true,"name":"","description":"Edges with a length above this threshold will not be rendered, however they will still contribute to the simulation. This can help large tangled graphs look more organised. Hovering over a node will still display these links.","placeholder":""},"info_minNodeRadius":{"show":true,"name":"","description":"How small should the smallest nodes be? The smaller a node is the less it will attract other nodes.","placeholder":""},"info_maxNodeRadius":{"show":true,"name":"","description":"How large should the largest nodes be? Nodes are sized by how many links they have. The larger a node is the more it will attract other nodes. This can be used to create a good grouping around the most important nodes.","placeholder":""}},"sidebar":{"featureId":"sidebar","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"allowResizing":true,"allowCollapsing":true,"rightDefaultWidth":"19em","leftDefaultWidth":"18em","info_allowResizing":{"show":true,"name":"","description":"Whether or not to allow the sidebars to be resized","placeholder":""},"info_allowCollapsing":{"show":true,"name":"","description":"Whether or not to allow the sidebars to be collapsed","placeholder":""},"info_rightDefaultWidth":{"show":true,"name":"","description":"The default width of the right sidebar","placeholder":""},"info_leftDefaultWidth":{"show":true,"name":"","description":"The default width of the left sidebar","placeholder":""}},"customHead":{"featureId":"custom-head","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#right-sidebar-content","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"info_includePath":{"show":false,"name":"","description":"","placeholder":""},"sourcePath":"HTML/site-lib/html/custom.html","info_sourcePath":{"show":true,"name":"","description":"The local path to the source .html file which will be included.","placeholder":"","fileInputOptions":{"makeRelativeToVault":true,"browseButton":true}},"includePath":"site-lib/html/custom-head.html"},"document":{"featureId":"obsidian-document","enabled":true,"unavailable":false,"alwaysEnabled":true,"hideSettingsButton":false,"allowFoldingLists":true,"allowFoldingHeadings":true,"documentWidth":"40em","info_allowFoldingLists":{"show":true,"name":"","description":"Whether or not to allow lists to be folded","placeholder":""},"info_allowFoldingHeadings":{"show":true,"name":"","description":"Whether or not to allow headings to be folded","placeholder":""},"info_documentWidth":{"show":true,"name":"","description":"The width of the document","placeholder":""}},"rss":{"featureId":"rss","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"siteUrl":"https://emujakic.github.io/TechKB/","authorName":"Ernad Mujakic","info_siteUrl":{"show":true,"name":"","description":"The url that this site will be hosted at","placeholder":"https://example.com/mysite"},"info_authorName":{"show":true,"name":"","description":"The name of the author of the site","placeholder":""}},"linkPreview":{"featureId":"link-preview","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":true}},"modifiedTime":1754410548259,"siteName":"Ernad Mujakic's KB","vaultName":"OBSIDIAN KB","exportRoot":"","baseURL":"https://emujakic.github.io/TechKB/","pluginVersion":"1.9.2","themeName":"","bodyClasses":"publish css-settings-manager native-scrollbars show-inline-title show-ribbon ss-title-gradient heading-ligatures body-ligatures ordinary-ol ss-links-spectral callout-border-gradient is-focused","hasFavicon":false}